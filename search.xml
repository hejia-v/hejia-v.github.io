<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[windows上安装tensorflow]]></title>
    <url>%2F2017%2F12%2F16%2Finstall-tensorflow%2F</url>
    <content type="text"><![CDATA[访问 tensorflow 官网方法一: 直接访问 https://tensorflow.google.cn 方法二: 修改 hosts 文件windows的hosts文件的位置是 C:\Windows\System32\drivers\etc\hosts ，在hosts文件末尾添加如下内容： 164.233.188.121 www.tensorflow.org 刷新dns缓存，在命令行执行下面命令(windows)： 1ipconfig /flushdns 在浏览器访问 www.tensorflow.org ，就可以浏览官网了。 安装cuda(win)在nvidia的官网下载cuda的安装包，按照默认选项安装就可以了。如果不出意外，安装完毕后，cuda的bin文件夹会自动添加到环境变量。 在NVIDIA官网的cudnn页面，下载与cuda适配的cudnn压缩包。解压后，会有bin、include、lib三个文件夹，将这三个文件夹拷贝到cuda的安装文件夹下面，并与cuda的bin、include、lib文件夹合并。 如果下载时的网络连接不稳定，推荐使用wget -c url命令下载。 通过pip安装如果cuda和cudnn的版本符合pip包的要求，可以直接通过pip安装pip3 install tensorflow-gpu 编译安装我一般是在虚拟机里使用linux，而虚拟机无法使用gpu加速，因此暂时不考虑在虚拟机的linux里编译tensorflow。tensorflow的windows编译有很多问题，很麻烦。想在windows机器上自己编译，最好的方案是等待wsl支持gpu后，在wsl中编译tensorflow。在OpenCL &amp; CUDA GPU support，Cannot find GPU devices on Bash可以关注一下wsl支持gpu的最新进展。 编译的过程中会下载一些文件，需要留意一下这些下载地址能否访问到。如果git clone时提示 ... port 443: Timed out，则表明这个地址被屏蔽了，为了简单省事，最好还是用shadowsocks。开启shadowsocks后，设置git的http/https代理协议12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; 编译完毕后，取消代理12git config --global --unset http.proxygit config --global --unset https.proxy 可以查看一下git的配置1git config --global -l 为了避免路径中有空格造成的烦恼，建议目录使用符号链接，例如1mklink /D C:\CUDA &quot;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA&quot; 检测gpu是否开启在python的交互环境下，输入如下代码，可以查看有哪些设备12from tensorflow.python.client import device_libprint(device_lib.list_local_devices()) 实际上在tensorflow开始运行后，输出的log中会有device信息，可以判断是否启用了gpu。 测试下面代码是之前的bp神经网络的tensorflow实现，可以用来测试一下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# -*- coding:utf-8 -*-import timefrom sklearn import datasetsimport numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltdef add_layer(inputs, dim_in, dim_out, layer_n, is_output_layer=False, y=None): layer_name = f'layer&#123;layer_n&#125;' with tf.name_scope(layer_name): with tf.name_scope('weights'): Weights = tf.Variable(tf.random_normal([dim_in, dim_out])) # Weight中都是随机变量 tf.summary.histogram(layer_name + "/weights", Weights) # 可视化观看变量 with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1, dim_out])) # biases推荐初始值不为0 tf.summary.histogram(layer_name + "/biases", biases) # 可视化观看变量 with tf.name_scope('z'): z = tf.matmul(inputs, Weights) + biases # inputs*Weight+biases tf.summary.histogram(layer_name + "/z", z) # 可视化观看变量 if is_output_layer: outputs = tf.nn.softmax(z, name='outputs') loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y, name='loss') tf.summary.histogram(layer_name + '/loss', loss) # 可视化观看变量 return outputs, loss else: outputs = tf.nn.tanh(z) tf.summary.histogram(layer_name + "/outputs", outputs) # 可视化观看变量 return outputs, Nonenp.random.seed(0)X_data, y_data = datasets.make_moons(200, noise=0.20)num_examples = len(X_data)ym_data = np.zeros((num_examples, 2))ym_data[range(num_examples), y_data] = 1# 生成一个带可展开符号的域with tf.name_scope('inputs'): xs = tf.placeholder(tf.float32, name='X') ys = tf.placeholder(tf.float32, name='y')tf.set_random_seed(0)# 三层神经网络，输入层（2个神经元），隐藏层（3神经元），输出层（2个神经元）layer1, _ = add_layer(xs, 2, 3, 1) # 隐藏层predict_step, loss = add_layer(layer1, 3, 2, 2, True, ys) # 输出层with tf.name_scope('train'): train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 0.01学习率,minimize(loss)减小loss误差init = tf.global_variables_initializer()config = tf.ConfigProto()# https://tensorflow.google.cn/tutorials/using_gpu#allowing_gpu_memory_growthconfig.gpu_options.allow_growth = Truesess = tf.Session(config=config)# 合并到Summary中merged = tf.summary.merge_all()# 选定可视化存储目录writer = tf.summary.FileWriter("./", sess.graph)sess.run(init) # 先执行initstart_time = time.time()# 训练2w次num_passes = 20000for i in range(num_passes): sess.run(train_step, feed_dict=&#123;xs: X_data, ys: ym_data&#125;) if i % 50 == 0: result = sess.run(merged, feed_dict=&#123;xs: X_data, ys: ym_data&#125;) # merged也是需要run的 writer.add_summary(result, i) # result是summary类型的，需要放入writer中，i步数（x轴）time_cost = time.time() - start_timesummary_text = f'cost: &#123;time_cost&#125;'print(summary_text)# --------------------------- predict ---------------------------def predict(x): predict = sess.run(predict_step, feed_dict=&#123;xs: x&#125;) return np.argmax(predict, axis=1)def visualize(X, y): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) plt.show() plt.title("bp nn")visualize(X_data, y_data) 运行完上面的脚本后，在该脚本目录下执行tensorboard --logdir=&quot;./&quot;命令，就可以在浏览器中查看tensorboard。]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在illustrator中使用latex公式]]></title>
    <url>%2F2017%2F12%2F04%2Fillustrator-latex%2F</url>
    <content type="text"><![CDATA[Adobe Illustrator 是一款强大的矢量图形软件，当我们在里面做好图形后，可能需要在里面插入一些数学符号或者数学公式什么的，这时一般考虑使用 Latex 来输出这些特殊文本。下面介绍一下如何在 Illustrator 中使用 Latex 。 安装 LaTeXLaTeX 有很多发行版，在这里可以查看到。我在windows环境下使用的是 MikTeX，在这里下载了安装包后，按照正常的Windows软件安装流程进行安装，途中可能会提示安装一些依赖，按照提示进行操作即可。 安装完 Latex 后，需要确认一下，从 PATH 环境变量里能否搜索到 pdflatex 命令，如果不能，则需要将 pdflatex 所在的文件夹添加到 PATH 环境变量里。 安装 LaTeX 字体LaTeX 会使用一些特殊的字体，为了让导出的文本能够在 Illustrator 中正常显示，需要让 Illustrator 能够搜索到这些字体。比较简单省事的做法是将这些字体拷贝到adobe的字体文件夹里。例如在Windows上，将 D:\Program Files\MiKTeX 2.9\fonts\type1\public\amsfonts\cm 文件夹里的文件全部拷贝到 C:\Program Files\Common Files\Adobe\Fonts 文件夹。 将 LaTeX 导出到 IllustratorIllustrator 可以从PDF文件中导入单独的页面，因此，可以先将 Latex 代码编译成PDF(可以使用pdflatex)，再将PDF导入到 Illustrator 中去。 为了简化 Latex -&gt; PDF -&gt; Illustrator 这一过程，有一些脚本可供使用，例如 latex-illustrator 和 illustratorLatexEquations等。 根据我的实际使用情况，我对脚本做了一下修改，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// latex2illustrator.jsvar PDF_LATEX_EXE = "pdflatex.exe"; // Add full path if necessaryvar LAST_TEX_CODE_FILE = 'latex2illustrator_lastcode.txt';var TEX_FILE = 'latex2illustrator.tex';var PDF_FILE = 'latex2illustrator.pdf';var BAT_FILE = 'latex2illustrator.bat';var TEMP_PATH = getWorkPath();function getWorkPath() &#123; // determining the local temporary directory var temppath = Folder.temp.fsName; // path already in Windows syntax: c:\... var i = temppath.indexOf("Temporary Internet Files"); if (i &gt;= 0) temppath = temppath.substr(0, i + 4); //temppath should now contain something like C:\Documents and Settings\&lt;user&gt;\Local Settings\Temp return temppath&#125;function getLastCode() &#123; // remember the last user input in a text file var lastCode = "$$" var lastCodeFile = File(TEMP_PATH + "\\" + LAST_TEX_CODE_FILE); if (lastCodeFile.exists) &#123; lastCodeFile.open("r"); lastCode = lastCodeFile.read(); lastCodeFile.close(); &#125; return lastCode&#125;function writeLatexFile(latexCode) &#123; // add latex header etc. to create a complete latex document var latexFile = new File(TEMP_PATH + '\\' + TEX_FILE); latexFile.open("w"); // latexFile.writeln("\\documentclass&#123;standalone&#125;"); latexFile.writeln("\\documentclass&#123;article&#125;"); // add or remove additional latex packages here latexFile.writeln("\\usepackage&#123;amsmath&#125;"); latexFile.writeln("\\usepackage&#123;amsthm&#125;"); latexFile.writeln("\\usepackage&#123;amssymb&#125;"); latexFile.writeln("\\usepackage&#123;gensymb&#125;"); // for \degree latexFile.writeln("\\usepackage&#123;textcomp&#125;"); // for \textdegree latexFile.writeln("\\usepackage&#123;bm&#125;"); // bold math latexFile.writeln("\\begin&#123;document&#125;"); latexFile.writeln("\\pagestyle&#123;empty&#125;"); // no page number latexFile.writeln(latexCode); latexFile.writeln("\\end&#123;document&#125;"); latexFile.close();&#125;function generate(latexcode) &#123; var pdfFile = File(TEMP_PATH + "\\" + PDF_FILE); if (pdfFile.exists) pdfFile.remove(); // create a batch file calling latex var batchFile = new File(TEMP_PATH + '\\' + BAT_FILE); batchFile.open("w"); batchFile.writeln(PDF_LATEX_EXE + ' -aux-directory="' + TEMP_PATH + '" -include-directory="' + TEMP_PATH + '" -output-directory="' + TEMP_PATH + '" "' + TEMP_PATH + '\\' + TEX_FILE + '"'); //batchFile.writeln('pause'); batchFile.writeln('del "' + TEMP_PATH + '\\' + BAT_FILE + '"'); batchFile.close(); batchFile.execute(); for (; batchFile.exists;) // wait until the batch file has removed itself var pdfFile = File(TEMP_PATH + "\\" + PDF_FILE); if (pdfFile.exists) &#123; // import pdf file into the current document var grp = app.activeDocument.activeLayer.groupItems.createFromFile(pdfFile); // The imported objects are grouped twice. Now move the subgroup // items to the main group and skip the last item which is the page frame for (var i = grp.pageItems[0].pageItems.length; --i &gt;= 0;) grp.pageItems[0].pageItems[i].move(grp, ElementPlacement.PLACEATEND); var last = grp.pageItems.length - 1; if (last &gt;= 0 &amp;&amp; grp.pageItems[last].typename == 'PathItem') grp.pageItems[last].remove(); // Move the imported objects to the center of the current view. grp.translate(app.activeDocument.activeView.centerPoint[0] - grp.left, app.activeDocument.activeView.centerPoint[1] - grp.top); &#125; else alert("File " + TEMP_PATH + "\\" + pdfFile.name + " could not be created. LaTeX error?");&#125;function latex2illustrator() &#123; TEMP_PATH = 'E:/data'; var latexCode = getLastCode(); if (latexCode != null) &#123; writeLatexFile(latexCode); generate(latexCode); &#125;&#125;latex2illustrator() 在E:/data/latex2illustrator_lastcode.txt中写入latex公式，然后在Illustrator中使用ctrl+F12执行latex2illustrator.js脚本，就可以插入数学公式了。 参考链接: Combining LaTeX and Illustrator Scripting Adobe Illustrator to create images from LaTeX equations]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个简单的BP神经网络的实现]]></title>
    <url>%2F2017%2F12%2F02%2Fbp-neural-network%2F</url>
    <content type="text"><![CDATA[不详述神经网络模型，只记录一下实现BP神经网络时的推导过程。 输入值和输出值输入值是一个维度是n的特征向量，记作 $x=(x_1,x_2,\ldots,x_n)$。一个数据集里一般有多个样本，假定有m个样本，则将这数据集记作 $X = (x^{(1)},x^{(2)},\ldots,x^{(m)})$ 。 输出值一般是一个数值，用于表示属于哪个类别。对于m个样本，输出值的集合记作 $y=(y^{(1)}, y^{(2)}, \ldots, y^{(m)})$ 。 输入层的节点个数取决于输入的特征向量的维度。 输出层的节点个数取决于拥有的类别个数。如果只有2类，则可以只用一个输出节点用于预测0或1。 隐藏层的节点越多，则越复杂的函数可以fit到。但是这样做的代价也比较大，首先，会增大训练参数和进行预测的计算量。其次，大量的参数也容易导致过拟合。 需要根据具体情况选择节点的个数。 有一个经验公式可以确定隐含层节点数目，如下 h=\sqrt{m+n} + a其中h为隐含层节点数目，m为输入层节点数目，n为输出层节点数目，a为1~10之间的调节常数。 正向传播 这里约定上标 (i) 为样本在样本集中的序号，上标 [i] 为神经网络的层的序号，下标 [i] 为网络中某一层的节点的序号，log的底数默认是e。 神经网络使用正向传播进行预测。 对于一个3层的神经网络，可以这样计算预测值 $\hat y$ ： \begin{align} & z^{[1]} = xW^{[1]}+ b^{[1]} \\ & a^{[1]} = \sigma (z^{[1]}) \\ & z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\ & a^{[2]} = \hat y = softmax (z^{[2]}) \\ \end{align}神经网络里的计算都需要进行向量化，具体来说 对于第1层的输入值和输出值 X = \begin{pmatrix} x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\ x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)} \\ \end{pmatrix} = \begin{pmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\ \end{pmatrix} , \qquad y = \begin{pmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \\ \end{pmatrix}第2层 假设第2层有u个节点，则这层的权重和偏置为 W^{[1]} = \begin{pmatrix} w_{1[1]} & w_{1[2]} & \cdots & w_{1[u]} \\ w_{2[1]} & w_{2[2]} & \cdots & w_{2[u]} \\ \vdots & \vdots & \ddots & \vdots \\ w_{n[1]} & w_{n[2]} & \cdots & w_{n[u]} \\ \end{pmatrix} = \begin{pmatrix} w_{[1]} & w_{[2]} & \ldots & w_{[u]} \end{pmatrix} , \qquad b^{[1]} = \begin{pmatrix} b_{[1]} & b_{[2]} & \ldots & b_{[u]} \end{pmatrix}根据矩阵的分块的性质 \begin{align} XW^{[1]} &= \begin{pmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\ \end{pmatrix} \begin{pmatrix} w_{[1]} & w_{[2]} & \ldots & w_{[u]} \end{pmatrix} = AB = \begin{pmatrix} A_{11} \\ A_{21} \\ \vdots \\ A_{m1} \\ \end{pmatrix} \begin{pmatrix} B_{11} & B_{12} & \ldots & B_{1u} \\ \end{pmatrix} \\ & = \begin{pmatrix} C_{11} & C_{12} & \cdots & C_{1u} \\ C_{21} & C_{22} & \cdots & C_{2u} \\ \vdots & \vdots & \ddots & \vdots \\ C_{m1} & C_{m2} & \cdots & C_{mu} \\ \end{pmatrix} = \begin{pmatrix} A_{11}B_{11} & A_{11}B_{12} & \cdots & A_{11}B_{1u} \\ A_{21}B_{11} & A_{21}B_{12} & \cdots & A_{21}B_{1u} \\ \vdots & \vdots & \ddots & \vdots \\ A_{m1}B_{11} & A_{m1}B_{12} & \cdots & A_{m1}B_{1u} \\ \end{pmatrix} \\ & = \begin{pmatrix} x^{(1)}w_{[1]} & x^{(1)}w_{[2]} & \cdots & x^{(1)}w_{[u]} \\ x^{(2)}w_{[1]} & x^{(2)}w_{[2]} & \cdots & x^{(2)}w_{[u]} \\ \vdots & \vdots & \ddots & \vdots \\ x^{(m)}w_{[1]} & x^{(m)}w_{[2]} & \cdots & x^{(m)}w_{[u]} \\ \end{pmatrix} \end{align}参考numpy中的boardcast，则有 \begin{align} z^{[1]} &= XW^{[1]} + boardcast\ (b^{[1]}) = \begin{pmatrix} x^{(1)}w_{[1]} + b_{[1]} & x^{(1)}w_{[2]} + b_{[2]} & \cdots & x^{(1)}w_{[u]} + b_{[u]} \\ x^{(2)}w_{[1]} + b_{[1]} & x^{(2)}w_{[2]} + b_{[2]} & \cdots & x^{(2)}w_{[u]} + b_{[u]} \\ \vdots & \vdots & \ddots & \vdots \\ x^{(m)}w_{[1]} + b_{[1]} & x^{(m)}w_{[2]} + b_{[2]} & \cdots & x^{(m)}w_{[u]} + b_{[u]} \\ \end{pmatrix} \\ & = \begin{pmatrix} z_{[1]}^{(1)} & z_{[2]}^{(1)} & \cdots & z_{[u]}^{(1)} \\ z_{[1]}^{(2)} & z_{[2]}^{(2)} & \cdots & z_{[u]}^{(2)} \\ \vdots & \vdots & \ddots & \vdots \\ z_{[1]}^{(m)} & z_{[2]}^{(m)} & \cdots & z_{[u]}^{(m)} \\ \end{pmatrix} \end{align} a^{[1]} = \begin{pmatrix} \sigma(z_{[1]}^{(1)}) & \sigma(z_{[2]}^{(1)}) & \cdots & \sigma(z_{[u]}^{(1)}) \\ \sigma(z_{[1]}^{(2)}) & \sigma(z_{[2]}^{(2)}) & \cdots & \sigma(z_{[u]}^{(2)}) \\ \vdots & \vdots & \ddots & \vdots \\ \sigma(z_{[1]}^{(m)}) & \sigma(z_{[2]}^{(m)}) & \cdots & \sigma(z_{[u]}^{(m)}) \\ \end{pmatrix}以上的计算方式可以推广到多层神经网络中的任一隐藏层，对于第$l$层，将 $X$ 类比成 $a^{[l-1]}$ ， $W^{[1]}$ 类比成 $W^{[l]}$ ， $b^{[1]}$ 类比成 $b^{[l]}$ ， $z^{[1]}$ 类比成 $z^{[l]}$ ， $a^{[1]}$ 类比成 $a^{[l]}$ ，就可以了。 对于输出层，因为我们希望网络输出各个分类的概率，所以将输出层的激活函数选为softmax 。softmax 是一种简便的将原始评分转换成概率的方法。可以将 softmax 看做 logistic 函数的在多分类下的推广。 a^{[2]} = softmax (z^{[2]}) = \begin{pmatrix} softmax(z^{[2](1)}) \\ softmax(z^{[2](2)}) \\ \vdots \\ softmax(z^{[2](m)}) \\ \end{pmatrix}权重和偏置的初始化权重 w 不能全部置为0，这样会导致每个层的所有节点的计算都是相同的。如果激活函数使用sigmoid或者tanh，随机出来的 w 最好小一点，一般乘以0.01。因为如果 w 比较大，通过激活函数计算出来的值会落到接近1的位置，导致学习速度变慢。 偏置 b 可以全部初始化为0，但是推荐初始值不为0。 激活函数 tanh sigmoid function ReLUs softmaxsoftmax 又称为归一化指数函数，是广义上的 logistic 函数。假设k 维向量 $z$ 的各项是任意实数，使用 softmax 对 $z$ 进行“压缩”处理后，压缩后的向量 $\sigma (z)$ 的每一项的值在 [0, 1] 之间，所有项之和等于1。函数公式如下 \sigma (z)_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad for\ j = 1, \ldots, K.一个简单的例子如下 1234import numpy as npz = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])exp_scores = np.exp(z)probs = exp_scores / np.sum(exp_scores) # 应用 softmax 在神经网络的输出层中，有C个分类，对于给定的输入 $z$ ，每个分类的概率可以表示为： \begin{bmatrix} P(t=1|z) \\ P(t=2|z) \\ \vdots \\ P(t=C|z) \\ \end{bmatrix} = \frac {1}{ \sum_{k=1}^C e^{z_k} } \begin{bmatrix} e^{z_1} \\ e^{z_2} \\ \vdots \\ e^{z_C} \\ \end{bmatrix}其中，$P(t=c|z)$ 表示，在给定输入$z$时，该输入数据是$c$分类的概率。 对于 softmax 函数 a_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad for\ j = 1, \ldots, K.softmax 函数的求导过程比较特别，分如下2种情况。这是因为i = j 时，$z_i$ 与 $z_j$ 是同一个变量，按偏导数的定义，将多元函数关于一个自变量求偏导数时，就将其余的自变量看成常数，因此需要分2种情况处理。 \begin{align} &if \ j=i \\ &\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr) = \frac { (e^{z_j})^\prime \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_j} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 } = \frac { e^{z_j} } { \sum_k e^{z_k} } - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_j} } { \sum_k e^{z_k} } = a_j ( 1-a_j ) \\ &if \ j \neq i \\ &\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr) = \frac { 0 \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_i} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 } = - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_i} } { \sum_k e^{z_k} } = -a_j a_i \end{align}反向传播反向传播主要思想是：（1）将训练集数据输入到输入层，经过隐藏层，最后达到输出层并输出结果，这是前向传播过程；（2）由于输出层的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。 代价函数我们把定义估计值与实际值之间误差(单个样本)的函数叫作误差损失(loss)函数，代价(cost)函数是各个样本的loss函数的平均。 如果误差损失函数采用二次代价函数 ，在实际中，如果误差越大，参数调整的幅度可能更小，训练更缓慢。使用交叉熵代价函数替换二次代价函数，可以解决学习缓慢的问题。 示性函数：$1\{\cdot\}$取值规则为：$1\{值为真的表达式\}=1$ ，$1\{值为假的表达式\}=0$ 。举例来说，表达式 $1\{2+2=4\}$的值为1，$1\{1+1=5\}$的值为 0。 在分类问题中，交叉熵代价函数与对数似然代价函数在形式上是基本一致的。 对于输出层的softmax激活函数，假设有m个样本，k个类别，将$p(x)$记为$1\{ y^{(i)} = j \}$，$q(x)$记为softmax函数的输出值 则根据交叉熵公式， H(p,q) = - \sum_x p(x)log\ q(x)可以得到交叉熵代价函数为： J(\theta) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \frac{e^{\theta_j^T x^{(i)}}}{ \sum_{l=1}^k e^{ \theta_l^T x^{(i)} } } \biggl] \Biggl]从似然函数的角度分析，记$h_{\theta j}(x)= \frac{e^{\theta_j^T x}}{ \sum_{l=1}^k e^{ \theta_l^T x } }$ (h一般是hypothesis的缩写)，在一个样本中，对于输入x的分类结果为j的概率为 P(y=j|x; \theta) = h_{\theta j} (x)^{ 1\{ y = j \} }将所有分类的概率综合起来，则有： P(y|x; \theta) = \prod_{j=1}^k h_{\theta j} (x)^{ 1\{ y = j \} }取似然函数为： \begin{align} L(\theta) &= \prod_{i=1}^m P(y^{(i)} | x^{(i)}; \theta) \\ &= \prod_{i=1}^m \biggl[ \prod_{j=1}^k h_{\theta j} (x^{(i)})^{1\{ y^{(i)} = j \}} \biggl] \end{align}对数似然函数为： \begin{align} l(\theta) &= log\ L(\theta) \\ &= \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ h_{\theta j} (x^{(i)}) \biggl] \end{align}最大似然估计就是要求得使$l(\theta)$取最大值时的$\theta$ 。一般将它乘上一个负系数-1/m，即： J(\theta) = - \frac{1}{m} l(\theta)则$J(\theta)$取最小值时的$\theta$为要求的最佳参数。这也就是上面的交叉熵代价函数。 log MN = log M + log N 很多文献里对数都没标底数，这说明可以取任意底数，一般取e或2，取不同的底数时，对数值只相差了一个常数系数，对算法不会有影响。 相关的具体分析参考如下链接： Softmax回归 Softmax Regression Softmax分类函数 neural_network_implementation_intermezzo02 Improving the way neural networks learn 交叉熵代价函数 交叉熵代价函数 cross-entropy loss二次代价函数的不足考察一下二次代价函数 C = \frac {1}{2n} \sum_x \| y(x) - a^L(x) \|^2其中，C表示代价，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。为简单起见，以一个样本为例进行说明，此时二次代价函数为： C = \frac {(y-a)^2}{2}在用梯度下降法(gradient descent) 调整权重w和偏置b的过程中，w和b的梯度推导如下： \begin{align} &\frac { \partial C }{ \partial w } = (a - y)\sigma^\prime(z) x \\ &\frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z) \\ \end{align}可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数或tanh函数，观察这些激活函数的图像，当初始的代价（误差）越大时，梯度（导数）越小，训练的速度越慢。这与我们的期望不符，即不能错误越大，改正的幅度越大，从而学习得越快。 交叉熵代价函数为了克服二次代价函数学习缓慢的缺点，引入了交叉熵代价函数： C = - \frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)]其中，x表示样本，n表示样本的总数。重新计算参数w的梯度： \begin{align} \frac{\partial C}{\partial w_j} &= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \frac{\partial \sigma(z)}{\partial w_j} \\ &= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \sigma^\prime(z)x_j \\ &= \frac{1}{n} \sum_x \frac{ \sigma^\prime(z)x_j }{\sigma(z) (1-\sigma(z))} (\sigma(z)-y) \\ &= \frac{1}{n} \sum_x x_j (\sigma(z)-y) \\ \end{align}其中，w的梯度公式中原来的 $\sigma^\prime(z)$ 被消掉了；另外，该梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数w调整得越快，训练速度也就越快。同理可得，b的梯度为： \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y)交叉熵代价函数的来源用交叉熵代替二次代价函数的想法源自哪里？ 以偏置b的梯度计算为例 \begin{align} \frac{\partial C}{\partial b} &= \frac{\partial C}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial b} \\ &= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \cdot \frac{\partial (wx+b)}{\partial b} \\ &= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \\ &= \frac{\partial C}{\partial a} \cdot a(1-a) \\ \end{align}而二次代价函数推导出来的b的梯度公式为： \frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z)为了消掉该公式中的 $\sigma^\prime(z)$ ，需要找到一个代价函数使得： \frac { \partial C }{ \partial b } = (a - y)即 \frac { \partial C }{ \partial a } \cdot a(1-a) = (a - y)对方程进行关于a的积分，可得： C = -[yln\ a + (1-y)ln(1-a)] + constant其中constant是积分常量。这是一个单独训练样本X对代价函数的贡献。为了得到整个的代价函数，还需要对所有的训练样本进行平均，可得： C = -\frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)] + constant而这就是前面的交叉熵代价函数。 在分类问题中，交叉熵其实就是对数似然函数的最大化。 关于交叉熵的更多内容，参考以下链接： 交叉熵（Cross-Entropy） Cross entropy 怎样理解 Cross Entropy 信息论的熵 Entropy 反向传播算法 求导的链式法则 (Chain rule)：表达式：$(f(g(x)))^\prime = f^\prime (g(x)) g^\prime (x)$其他形式：$\frac {dy}{dx} = \frac {dy}{dz} \cdot \frac {dz}{dx}$ 求$J(W, b)$的最小值可以使用梯度下降法，根据梯度下降法可得$W$和$b$的更新过程： W^{[l]}_{[i]j} := W^{[l]}_{[i]j} - \alpha \frac{\partial}{\partial \ W^{[l]}_{[i]j}} J(W, b) \\ b^{[l]}_{[i]} := b^{[l]}_{[i]} - \alpha \frac{\partial}{\partial \ b^{[l]}_{[i]}} J(W, b)其中，$\alpha$为学习步长，$W^{[l]}_{[i]j}$为第l层的第i个节点的权重第j个分量，$b^{[l]}_{[i]}$为第l层的第i个节点的偏置。 输出层输出层的激活函数采用的是softmax函数。根据前文，输出层的误差采用交叉熵代价函数来衡量，即： x^{(i)} = a ^{[1](i)}\\ z_{[j]}^{[2](i)} = x^{(i)}W_{[j]}^{[2]} + b_{[j]}^{[2]} \\ a_{[j]}^{[2](i)} = \frac{e^{z_{[j]}^{[2](i)}}}{ \sum_{k=1}^K e^{ z_{[k]}^{[2](i)} } } \\ J(W, b) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \ a_{[j]}^{[2](i)} \biggl] \Biggl]其中，依照前文约定，下标 [j] 为第$j$个节点的序号，上标 (i) 为样本在样本集中的序号。 输出层的第$l$个节点的权重$W_{[l]}$的第$t$个分量，求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$) \begin{align} \frac{\partial}{\partial \ W_{[l]t}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ W_{[l]t}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ W_{[l]t}} \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]} \biggl) + \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}} \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl( 1\{ y^{(i)} = l \} - a_{[l]} \biggl) \cdot x^{(i)}_t \Biggl] \\ \end{align}输出层的第$l$个节点的偏置$b_{[l]}$的求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$) \begin{align} \frac{\partial}{\partial \ b_{[l]}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ b_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ b_{[l]}} \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]} \biggl) + \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}} \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \biggl] \Biggl] \\ \end{align}在很多的文献中，会把上式记成如下形式，其中上标(i)表示是第i个样本： \begin{align} \delta_{[l]}^{[2](i)} &= \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} = \frac{\partial}{ \partial \ z_{[l]}^{[2](i)} } \biggl[- \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}^{[2](i)} \biggl] = a_{[l]}^{[2](i)} - 1\{ y^{(i)} = l \} \\ \frac{\partial}{\partial \ W_{[l]t}^{[2]}} J(W, b) &= \frac{\partial }{\partial \ W_{[l]t}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)}) = \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ W_{[l]t}^{[2]}} ) = \frac{1}{m} \sum_{i=1}^m ( \delta_{[l]}^{[2](i)} \cdot a_t^{[1](i)} ) \\ \frac{\partial}{\partial \ b_{[l]}^{[2]}} J(W, b) &= \frac{\partial }{\partial \ b_{[l]}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)}) = \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ b_{[l]}^{[2]}} ) = \frac{1}{m} \sum_{i=1}^m \delta_{[l]}^{[2](i)} \\ \end{align} 这里log的底数用的是e 这里的x指的是输出层的上一层的输出 关于$\delta$的原文是：for each node $i$ in layer $l$, we would like to compute an “error term” $\delta^{(l)}$ that measures how much that node was “responsible” for any errors in our output ，链接在这里 隐藏层隐藏层的激活函数采用的是tanh函数。 先考虑只有一个隐藏层的情况，参考下面这张图片，可以发现当前层的每个节点的权重和偏置会影响下一层的各个节点 \begin{align} & z^{[1]} = xW^{[1]}+ b^{[1]} \\ & a^{[1]} = \sigma (z^{[1]}) = tanh(z^{(i)}) \\ & z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\ & a^{[2]} = \hat y = softmax (z^{[2]}) \\ \end{align} \begin{align} \frac{\partial}{\partial \ W^{[1]}_{[v]t}} J(W, b) &= \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } E^{(i)} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggr] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \quad \text{(multivariate chain rule)}\\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggr] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \cdot x_t^{(i)} \biggr] \\ \end{align} \begin{align} \frac{\partial}{\partial \ b^{[1]}_{[v]}} J(W, b) &= \frac{\partial}{ \partial \ b^{[1]}_{[v]} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ b^{[1]}_{[v]} } E^{(i)} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggl] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \quad \text{(multivariate chain rule)}\\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggl] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \biggr] \\ \end{align}按照输出层的$\delta$的定义，可以定义任意一层的$\delta$，并在计算的时候将本层的$\delta$传递给下一层，从而计算各层的权重和偏置的导数。 几个变量相互之间有依赖关系，这时某个变量的偏导数不能反映变化率，要表示在该变量上的变化率，应该使用该变量的全导数。变量相互独立时，偏导数可以表示变化率。 参考链接： Total derivative 如何理解神经网络里面的反向传播算法 A Step by Step Backpropagation Example 终止条件 权重的更新低于某个阈值； 预测的错误率低于某个阈值； 达到预设一定的循环次数； 向量化下面这张图描述了正向传播时，各变量的维度 其中$x$的横线表示$x$可以看做是由一组横向量组成的，$W$的竖线表示$W$可以看做是由一组竖向量组成的。 附录反向传播的一种错误求导在隐藏层的反向传播求导时，下面的求导方式是错误的： \begin{align} \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } &= \sum_o \bigl[ \frac{\partial \ E_{[o]}^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \bigr] \\ \end{align}原因是$z_{[o]}^{(i)[2]}$对所有的$E_{[j]}^{(i)}$都有影响，而这里只考虑了与$z_{[o]}^{(i)[2]}$相对应的$E_{[o]}^{(i)}$。 输出层的求导是没问题的，因为考虑了所有的分量。 数学复习 矩阵A(m, n)，m指行数，n指列数 sigmoid函数求导 \begin{align} \sigma^\prime(z) &= \bigl( \frac{1}{1 + e^{-z}} \bigr)^\prime = (-1)(1+e^{-z})^{(-1)-1} \cdot (e^{-z})^\prime = \frac{1}{(1+e^{-z})^2} \cdot (e^{-z}) \\ &= \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} = \frac{1}{1+e^{-z}} \cdot (1-\frac{1}{1+e^{-z}}) \\ &= \sigma(z)(1-\sigma(z)) \end{align}使用数学软件在一些推导中，特别是矩阵的计算，想直观的看一下展开后的结果，如果完全手算，会比较费时，可以使用mathematica进行符号计算。例如，计算矩阵的相乘，在mathematica中输入以下代码 12&#123;u, n, m&#125; = &#123;3, 2, 5&#125;Array[Subscript[w, ##] &amp;, &#123;u, n&#125;] . Array[Subscript[x, ##] &amp;, &#123;n, m&#125;] 得到结果 \left( \begin{array}{ccccc} w_{1,1} x_{1,1}+w_{1,2} x_{2,1} & w_{1,1} x_{1,2}+w_{1,2} x_{2,2} & w_{1,1} x_{1,3}+w_{1,2} x_{2,3} & w_{1,1} x_{1,4}+w_{1,2} x_{2,4} & w_{1,1} x_{1,5}+w_{1,2} x_{2,5} \\ w_{2,1} x_{1,1}+w_{2,2} x_{2,1} & w_{2,1} x_{1,2}+w_{2,2} x_{2,2} & w_{2,1} x_{1,3}+w_{2,2} x_{2,3} & w_{2,1} x_{1,4}+w_{2,2} x_{2,4} & w_{2,1} x_{1,5}+w_{2,2} x_{2,5} \\ w_{3,1} x_{1,1}+w_{3,2} x_{2,1} & w_{3,1} x_{1,2}+w_{3,2} x_{2,2} & w_{3,1} x_{1,3}+w_{3,2} x_{2,3} & w_{3,1} x_{1,4}+w_{3,2} x_{2,4} & w_{3,1} x_{1,5}+w_{3,2} x_{2,5} \\ \end{array} \right)代码下面是3层bp神经网络的python实现，取自这里，我做了一些修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# -*- coding:utf-8 -*-import timeimport numpy as npfrom sklearn import datasetsimport matplotlib.pyplot as pltdef generate_data(): np.random.seed(0) X, y = datasets.make_moons(200, noise=0.20) return X, ydef visualize(X, y, model): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = predict(model, np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) plt.show() plt.title("bp nn")def predict(model, x): layer1, layer2 = model feedforward(x, layer1) feedforward(layer1.a, layer2, True) return np.argmax(layer2.a, axis=1)class Layer(object): def __init__(self, last_layer_dim: int, dim: int): self.W = np.random.randn(last_layer_dim, dim) / np.sqrt(last_layer_dim) self.b = np.zeros((1, dim)) self.z = None self.a = None self.delta = None self.dW = None self.db = Nonedef softmax(X): # m行dim列 exp_scores = np.exp(X) probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) return probsdef feedforward(x: np.ndarray, layer: Layer, is_final: bool=False): layer.z = x.dot(layer.W) + layer.b # m行dim列 if is_final: layer.a = softmax(layer.z) else: layer.a = np.tanh(layer.z)def backprop(x: np.ndarray, y: np.ndarray, layer: Layer, nextlayer: Layer, num_examples: int, is_final: bool=False): if is_final: delta = layer.a delta[range(num_examples), y] -= 1 # m行dim列, a_&#123;[l]&#125;^&#123;(i)&#125; - 1&#123;y_&#123;(i)&#125;=l&#125; # layer.dW = (x.T).dot(delta) / num_examples # layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples layer.dW = (x.T).dot(delta) layer.db = np.sum(delta, axis=0, keepdims=True) layer.delta = delta else: delta = nextlayer.delta.dot(nextlayer.W.T) * (1 - np.power(layer.a, 2)) # * 对应元素相乘 # layer.dW = np.dot(x.T, delta) / num_examples # layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples layer.dW = np.dot(x.T, delta) layer.db = np.sum(delta, axis=0, keepdims=True) layer.delta = delta learn_rate = 0.01 layer.W += -learn_rate * layer.dW layer.b += -learn_rate * layer.dbdef build_model(X, y, nn_hdim, num_passes=20000, print_loss=False): num_examples = len(X) np.random.seed(0) input_dim = X.shape[1] nn_output_dim = 2 layer1 = Layer(input_dim, nn_hdim) layer2 = Layer(nn_hdim, nn_output_dim) for i in range(0, num_passes): feedforward(X, layer1) feedforward(layer1.a, layer2, True) backprop(layer1.a, y, layer2, None, num_examples, True) backprop(X, y, layer1, layer2, num_examples) model = [layer1, layer2] return modeldef main(): X, y = generate_data() start_time = time.time() model = build_model(X, y, 3) time_cost = time.time() - start_time summary = f'cost: &#123;time_cost&#125;' print(summary) visualize(X, y, model)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android开发中的一些经验]]></title>
    <url>%2F2017%2F12%2F01%2Fandroid-issues-md%2F</url>
    <content type="text"><![CDATA[记录一下android开发中的一些经验，本文将持续更新。 jar命令的使用JAR文件即 Java Archive File，是 Java 的一种文档格式。JAR 文件实际上就是 ZIP 文件，使用unzip xxx.jar -d dest/命令即可解压。 jar命令的说明如下： 下面是jar命令的一些常用用法： 显示jar包 1jar tvf hello.jar # 查看hello.jar包的内容 解压jar包 1jar xvf hello.jar # 解压hello.jar至当前目录 更多用法参考 JAR命令&amp;JAR包详解 java的反编译很多情况下，当我们使用别人的jar包时，很有可能会遇到一些问题，这时需要了解一下jar包里到底有些什么，方便定位问题所在。jar命令只是对jar包进行了一下解包，想查看其中的代码细节，还需要进行反编译。Java Decompiler是一款比较好用的反编译软件，在实际使用中，能够反编译出大部分的java代码。eclipse、android studio安装Java Decompiler插件后，在使用第三方jar库时，会减少很多烦恼。 获取apk的签名和MD5指纹在很多情况下，需要确认apk的签名。例如，接入渠道sdk后，sdk的初始化或者登陆、支付等功能出现了问题，就有可能是apk的SHA1签名与在渠道后台配置的SHA1签名不一致导致的。 获取apk的SHA1签名的一般步骤是: 解压apk：unzip testapp.apk。 找到解压出来的RSA文件。 将终端切到RSA文件所在的目录, 在命令行输入 keytool -printcert -file ./***.RSA ，即可获取sha1签名和md5指纹。具体操作如图 apktool的使用解包和打包apktool 是一款逆向工程工具。在这里有相关的文档。 基础功能 解压apk，生成可读的AndroidManifest.xml， 1apktool d testapp.apk 解压jar 1apktool d foo.jar smali调试，参考wiki 查看apk里面的java源代码 下载dex2jar 使用unzip解压apk：unzip testapp.apk 使用dex2jar将dex文件转换成jar文件 使用jd-gui打开生成的classes-dex2jar.jar，就可以查看java源代码了 具体操作如图 修改apk里面的内容 使用apktool解压apk，apktool d testapp.apk 更新so包，比如可以将release版的so替换成debug版的so 更新资源文件 更改smali，smali的语法可以参考这篇文章 打包回apk，打包出来的文件在apk文件夹中的dist目录下 对重新打包后的apk文件进行签名1jarsigner -verbose -keystore yourKey.keystore -storepass yourPassword path/to/apk /path/to/keystore/file 查看apk的一些信息使用android studio直接打开apk包，可以快速的浏览apk中的一些信息，但是会在用户文件夹生成一个临时工程，占用c盘资源。 log查看目前常用的获取android上面的log的方法有这些： 直接使用logcat命令，一般是将logcat的输出重定向到文件，然后再在文件里查找需要的信息 使用eclipse或者android studio的logcat窗口查看log。eclipse在非调试模式下，logcat窗口并不是很好用；android studio有一定几率连不上设备，这时很会恼火。 app收集log并显示在屏幕上，由于log有可能会刷新得很快，在设备的屏幕上，想定位到具体信息并不是很方便，并且这种方式对app的性能有一定的影响。 个人感觉比较好的方式是参考一下AirDroid，开发一个独立的app用于收集android上的log，这个app开启一个web服务，并将收集到的log输出到这个web上，在电脑上的浏览器中访问这个web，可以获取到设备上的实时log。 一些c/c++库在android上的编译 编译Boost，参考Boost-for-Android 编译Python，参考python的issue30386 android studio引用外部工程很多时候，库工程并不是放在项目文件夹下面，而是放在其他位置，常见的原因是想将这个库工程作为一个公共的库，在几个项目之间使用。 android studio中引用外部库的方法是这样的，在项目的settings.gradle文件中添加如下语句：12include &apos;:BaiduLBS&apos;project(&apos;:BaiduLBS&apos;).projectDir = new File(settingsDir, &apos;../platform/android/sdk/BaiduLBS&apos;) 即可添加相对路径在../platform/android/sdk/BaiduLBS处的外部库BaiduLBS。 横竖屏的问题当手机进行横竖屏切换，弹出键盘，窗口大小发生变化等情况发生时，activity会重新走一遍OnCreate等生命周期方法。要避免这种行为，需要在AndroidManifest.xml中，为activity添加android:configChanges属性，例如1234&lt;activity android:name="com.xxx.MainActivity" android:configChanges="orientation|keyboardHidden|screenSize" &gt;&lt;/activity&gt; 这时，当有orientation、keyboardHidden、screenSize情况发生时，就不会重建activity并调用OnCreate等方法，而是调用原实例的onConfigurationChanged方法。 游戏画面只有屏幕一半在android手机上玩游戏时，经常会遇到这样一种情况，就是不知道做了一些什么操作，屏幕上只有部分(不一定是1/2)区域有游戏画面，其余区域是黑色的。 个人感觉是经过了某些操作，导致GLSurfaceView的Layout有问题。我使用下面的方法试了一下，能够很大程度的降低这种情况的发生123456789101112131415161718192021// 固定为横屏private void resetGLSurfaceViewSize() &#123; DisplayMetrics dm = new DisplayMetrics(); getWindowManager().getDefaultDisplay().getMetrics(dm); int w = dm.widthPixels; int h = dm.heightPixels; Log.d(TAG, "屏幕的分辨率为：" + w + "*" + h); if (w &lt;= 0 || h &lt;= 0) &#123; return; &#125; w = w &gt; h ? w : h; h = w &gt; h ? h : w; mGLSurfaceView.getLayoutParams().width = w; mGLSurfaceView.getLayoutParams().height = h;&#125;@Overridepublic void onConfigurationChanged(Configuration newConfig) &#123; super.onConfigurationChanged(newConfig); resetGLSurfaceViewSize();&#125; 多次点击app icon的问题如果一个游戏接入了第3方的登录模块，当打开游戏弹出第3方的登录框时，按下home键回到桌面，再次点击这个游戏的icon，期望能返回游戏，并且登录框不被清除，这时需要将游戏activity的launchMode设置为singleTop，如下1234&lt;activity android:name="com.xxx.MainActivity" android:launchMode="singleTop" &gt;&lt;/activity&gt;]]></content>
      <categories>
        <category>android</category>
      </categories>
      <tags>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HLSL 常用函数]]></title>
    <url>%2F2017%2F12%2F01%2Fhlsl-library%2F</url>
    <content type="text"><![CDATA[HLSL是unity推荐的shader语言，HLSL和Cg很相似。这里整理了一下网络上收集到的相关资料，方便自己学习和查询。 HLSL固有函数Intrinsic Functions (DirectX HLSL) 函数名 用法 描述 Description Minimum shader model abs abs(x) 计算输入值的绝对值。 Absolute value (per component). 11 acos acos(x) 返回输入值反余弦值。 Returns the arccosine of each component of x. 11 all all(x) 测试非0值。 Test if all components of x are nonzero. 11 AllMemoryBarrier Blocks execution of all threads in a group until all memory accesses have been completed. 5 AllMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all memory accesses have been completed and all threads in the group have reached this call. 5 any any(x) 测试输入值中的任何非零值。 Test if any component of x is nonzero. 11 asdouble Reinterprets a cast value into a double. 5 asfloat asfloat(x) Convert the input type to a float. 4 asin asin(x) 返回输入值的反正弦值。 Returns the arcsine of each component of x. 11 asint asint(x) Convert the input type to an integer. 4 asuint Reinterprets the bit pattern of a 64-bit type to a uint. 5 asuint asuint(x) Convert the input type to an unsigned integer. 4 atan atan(x) 返回输入值的反正切值。 Returns the arctangent of x. 11 atan2 atan2(y, x) 返回y/x的反正切值。 Returns the arctangent of of two values (x,y). 11 ceil ceil(x) 返回大于或等于输入值的最小整数。 Returns the smallest integer which is greater than or equal to x. 11 clamp clamp(x, min, max) 把输入值限制在[min, max]范围内。 Clamps x to the range [min, max]. 11 clip clip(x) 如果输入向量中的任何元素小于0，则丢弃当前像素。 Discards the current pixel, if any component of x is less than zero. 11 cos cos(x) 返回输入值的余弦。 Returns the cosine of x. 11 cosh cosh(x) 返回输入值的双曲余弦。 Returns the hyperbolic cosine of x. 11 countbits Counts the number of bits (per component) in the input integer. 5 cross cross(x, y) 返回两个3D向量的叉积。 Returns the cross product of two 3D vectors. 11 D3DCOLORtoUBYTE4 D3DCOLORtoUBYTE4(x) Swizzles and scales components of the 4D vector xto compensate for the lack of UBYTE4 support in some hardware. 11 ddx ddx(x) 返回关于屏幕坐标x轴的偏导数。 Returns the partial derivative of x with respect to the screen-space x-coordinate. 21 ddx_coarse Computes a low precision partial derivative with respect to the screen-space x-coordinate. 5 ddx_fine Computes a high precision partial derivative with respect to the screen-space x-coordinate. 5 ddy ddy(x) 返回关于屏幕坐标y轴的偏导数。 Returns the partial derivative of x with respect to the screen-space y-coordinate. 21 ddy_coarse Computes a low precision partial derivative with respect to the screen-space y-coordinate. 5 ddy_fine Computes a high precision partial derivative with respect to the screen-space y-coordinate. 5 degrees degrees(x) 弧度到角度的转换 Converts x from radians to degrees. 11 determinant determinant(m) 返回输入矩阵的值。 Returns the determinant of the square matrix m. 11 DeviceMemoryBarrier Blocks execution of all threads in a group until all device memory accesses have been completed. 5 DeviceMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all device memory accesses have been completed and all threads in the group have reached this call. 5 distance distance(x, y) 返回两个输入点间的距离。 Returns the distance between two points. 11 dot dot(x, y) 返回两个向量的点积。 Returns the dot product of two vectors. 1 dst Calculates a distance vector. 5 EvaluateAttributeAtCentroid Evaluates at the pixel centroid. 5 EvaluateAttributeAtSample Evaluates at the indexed sample location. 5 EvaluateAttributeSnapped Evaluates at the pixel centroid with an offset. 5 exp exp(x) 返回以e为底数，输入值为指数的指数函数值。 Returns the base-e exponent. 11 exp2 exp2(x) 返回以2为底数，输入值为指数的指数函数值。 Base 2 exponent (per component). 11 f16tof32 Converts the float16 stored in the low-half of the uint to a float. 5 f32tof16 Converts an input into a float16 type. 5 faceforward faceforward(n, i, ng) 检测多边形是否位于正面。 Returns -n * sign(dot(i, ng)). 11 firstbithigh Gets the location of the first set bit starting from the highest order bit and working downward, per component. 5 firstbitlow Returns the location of the first set bit starting from the lowest order bit and working upward, per component. 5 floor floor(x) 返回小于等于x的最大整数。 Returns the greatest integer which is less than or equal to x. 11 fmod fmod(x, y) 返回a / b的浮点余数。 Returns the floating point remainder of x/y. 11 frac frac(x) 返回输入值的小数部分。 Returns the fractional part of x. 11 frexp frexp(x, exp) 返回输入值的尾数和指数 Returns the mantissa and exponent of x. 21 fwidth fwidth(x) 返回 abs(ddx(x)) + abs(ddy(x)) 。 Returns abs(ddx(x)) + abs(ddy(x)) 21 GetRenderTargetSampleCount GetRenderTargetSampleCount() Returns the number of render-target samples. 4 GetRenderTargetSamplePosition GetRenderTargetSamplePosition(x) Returns a sample position (x,y) for a given sample index. 4 GroupMemoryBarrier Blocks execution of all threads in a group until all group shared accesses have been completed. 5 GroupMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all group shared accesses have been completed and all threads in the group have reached this call. 5 InterlockedAdd Performs a guaranteed atomic add of value to the dest resource variable. 5 InterlockedAnd Performs a guaranteed atomic and. 5 InterlockedCompareExchange Atomically compares the input to the comparison value and exchanges the result. 5 InterlockedCompareStore Atomically compares the input to the comparison value. 5 InterlockedExchange Assigns value to dest and returns the original value. 5 InterlockedMax Performs a guaranteed atomic max. 5 InterlockedMin Performs a guaranteed atomic min. 5 InterlockedOr Performs a guaranteed atomic or. 5 InterlockedXor Performs a guaranteed atomic xor. 5 isfinite isfinite(x) 如果输入值为有限值则返回true，否则返回false。 Returns true if x is finite, false otherwise. 11 isinf isinf(x) 如何输入值为无限的则返回true。 Returns true if x is +INF or -INF, false otherwise. 11 isnan isnan(x) 如果输入值为NAN或QNAN则返回true。 Returns true if x is NAN or QNAN, false otherwise. 11 ldexp ldexp(x, exp) frexp的逆运算，返回 x * 2 ^ exp。 Returns x * 2exp 11 length length(v) Returns the length of the vector v. 11 lerp lerp(x, y, s) 对输入值进行插值计算。 Returns x + s(y - x). 11 lit lit(n • l, n • h, m) 返回光照向量（环境光，漫反射光，镜面高光，1）。 Returns a lighting vector (ambient, diffuse, specular, 1) 11 log log(x) 返回以e为底的对数。 Returns the base-e logarithm of x. 11 log10 log10(x) 返回以10为底的对数。 Returns the base-10 logarithm of x. 11 log2 log2(x) 返回以2为底的对数。 Returns the base-2 logarithm of x. 11 mad Performs an arithmetic multiply/add operation on three values. 5 max max(x, y) 返回两个输入值中较大的一个。 Selects the greater of x and y. 11 min min(x, y) 返回两个输入值中较小的一个。 Selects the lesser of x and y. 11 modf modf(x, out ip) 把输入值分解为整数和小数部分。 Splits the value x into fractional and integer parts. 11 mul mul(x, y) 返回输入矩阵相乘的积。 Performs matrix multiplication using x and y. 1 noise noise(x) Generates a random value using the Perlin-noise algorithm. 11 normalize normalize(x) 返回规范化的向量，定义为 x / length(x)。 Returns a normalized vector. 11 pow pow(x, y) 返回输入值的指定次幂。 Returns xy. 11 Process2DQuadTessFactorsAvg Generates the corrected tessellation factors for a quad patch. 5 Process2DQuadTessFactorsMax Generates the corrected tessellation factors for a quad patch. 5 Process2DQuadTessFactorsMin Generates the corrected tessellation factors for a quad patch. 5 ProcessIsolineTessFactors Generates the rounded tessellation factors for an isoline. 5 ProcessQuadTessFactorsAvg Generates the corrected tessellation factors for a quad patch. 5 ProcessQuadTessFactorsMax Generates the corrected tessellation factors for a quad patch. 5 ProcessQuadTessFactorsMin Generates the corrected tessellation factors for a quad patch. 5 ProcessTriTessFactorsAvg Generates the corrected tessellation factors for a tri patch. 5 ProcessTriTessFactorsMax Generates the corrected tessellation factors for a tri patch. 5 ProcessTriTessFactorsMin Generates the corrected tessellation factors for a tri patch. 5 radians radians(x) 角度到弧度的转换。 Converts x from degrees to radians. 1 rcp Calculates a fast, approximate, per-component reciprocal. 5 reflect reflect(i, n) 返回入射光线i对表面法线n的反射光线。 Returns a reflection vector. 1 refract refract(i, n, R) 返回在入射光线i，表面法线n，折射率为eta下的折射光线v。 Returns the refraction vector. 11 reversebits Reverses the order of the bits, per component. 5 round round(x) 返回最接近于输入值的整数。 Rounds x to the nearest integer 11 rsqrt rsqrt(x) 返回输入值平方根的倒数。 Returns 1 / sqrt(x) 11 saturate saturate(x) 把输入值限制到[0, 1]之间。 Clamps x to the range [0, 1] 1 sign sign(x) 计算输入值的符号。 Computes the sign of x. 11 sin sin(x) 计算输入值的正弦值。 Returns the sine of x 11 sincos sincos(x, out s, out c) 返回输入值的正弦和余弦值。 Returns the sine and cosine of x. 11 sinh sinh(x) 返回x的双曲正弦。 Returns the hyperbolic sine of x 11 smoothstep smoothstep(min, max, x) 返回一个在输入值之间平稳变化的插值。 Returns a smooth Hermite interpolation between 0 and 1. 11 sqrt sqrt(x) 返回输入值的平方根。 Square root (per component) 11 step step(a, x) 返回（x &gt;= a）? 1 : 0。 Returns (x &gt;= a) ? 1 : 0 11 tan tan(x) 返回输入值的正切值。 Returns the tangent of x 11 tanh tanh(x) Returns the hyperbolic tangent of x 11 tex1D(s, t) 1D纹理查询。 1D texture lookup. 1 tex1D(s, t, ddx, ddy) 1D texture lookup. 21 tex1Dbias tex1Dbias(s, t) 1D texture lookup with bias. 21 tex1Dgrad tex1Dgrad(s, t, ddx, ddy) 1D texture lookup with a gradient. 21 tex1Dlod tex1Dlod(s, t) 1D texture lookup with LOD. 31 tex1Dproj tex1Dproj(s, t) 1D texture lookup with projective divide. 21 tex2D(s, t) 2D纹理查询。 2D texture lookup. 11 tex2D(s, t, ddx, ddy) 2D texture lookup. 21 tex2Dbias tex2Dbias(s, t) 2D texture lookup with bias. 21 tex2Dgrad tex2Dgrad(s, t, ddx, ddy) 2D texture lookup with a gradient. 21 tex2Dlod tex2Dlod(s, t) 2D texture lookup with LOD. 3 tex2Dproj tex2Dproj(s, t) 2D texture lookup with projective divide. 21 tex3D(s, t) 3D纹理查询。 3D texture lookup. 11 tex3D(s, t, ddx, ddy) 3D texture lookup. 21 tex3Dbias tex3Dbias(s, t) 3D texture lookup with bias. 21 tex3Dgrad tex3Dgrad(s, t, ddx, ddy) 3D texture lookup with a gradient. 21 tex3Dlod tex3Dlod(s, t) 3D texture lookup with LOD. 31 tex3Dproj tex3Dproj(s, t) 3D texture lookup with projective divide. 21 texCUBE(s, t) 立方纹理查询。 Cube texture lookup. 11 texCUBE(s, t, ddx, ddy) Cube texture lookup. 21 texCUBEbias texCUBEbias(s, t) Cube texture lookup with bias. 21 texCUBEgrad texCUBEgrad(s, t, ddx, ddy) Cube texture lookup with a gradient. 21 texCUBElod Cube texture lookup with LOD. 31 texCUBEproj texCUBEproj(s, t) Cube texture lookup with projective divide. 21 transpose transpose(m) 返回输入矩阵的转置。 Returns the transpose of the matrix m. 1 trunc trunc(x) Truncates floating-point value(s) to integer value(s) 1 参考链接 每天30分钟看Shader—(1)HLSL固有函数 【Intrinsic Functions (DirectX HLSL)】]]></content>
      <categories>
        <category>shader</category>
      </categories>
      <tags>
        <tag>shader</tag>
      </tags>
  </entry>
</search>
