<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[武侠动作手游初稿(个人向)]]></title>
    <url>%2F2018%2F02%2F09%2Fwuxia-act-mobile-game%2F</url>
    <content type="text"><![CDATA[假日里闲来无事，回顾了一下自己掌握的知识，根据自己已有的技术储备，整理了一下开发一款武侠动作手游的方方面面。我并非专业策划，写下这篇文章，依据的是之前的开发经验和阅历，以及体验过的一些游戏，难免会有一些不合理之处，这些还恳请小伙伴们多多指教。本文并非策划文档，也不详谈技术，大致相当于一个大纲，对游戏的开发做一个总览，需要做些什么，目前能做到什么样的程度，心里先有一个底。 题材和类型选择武侠这题材，一方面自己喜欢武侠，并且我对武侠可以说是相当的熟悉了，从小到大看过的武侠小说数不胜数，容易驾驭，比较稳，同时，能够获取到的参考和素材也相当的多；另一方面，个人认为武侠这个题材的市场还是比较广的，这从国内游戏的题材分布可以看出。 选择偏动作的游戏类型，个人认为，一个优秀的即时战斗系统才可以比较好的表现出武侠里的刀光剑影、快意恩仇；自己也比较喜欢偏动作的游戏，开发3d Act一直是自己的夙愿和执念，嘿嘿。 世界观先定下世界观设定，然后确定大致的编年史，然后定下故事线。 目前我初步是这么设定的，游戏发生在一个架空的时代，时间点在唐朝之后，大概是五代时期。当然既然架空了，和历史肯定是不太一样的，这样设定的原因，一方面是对五代历史了解的人不多，方便胡乱瞎编，另一方面，五代之后的宋朝是武侠小说非常集中的时代，可以方便剧情的后续扩展。 故事的起点是一次陨石坠落事件，陨石坠落事件带来了一种非常奇特的物质，在后续的剧情里，这种物质被发现是可以再生的。它的作用有这么几种：1. 赋予武侠中的内力更多的属性，这个可以参考《沧海》里的周流六劲，或者是一些超能力战斗漫画里的设定。 2. 充当游戏世界里的动力能源，甚至可以让它取代蒸汽机的作用。 3. 让游戏世界变得更加奇幻，但又说得通。 文案方面，个人比较喜欢流畅一些的文笔，例如马舸的文风： 二人沿山道下行，走了多时，来到一处涧桥边。那老僧眼望仙山幽美，恍如幻境，忽停步一叹，语含失望道：“我来此山，本欲一会当世‘真人’，可惜张全一修为虽高，却只能算半个‘真人’。”尚瑞生心中诧异，问道：“难道老师还当不得‘真人’二字？”那老僧摇了摇头，举目望天道：“果‘真人’者，无虚妄，无偶像，傲立天地，看破生死，蔑笑神佛。知‘因果’之无稽，洞人智之有穷。不悲不狂，永爱人生之风景；大真大痴，唯珍一世之运命。此才是人的生涯，可惜这人我看不到了！你有这样的后人，虽死亦如永生，连老衲也要羡慕了。”（《幻真缘》） 温婉一点的可以这样： 想到这一节，我心中已经雪亮，梁凉早就算好这一扑，我刚才岳阳楼吃饭喝茶动武，都是惯用右手，若用左手一定慢了一点而且不灵活，所以刚才故意站在左边，让右侧的我只能出左手，已是先赢了七分，更何况我生性喜洁，随随便便扑在地上，心里有所犹豫，出手自然又慢了些许，这也不能算他作弊，倒是我吃了哑巴亏。（《横吹洞萧》） 不过具体的实施还是得看情况。 技术选型客户端引擎使用Unity，开发方便，资源丰富，易于扩展，跨平台容易，好处是显而易见的。将来web端如果真崛起了，那时用的应该是webassembly之类的技术，显然Unity是可以支持的。 游戏服务器采用C#语言，前后端都使用ET框架，分布式和组件式的设计十分适合本类型游戏。自己也已经实现了在C#中嵌入CPython，对于需要使用Python做算法的模块，可以直接用C#封装成组件并挂载到游戏服务器上。 游戏服务器的web控制台，支付服务器，辅助服务器，分析工具，各种脚本，全部使用Python编写，毕竟在小型项目里，Python的开发效率无与伦比。 如果存在性能压力比较大的地方，就使用C/C++实现，再导出到C#或Python中。 开发规划开发分三个阶段吧：原型 -&gt; 初版 -&gt; 版本迭代 原型这是项目的开荒阶段，也是最艰苦的阶段。在我看来，要完成以下几点： 定下渲染技术，确定基本的画面基调，准备好场景和角色需要用到的范例材质和着色器。 制作好至少一个通用角色，基本上要包含80%的角色特性，头发，布料，绑定，物理等都要弄好。 制作好敌兵的各式基本AI行为。 制作好基本的技能系统和战斗系统，需要支持pvp的1v1和pve。 前后端同步进行，ET框架比较好的一点在于，某种程度上是开箱即用，全栈开发，前后端无缝结合。 由于只是技术验证，模型和动作等使用网上流传的资源就可以了。先做出一个可玩的原型，边玩边体验边调整，逐步迭代，C#是强类型语言，重构起来也是十分轻松愉快的。原型阶段尤为重要，毕竟即时制战斗，口说无凭，只有体验了，才可以定下接下来的调整方向。这个没做好，后续的无从谈起。 虽说这类游戏的demo、源码一大把，其实含金量大多数都不咋地，要做出自己的特色不那么容易，想做成自己心目中的完美效果更难，还是要有一定的妥协的，看自己的权衡和取舍吧。 初版这是第一个上线的版本。在原型的基础上，丰富各项游戏功能，达到一个比较高的完成度。这时也要避免同质化的问题，找到自己的定位和特色。 版本迭代初版的表现不错的话，那么就要开始不断更新版本了。至少我自己就挺喜欢这类游戏的，成绩不好肯定有原因，完成度不高？画面不好？体验不好？表现力不行？战斗不爽快？数值不合理？难度不合理？卡顿？粘度太差？出现新游戏了？。。。总之，抱着做ip的心，万一成了呢，这个世界观，我觉得不错的。 画面风格画面采用卡通风格，使用 cel(赛璐璐) 渲染技术。目前市场上的崩坏3、桃源乡等二次元手游采用的都是这种风格，如图： 采用这种画面风格，一方面这种画面确实好看，另一方面，避免同质化，避开和大厂的竞争。 我已经研究过一段时间这种渲染技术，花了不少精力收集这方面的资料：公开的技术分享，技术博客，shader源码，paper，youtube视频，工程文件等。 总体来说，实现这种卡通渲染不算难，网上的工程一大把，但这种非真实渲染技术太过tricky，画面好坏的评判主要依据的是人的主观，需要根据具体的项目进行针对性开发。 毕竟画面是游戏的脸面，在我对画面的需求中，想要线条更光滑一点，而不是常见到的多边形状，勾线更细腻一点，线条更多一点，高光和阴影的层数更多一点，层次感更强一点，漫画风的感官更强烈一点，指不定还得加上后期(post process)，同时还得兼顾到手机的性能消耗。要达成这些，还需要花时间来研究。 至于画风这方面，使用真人比例，《刀剑异闻录》这部漫画我比较喜欢（如下图），原画方面可以试试在米大师约稿。 其实如果不是非得要按自己的想法来，现在已经有这么几个现存的方案： Asset Store的RealToon (PC/MAC &amp; Mobile)，效果如下： Pencil+ 4 for unity，这个是pencil+的unity版，价格小贵，购买起来还比较麻烦，实际的性能消耗也不清楚，好像国内还没有谁在用。 Unity Chan Toon Shader，这个手头已有源码，效果如下： 总之，做的时候，看看同类产品到了什么样的水平，既不能掉队，也最好不要在细节上死磕。 用户界面用户界面主体上使用简约风格，可以参考国外单机游戏。 下面这个是刺客信条ios版，可以看看：http://www.bilibili.com/video/av19424264/下面这个的角色控制做得比较不错http://www.bilibili.com/video/av19424408/ 主界面按钮太多时，可以做一个一级按钮和一个快速按钮，按下一级按钮时呼出环形菜单，然后点击里面的按钮，进行相应操作，快速按钮自动切换成上一次点击的二级按钮，方便再次点击上次点击的按钮。 下图是一个环形菜单的例子： 提示系统需要做好一点，刺客信条ios版做得就比较好，根据当前的状态和环境，自动弹出提示和按钮。我甚至觉得吃药等类似操作的按钮都可以做成根据状态自动显示和隐藏。 角色的标记，卡片的流光，技能按钮的cd，红蓝球/条等细节效果，基本上都找得到实现，无需太多担心。 场景的切换需要有Camera Fade效果，UI界面需要有进场和退场的效果。UI界面在实现时会大量使用行为树，将各种效果做成行为节点，这样不用写一行代码，只需连接行为节点和修改节点参数，就可以轻松的调整UI了，极大的提升效率，如下图： 技能按钮暂时先参考手游通行的摆放方式，跟随战斗系统的开发，不断调整，看是使用双摇杆还是多级按钮啥的。 本游戏的定位不是mmo，至少在初版不是，没有大世界，进入游戏后，玩家在一个屋子里，可以在屋子里活动，屋子里的一些物件，例如炉子，门，窗子等代表一些玩法的入口，与这些入口交互后，可进入相应的玩法。 手头收集有一些UI界面，可以参考参考： 捏脸系统对于玩家自己创建的角色来说，捏脸和换装还是比较必要的。 我手头有这么几个demo： 这几个demo虽然简陋，但总好过自己从零起步。将这几个demo整合起来，并根据自己的需求，进行开发，最终实现自己比较满意的效果。 如果后期追求更高品质的捏脸系统，可以参考天刀的技术分享： 千人千面如何炼成 技术讲解捏脸系统设计原理 天涯明月刀之千人千面 手头有一个提取出来的天刀角色模型，面部的骨骼保留得比较完整，可以参考一下。 城建本游戏在初版里，场景只有一个主城，所有的剧情和玩法都发生在这个城池里。这也是为了降低成本，这些建筑的模型资源其实比较丰富的，并且也比较容易获取到，由于是卡通渲染，纹理贴图都简单了很多，如果要外包，成本应该不高。就算自己做也不算难，MC里那些玩家做出的建筑群就可以说明。建筑的很多部分都是可以程序化创建的，使用少量模板，通过blender的Python编程，可以创建出大片建筑。 后期可能会做在整个城池内进行的多人玩法，比如夺宝玩法、吃鸡玩法啥的。因此在城建设计之初需要考虑一下随机地形和场景破坏，留下后续升级改进的空间。 城池的设计上，可以参考这些：1. 妖猫传里的唐城。 2. 刀剑异闻录里的建筑设计。 3. 神都洛阳的微缩模型。 4. 一些概念设计。 5. 。。。。 植被模型优先到asset store里找找，应该可以满足绝大多数需求，如果找不到合适的，就到网上找，实在找不到合适的，再考虑外包。 场景里的一些细小物件，如箱子，幡子等，获取方式同上。 我手里有几个高精度的古建模型，应该可以作为参考，以及程序化的模板： 酒馆其实就是扭蛋啦。至于抽啥，还没想好，有这么几个备选项： 装备 道具材料 武学 高手榜上的角色。美名其曰：广交豪杰，慕名拜访。抽到后有这么几个用法： 战斗时可以召唤出来，助战一段时间（有时间限制，到点后就消失）。 主城多人玩法里可能会用到，利用这些高手角色的特质，完成一些任务。这个具体如何实施还没想好。 剧情副本剧情关卡剧情关卡算是一个比较重要的拖游戏时长的功能。 据我所知，在国内的手游中，只有最近出来的大厂的重型MMORPG（例如楚留香）才有接近端游的剧情演出，其他的手游的剧情表现形式基本上就是对话框，或者是对话框的变种。本游戏的剧情表现形式自然就是对话框类型，其实这个对话框也是可以做一些微创新的，我在Steam上就看到了不少有新意的对话框形式。 就我的开发经验来看，这个剧情关卡的品质要做到普通手游的水平，并不复杂。首先，剧情部分自然就是一张Excel表了。所有剧情关卡的主配置也是一张Excel表，里面填写关卡相关的各项控制参数，例如地图配置，玩家配置，敌人配置，Boss配置，剧情配置等等。如果需要对剧情关卡做更精细的配置，那就需要做一个战斗编辑器，星际2的银河编辑器是这其中的佼佼者，不过对于本游戏来说，倒不需要这么复杂的编辑器，能够做一定程度的配置就可以了。 副本在我看来，多人副本能够极大的加强玩家之间的交流，做好了可以给游戏加分不少。多人副本的寿命也是比较长的，至少在端游上，有些游戏的的单个大型副本可以维持大几个月。 武侠小说中的阵法和斗阵给了我不少灵感，其中一个印象比较深刻的是在萧逸的《无忧公主》中，几个阵法和斗阵比较不错，糅合真实空间和心理幻想空间，在真实世界和心理世界不断切换，心理世界里，空间不断地在撕裂（我脑补出了新鬼泣的画面，哈哈），比较出彩，可以参考参考，做成多人副本。 战斗系统战斗系统是一个十分复杂的系统，这方面的理论与实践不是几篇文章就说得完的。这里不详述技能的设计和技术上的细节，只说说要做成什么样，以及从哪方面做。 做成什么样首先定一个基调：本游戏的战斗系统需要在pvp的1v1和pve上有比较好的表现。这种做法虽然和当前的一些mmorpg太像了，但也说明了这种模式比较符合市场，当然还是要有自己特色的战斗乐趣的。 原型开发时，可以直接仿制天刀楚留香的战斗系统，等原型出来了之后，再来尝试将act游戏的一些特性引入到战斗系统中来。 先做剑，刀，鞭，镖这四种武器和技能，相关的灵感从影片、游戏、霹雳、书籍图谱等方面获取。 剑无声团队的访谈可以听听：摸索·成长 -《剑无生》开发记 从哪方面做技能的制作方面，个人感觉应该要用行为树来做比较好。当然了，一些数值还是要填表的。具体如何实施，并取得比较好的效果，得亲自实践。 智能体(Agent)的AI行为，手头有如下demo：到达逃跑累积躲藏干预非渗透约束避障有偏差的追逐路径跟随追逐寻找徘徊模拟海滩里的鱼群地图的网格导航 服务器与客户端的网络同步方面，反复看了网上关于状态同步和帧同步的讨论，考虑到玩法的扩展性，感觉以状态同步为主比较好，为了优化体验，可以掺杂一些帧同步的理念在里头，有点类似于当年做的战斗本地化的超级加强版。由于客户端和服务端使用同一种语言，同一种框架，代码比较容易共享，因此使用这种方案做起来应该是可行的。 相比较于ET框架，kbengine实现的业务逻辑更多一些，可以学习一下kbengine里一些业务逻辑的实现思路。 网络上关于战斗系统和技能设计的资料非常多，这里就不一一列举了。 下面是手里的几个demo： 导表在游戏的开发过程中，导表功能必不可少。传统的做法是，按照预先约定好的规则填写Excel表格，然后将Excel表格解析成游戏能够读取的数据文件例如csv，或者直接生成代码文件，将表格写到代码里。这个我已经用Python实现了，并且支持Excel表格里数据的无限嵌套。 目前我已经实现了在Unity Editor中嵌入CPython。本项目中需要对导表流程进行改进，具体如下： 按照预先约定好的规则填写Excel表格，表格用svn管理，放在Unity Asset文件夹外面。 Unity Editor调用Python解析Excel表格，生成Unity能够方便读取的数据文件。 Unity Editor读取数据文件，并根据表格数据生GUI界面。 在这些GUI界面上，可以可视化调整数据，所见即所得，实时反馈在游戏窗口。 点击保存按钮，将数据覆写回Excel文件，也不用担心覆写出错，毕竟版本管理嘛。 由于客户端和服务端共用数据代码，直接在Unity Editor的GUI中调数据的好处是显而易见的，连服务器的填表工具都省了。 下图是是Unity Editor里导表数据编辑界面的大概模样，最终成品要以实物为准： 下图是Excel导表的大概模样，只是演示一下表格结构，实际的数据参数要复杂一些。表格里的内容是我胡乱填的，与本游戏无关。道具导表装备导表新手指引导表 常用模块这里简单列一下游戏里常见的功能模块，看情况进行取舍吧： 玩家名称生成 改名 角色信息 任务 邮件 道具的合成分解与进化 装备的合成分解与进化 成就 声望 帮会 阵营 家园 好友 师徒 。。。 各模块之间需要合理的安排跳转链接。 版本迭代时，可以开发一些自由度比较高的玩法，比如参考Besiege的设计理念，不过这个是后话了。 日程表在一周的各天安排不同的活动，各天产出不同的材料，过期不补，这些材料是装备升级和角色提升所必需的。算是一种运营策略吧，提高留存。 小活动做一些小游戏，当做调味剂吧，暂时列举这些： 猜拳 答题 打地鼠 连连看 五子棋 聊天语音功能，这个比较纠结，大厂的游戏中一般集成了这个功能，打本的时候比较重要，直接语音转文字，十分方便。如果要做，使用云平台的服务的话，肯定不划算，看看有没有变通的方案吧。 聊天系统除了传统的功能，例如聊天，发装备链接等，可不可以微创新？例如： 游戏内发红包，当然使用的是游戏内的货币啦。 可否与微信关联？玩家A通过游戏内的聊天系统向离线的玩家B发送消息，消息被传递到玩家B的微信上。 社交支持个人主页这个功能，然后支持留言。我更进一步的构思是这样的，搞一个游戏内的微博系统，玩家可以在里面推文，然后关注列表，动态推送，博主推荐，热门博主等功能也是一个不少。 助手系统关于新手引导，UI系统在设计的时候就要考虑引导层，强制式的，触发式的等等。 游戏内需要做一个助手系统，给我印象最深的软件助手就是office2003的小别针。不过在本游戏中，助手的形象是一个人形角色，可以替换成不同的形象，与这个助手可以进行许多交互，例如获取帮助文档，打开教学指引，开启活动等等。 助手系统最核心的应该就是聊天机器人，这个在GitHub上有很多开源实现，基本上都是Python实现的，应该可以用C#包装后挂载到游戏服务器上。 这个助手的部分功能，是否可以只对月卡用户开放呢？可以考虑考虑。 机器学习本游戏还是以传统的游戏AI为主。但如果某些功能或特性，使用机器学习可以获得更好的效果，倒也可以试试。 下面这个demo就是使用神经网络模拟四足动物倒地后的效果：可以想象，利用这种技术可以让敌兵的受击反应更具表现力。 总之，还是要看具体场景，判断一下是否真的需要这种技术？是否真的有用？有没有简单的替代方案？然后再确定方案。 资源规划音效暂时就拿来主义吧。 音乐外包应该是按时长计价的吧，看情况。 原画米大师约稿。 模型先看看模型库里有没有合适的，贴图也根据情况判断是否需要外包，数量不会太多，一般在项目尾声换皮。 动画对于手游这种级别的动画，模型绑定和key帧，还真不一定要外包，自己就可以搞定，这类的教程youtube上数不胜数，也没啥难的，就是个体力活，真正的难点在于动作设计。 对于本游戏，动画主要来自以下几个途径： Asset Store里获取。 使用别人做好的动作库，在Blender中编写Python脚本，将这些动作应用到模型上。 对其他游戏进行录屏，逐帧捕获动作画面，作为参考导入到Blender，然后对着这些参考图，摆pose，key帧。 从一些影片截取动作画面，从一些武术教学图谱制作故事版，作为参考导入到Blender，然后对着这些参考图，摆pose，key帧。 如果觉得做出的动画有些别扭，可以考虑使用差值或拟合等数学方法对动画数据进行处理，再看看效果。 特效 Asset Store里获取。 自己搞定，这方面的教程太多了，如果纹理贴图实在找不到自己想要的，自己做的效果又不太好，就把纹理贴图外包吧。 服务器控制台如果游戏上线了，服务器OP控制台还是比较重要的，不过这个不必一下子全部做完，慢慢来就可以了。 Web控制台使用Python开发。kbengine已经有实现了，可以拿来参考参考。 测试游戏开发的测试过程中，通过GM指令修改游戏数据的功能必不可少： 方便快速的的获取log也是需要考虑的。 报错的邮件推送，可以看情况决定做不做。 渠道接入游戏Icon，我个人偏向手绘风格，参考图如下： sdk的接入方面可以参考u8sdk，不过没必要那么复杂，优先考虑主流的渠道吧。 支付服务器使用Python开发，这个自然不在话下。 如果还有精力，可以做一下用户画像。 特色点与竞争力最近出的重型mmorpg武侠手游楚留香，很多方面都接近端游的水平了，如此高的品质，必然会挤压同题材手游的生存空间。如何避免与这些大佬重叠，以及同质化，做出自己独有的特色，是需要不断思考的。 本游戏的定位并不是严格意义的mmorpg，没有大世界，算是一个中型游戏。二次元卡通风格的画面算是本游戏的一个特色。如果战斗系统做出了自己的味道，那也算是一个特色。再一个就是做出了比较新奇的玩法。亦或者是剧情和原画爆发了，创造出了广受喜爱的角色。但这些到底有多大作用，还真不清楚。 我个人认为，游戏和社交软件还是有区别的。某个社交软件强大后，其他的同类产品估计都没活路。游戏对社会的渗透是没有社交软件深的，并且产品性质也不太一样，虹吸效应远不及社交软件，就算出现了一家独大的全民游戏，但是各人有各人的胃口和爱好，其余的游戏应该还是有一定生存空间的。]]></content>
      <categories>
        <category>game</category>
      </categories>
      <tags>
        <tag>game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ET框架客户端源码阅读]]></title>
    <url>%2F2018%2F01%2F05%2Fread-et-framework-client-code%2F</url>
    <content type="text"><![CDATA[ET是一个unity的前后端框架，包含客户端和服务端两部分。采用的是组件式设计，前后端都由C#实现，并且前后端共享了部分代码，可以很方便的进行全栈开发。客户端方面的功能也比较齐全，基本包含了手游开发中使用频率比较高的各个功能。前后端都非常完整地实现了热更新，热更新机制采用的是ILRuntime，即把要热更新的C#代码当做脚本来进行热更，终于可以不用使用讨厌鬼lua了ヽ(￣▽￣)ﾉ。如果手头没有代码积累，ET框架是一个非常不错的选择。这个框架很对我胃口，业余抽空先看了一下客户端部分，感觉设计得挺精妙的，这里梳理一下，方便理解，以免使用该框架时没有头绪。另外也感慨一下，C#确实好用，语法很甜，使用起来，舒适度和Python差不多，同时又是强类型的，效率也不错，开发起来简直是一种享受。 修复工程文件的bug客户端的Hotfix工程可能存在找不到UnityEngine库的bug。这时需要修改一下ET/Unity/Hotfix/Unity.Hotfix.csproj文件。对照Unity.csproj的库引用部分的配置，修改Unity.Hotfix.csproj工程的引用配置。例如将Unity.Hotfix.csproj的123456&lt;Reference Include="UnityEngine, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null"&gt; &lt;HintPath&gt;C:\Apps\Unity\Editor\Data\Managed\UnityEngine.dll&lt;/HintPath&gt;&lt;/Reference&gt;&lt;Reference Include="UnityEngine.UI, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null"&gt; &lt;HintPath&gt;C:\Apps\Unity\Editor\Data\UnityExtensions\Unity\GUISystem\UnityEngine.UI.dll&lt;/HintPath&gt;&lt;/Reference&gt; 改成123456789&lt;Reference Include="UnityEngine"&gt; &lt;HintPath&gt;D:/Program Files/Unity/Editor/Data/Managed/UnityEngine/UnityEngine.dll&lt;/HintPath&gt;&lt;/Reference&gt;&lt;Reference Include="UnityEngine.CoreModule"&gt; &lt;HintPath&gt;D:/Program Files/Unity/Editor/Data/Managed/UnityEngine/UnityEngine.CoreModule.dll&lt;/HintPath&gt;&lt;/Reference&gt;&lt;Reference Include="UnityEngine.UI"&gt; &lt;HintPath&gt;D:/Program Files/Unity/Editor/Data/UnityExtensions/Unity/GUISystem/UnityEngine.UI.dll&lt;/HintPath&gt;&lt;/Reference&gt; 阅读ET客户端源码记录一下自己的阅读过程，以便将来阅读陌生项目时，有经验可供参考，并改进阅读方法。 查看项目的文档。 查看项目结构，文件及文件夹布局，凭经验粗略判断了一下核心文件，翻了一下主要文件的代码。 运行了几次程序，看一下有哪些功能，了解一下程序长什么样。 开始调试源码，跟随程序的运行，一步一步的跟，对程序的运行流程有一个初步的印象。由于是C#编写的，并且ide使用的是vs2017，在调试信息中经常可以看到相关连的类或结构的名字，甚至是代码文件的路径，复制变量的值，借助于vs2017的转到文件、转到符号等功能，可以迅速定位到相关位置。 开始详细阅读主流程代码，关键点做下记录，以备将来查阅。 客户端demoET框架的客户端demo只有一个Init.unity场景，里面的内容如下图Init.cs就挂在名为Global的GameObject上，Init.cs是游戏逻辑的入口。 启动服务器后，同时开2个客户端时，屏幕中会出现2个小人，开始时是重叠在一起的，右键可以移动小人，观察帧同步的效果。在实际测试中发现，运动出现不同步的频率还是有点高的，实际使用时需要优化。 主要的类这里列出了一些比较重要的类以及方法的说明，相当于一个大纲，方便快速理解框架结构。这里整理出的内容仅供参考，因为代码在不断变更，相关的类的功能也在变化。 DllHelper路径：Scripts/helper/DllHelper.cs static class DllHelper + static void/Assembly LoadHotfixAssembly ILRuntime模式 将Hotfix.dll和Hotfix.pdb读取到byte数组, 使用Init.Instance.AppDomain.LoadAssembly载入 普通模式 将Hotfix.dll和Hotfix.mdb读取到byte数组, 使用Assembly.Load加载成Assembly对象并返回 + static Type[] GetMonoTypes() List&lt;Type&gt; types = new List&lt;Type&gt;(); foreach (Assembly assembly in ObjectEvents.Instance.GetAll()) { types.AddRange(assembly.GetTypes()); } return types.ToArray(); + static Type[] GetHotfixTypes() ILRuntime模式 return appDomain.LoadedTypes.Values.Select(x =&gt; x.ReflectionType).ToArray(); 普通模式 return ObjectEvents.Instance.HotfixAssembly.GetTypes() ObjectEvents路径：Scripts/base/object/ObjectEvents.cs interface IObjectEvent + Type Type() + void Set(object value) abstract class ObjectEvent&lt;T&gt; : IObjectEvent - T value + T Get() + Set(object v) 设置value + Type Type() sealed class ObjectEvents - static ObjectEvents instance 单例 - Assembly hotfixAssembly - Dictionary&lt;string, Assembly&gt; assemblies - Dictionary&lt;Type, IObjectEvent&gt; disposerEvents - EQueue&lt;Disposer&gt; updates, updates2, starts, loaders, loaders2, lateUpdates, lateUpdates2 + void LoadHotfixDll() ILRuntime模式 DllHelper.LoadHotfixAssembly载入Hotfix.dll 普通模式 hotfixAssembly引用DllHelper.LoadHotfixAssembly()返回的Assembly对象 this.Load(); + void Add(string name, Assembly assembly) assemblies[name] = assembly; 遍历assemblies里的assembly的所有type, 如果具有ObjectEventAttribute特性, 且继承了IObjectEvent, 就创建type的实例objectEvent, 并添加进disposerEvents，即disposerEvents[objectEvent.Type()] = objectEvent; + Assembly Get(string name) return this.assemblies[name]; + Assembly[] GetAll() return this.assemblies.Values.ToArray(); + void Add(Disposer disposer) 通过disposer的type从disposerEvents中获取objectEvent， 如果是ILoad, 就添加进loaders 如果是IUpdate, 就添加进updates 如果是IStart, 就添加进starts + void Awake(Disposer disposer) 有泛型版本，用于扩展传到disposer的参数个数，但逻辑是一致的 Add(disposer) 通过disposer的type从disposerEvents中获取objectEvent， IAwake iAwake = objectEvent as IAwake; objectEvent.Set(disposer); iAwake.Awake(); objectEvent相当于是一个包装器, disposer是具体做这件事的对象，objectEvent调用disposer的Awake, 继承接口的是objectEvent, disposer对objectEvent是已知的, disposer不用继承任何IAwake之类的接口，简化disposer的继承 + void Load() 从loaders弹出一个disposer，通过disposer的type从disposerEvents中获取objectEvent，再将disposer添加到loaders2， ILoad iLoad = objectEvent as ILoad; objectEvent.Set(disposer); iLoad.Load(); --&gt; 通过objectEvent调用disposer里的方法， 重复以上操作直到loaders里的弹完为止 loaders和loaders2对调 + void Start() 从starts里弹出disposer 通过objectEvent调用disposer里的Start方法 流程和Load()差不多，只是从starts里弹出disposer没有被回收。 + void Update() 类似Load() + void LateUpdate() 类似Load() Disposer路径：Scripts/base/object/Disposer.cs abstract class Object: ISupportInitialize + virtual void BeginInit() + virtual void EndInit() [BsonKnownTypes(typeof(Component))] abstract class Disposer : Object, IDisposable - long Id - bool IsFromPool + Disposer() + Disposer(long id) + virtual void Dispose() Component路径：Scripts/Base/Object/Component.cs [BsonIgnoreExtraElements] abstract partial class Component: Disposer - Entity Parent + Component() + T GetParent&lt;T&gt;() where T : Entity + override void Dispose() 路径：Scripts/Base/Object/ComponentAttribute.cs [BsonKnownTypes(typeof(AConfigComponent))] [BsonKnownTypes(typeof(Entity))] partial class Component ComponentFactory路径：Scripts/Base/Object/ComponentFactory.cs static class ComponentFactory + Create 一组泛型函数， 从对象池里创建组件，并通过ObjectEvents.Instance.Awake(disposer);来Awake组件 这是一组泛型函数，支持传入多个参数给组件的Awake函数 EventComponent路径：Scripts/component/EventComponent.cs interface IEvent + Run：一组泛型函数，支持最多6个参数，无返回值 interface IEventMethod + Run：一组泛型函数，支持最多4个参数，无返回值 class IEventMonoMethod : IEventMethod - object obj + IEventMonoMethod(object obj) 初始化时引用一个IEvent对象obj， + Run：一组泛型函数，支持最多4个参数，无返回值，让obj执行IEvent的run方法 class IEventILMethod : IEventMethod - ILRuntime.Runtime.Enviorment.AppDomain appDomain - ILTypeInstance instance - IMethod method --&gt; ILRuntime貌似只能通过这种方式执行函数 - object[] param + IEventILMethod(Type type, string methodName) 使用AppDomain获取将要调用的方法，同时appDomain.Instantiate(type.FullName)创建一个实例并赋值给instance， 并从type获取到method 注： 创建IEventMonoMethod和IEventILMethod的地方，在EventComponent的Load方法里， 在Load里是直接通过类(继承了IEvent)来创建实例的，只不过IEventMonoMethod是在构造函数外面创建IEvent实例， IEventILMethod是在构造函数里面创建IEvent实例，感觉都在构造函数里面创建IEvent实例比较好，这样统一一些，不会那么奇怪 + Run：一组泛型函数，支持最多4个参数，无返回值 让obj执行IEvent的run方法，内部是通过appdomain Invoke方法，实现方法的调用 class EventComponentEvent : ObjectEvent&lt;EventComponent&gt;, IAwake, ILoad + void Awake() 调用EventComponent的Awake函数 + void Load() 调用EventComponent的Load函数 class EventComponent : Component - EventComponent Instance 单例 - Dictionary&lt;EventIdType, List&lt;IEventMethod&gt;&gt; allEvents + Awake Instance = this; this.Load(); + Load 新建allEvents 通过DllHelper.GetMonoTypes获取types列表，遍历types，获取有EventAttribute特性的type， 从type获取到的EventAttribute为aEventAttribute，创建该type的实例obj，然后添加到allEvents， 即allEvents[(EventIdType)aEventAttribute.Type].Add(new IEventMonoMethod(obj)); 感觉有点问题，要是type没继承IEvent，无法静态检查出来，运行时就gg了 ￣ω￣= 通过DllHelper.GetHotfixTypes获取types列表，重复上面的过程， 不同之处在于，在ILRuntime模式下，创建IEventILMethod而不是IEventMonoMethod + Run(EventIdType type, ...)：一组泛型函数，支持最多3个参数，无返回值 从allEvents根据EventIdType取出IEventMethod列表，然后执行列表里的IEventMethod对象的Run方法 因为各Component基本上都有对应的ComponentEvent，ComponentEvent的功能基本上就是调用Component的Awake,Start,Update等方法，少数ComponentEvent可能有一些额外的功能。以下部分，如果XXXComponentEvent没有特殊功能，就略过不表了。 UIComponent路径：Scripts/Other/UIFactoryAttribute.cs [AttributeUsage(AttributeTargets.Class)] class UIFactoryAttribute: Attribute - int Type --&gt; 枚举值 + UIFactoryAttribute(int type) 路径：Scripts/component/UIComponent.cs class class UIComponent: Component - GameObject Root --&gt; ui的根节点 - Dictionary&lt;UIType, IUIFactory&gt; UiTypes - Dictionary&lt;UIType, UI&gt; uis + void Awake() this.Root = GameObject.Find(&quot;Global/UI/&quot;); this.Load(); + void Load() 清空UiTypes， 遍历DllHelper.GetMonoTypes()获取到的types列表，获取有UIFactoryAttribute特性的type，创建type的实例factory， 添加到UiTypes，即this.UiTypes.Add((UIType)attribute.Type, factory); attribute为从type获取的UIFactoryAttribute特性 + UI Create(UIType type) 从UiTypes中取出type类型的factory，创建出UI类型实体ui，并将ui添加到uis列表中。 // 设置canvas string cavasName = ui.GameObject.GetComponent&lt;CanvasConfig&gt;().CanvasName; ui.GameObject.transform.SetParent(this.Root.Get&lt;GameObject&gt;(cavasName).transform, false); + void Dispose() 遍历uis并Dispose，清空uis和UiTypes + void Add(UIType type, UI ui) + void Remove(UIType type) + void RemoveAll() + UI Get(UIType type) + List&lt;UIType&gt; GetUITypeList() IUIFactory路径：Scripts/Other/IUIFactory.cs interface IUIFactory + UI Create(Scene scene, UIType type, GameObject parent); + void Remove(UIType type); UILoadingFactory路径：Scripts/UI/UILoading/Factory/UILoadingFactory.cs class UILoadingFactory : IUIFactory + UI Create(Scene scene, UIType type, GameObject gameObject) 这里只创建了加载界面， 资源在Resources/KV.prefab 从UnityEngine.Resources.Load(&#39;KV&#39;).Get&lt;GameObject&gt;(&quot;UILoading&quot;);取出界面 UnityEngine.Object.Instantiate 实例化界面得到对象go 设置go的层 使用go创建UI类型的实体ui 给ui添加UILoadingComponent组件 + void Remove(UIType type) 空 UILoadingComponent路径：Scripts/UI/UILoading/Component/UILoadingComponent.cs UILoadingComponentEvent : ObjectEvent&lt;UILoadingComponent&gt;, IAwake, IStart + void Awake() 获取界面的Text控件的引用，赋值给UILoadingComponent的text + async void Start() 在一个while循环中，不断更新UILoadingComponent的text，更新下载进度的文字显示，直到下载完成为止。 class UILoadingComponent : Component - Text text UI路径：Scripts/Entity/UI.cs class UIEvent : ObjectEvent&lt;UI&gt;, IAwake&lt;Scene, UI, GameObject&gt; + void Awake(Scene scene, UI parent, GameObject gameObject) sealed class UI: Entity - Scene Scene - string Name --&gt; this.GameObject.name - GameObject GameObject - Dictionary&lt;string, UI&gt; children + void Awake(Scene scene, UI parent, GameObject gameObject) 清空children 设置Scene和GameObject gameObject.transform.SetParent(parent.GameObject.transform, false) + void Dispose() 遍历children进行Dispose UnityEngine.Object.Destroy(GameObject); 清空children + void Add(UI ui) + void Remove(string name) + UI Get(string name) ResourcesComponent路径：Scripts/component/ResourcesComponent.cs Opcodeenum Opcode 一组网络协议号 MessageAttribute粗略观察了一下，有MessageAttribute的type基本上都是协议包的结构体 class MessageAttribute: Attribute - Opcode Opcode OpcodeTypeComponentclass OpcodeTypeComponent : Component - DoubleMap&lt;Opcode, Type&gt; opcodeTypes + void Awake() 遍历DllHelper.GetMonoTypes()，取出有MessageAttribute的type,添加到opcodeTypes GlobalConfigComponentclass GlobalConfigComponent : Component - GlobalConfigComponent Instance 单例 - GlobalProto GlobalProto + void Awake() 资源在Assets/Res/Config/GlobalProto.txt ConfigHelper.GetGlobal() 从Resources/KV.prefab取名为GlobalProto的文本，然后反序列化到GlobalProto中 GlobalProto.txt就是一个json文件，主要存储热更新地址和服务器地址， NetOuterComponentclass NetOuterComponent : NetworkComponent + void Awake() this.Awake(NetworkProtocol.TCP); this.MessagePacker = new ProtobufPacker(); 将数据压成包，压包器，采用Protobuf this.MessageDispatcher = new ClientDispatcher(); + new void Update() ClientDispatcher路径：Scripts/Base/Message/ClientDispatcher.cs interface IMessageDispatcher + void Dispatch(Session session, Opcode opcode, int offset, byte[] messageBytes, AMessage message) class ClientDispatcher: IMessageDispatcher + void Dispatch(Session session, Opcode opcode, int offset, byte[] messageBytes, AMessage message) 根据message类型判断消息类型， message如果是FrameMessage，那就是帧同步消息， Game.Scene.GetComponent&lt;ClientFrameComponent&gt;().Add(session, frameMessage); message如果是AMessage或ARequest，那就是普通消息或者是Rpc请求消息， 由MessageDispatherComponent处理 Session路径：Scripts/Entity/Session.cs sealed class Session : Entity - static uint RpcId - NetworkComponent network - AChannel channel - Dictionary&lt;uint, Action&lt;object&gt;&gt; requestCallback - List&lt;byte[]&gt; byteses + void Awake(NetworkComponent net, AChannel c) 设置network和channel，清空requestCallback + void Start() this.StartRecv(); + override void Dispose() + async void StartRecv() 一个while循环使之不断运转。 packet = await this.channel.Recv(); 接收包，收包的任一环节出现异常，都会导致循环停止。 先取出opcode,在调用RunDecompressedBytes，处理协议消息。完了之后进入下一轮循环，等待接受消息。 + void RunDecompressedBytes(ushort opcode, byte[] messageBytes, int offset, int count) 根据opcode从OpcodeTypeComponent中取出对应的协议结构体，然后将messageBytes反序列化到协议结构体中。 然后把协议消息发送出去，这里有判断rpc的逻辑。 + void CallWithAction(ARequest request, Action&lt;AResponse&gt; action) rpc调用 + Task&lt;AResponse&gt; Call(ARequest request, bool isHotfix) Rpc调用,发送一个消息,等待返回一个消息 + Task&lt;AResponse&gt; Call(ARequest request, bool isHotfix, CancellationToken cancellationToken) Rpc调用 + Task&lt;Response&gt; Call&lt;Response&gt;(ARequest request) where Response : AResponse Rpc调用,发送一个消息,等待返回一个消息 + Task&lt;Response&gt; Call&lt;Response&gt;(ARequest request, CancellationToken cancellationToken) Rpc调用 + void Send(AMessage message) 发送消息 + void Reply&lt;Response&gt;(Response message) where Response : AResponse 发送消息 + void SendMessage(object message) 将协议消息发往服务器 AChannel路径：Scripts/Base/Network/AChannel.cs enum PacketFlags enum ChannelType abstract class AChannel: IDisposable - long Id - ChannelType ChannelType - AService service - IPEndPoint RemoteAddress + abstract void Send(byte[] buffer) + abstract void Send(List&lt;byte[]&gt; buffers); + abstract Task&lt;Packet&gt; Recv(); + virtual void Dispose() MessageDispatherComponent类似EventComponent模块，分有热更版本 interface IMessageMethod + void Run(Session session, AMessage a); class IMessageMonoMethod : IMessageMethod + void Run(Session session, AMessage a) class IMessageILMethod : IMessageMethod + void Run(Session session, AMessage a) class MessageDispatherComponent : Component - Dictionary&lt;Opcode, List&lt;IMessageMethod&gt;&gt; handlers + void Awake() this.Load(); + void Load() 从DllHelper.GetMonoTypes()和DllHelper.GetHotfixTypes()获取types, 从中选择出带有MessageHandlerAttribute的type，添加到handlers中 + void Handle(Session session, MessageInfo messageInfo) 从handlers中找出与messageInfo.Opcode对应的IMessageMethod，运行它的Run方法 ClientFrameComponentclass FrameMessage : AActorMessage - int Frame - List&lt;AFrameMessage&gt; Messages struct SessionFrameMessage - Session Session - FrameMessage FrameMessage class ClientFrameComponent: Component - int Frame - EQueue&lt;SessionFrameMessage&gt; Queue - int count = 1 - int waitTime - const int maxWaitTime = 40 + void Start() UpdateAsync(); + void Add(Session session, FrameMessage frameMessage) 添加到Queue + async void UpdateAsync() 一个while循环使之不断运转。每隔一段时间调用一次this.UpdateFrame(); 如果队列中消息多于4个，则加速跑帧 + void UpdateFrame() 从Queue出队一个SessionFrameMessage， 将SessionFrameMessage.FrameMessage.Messages里的消息通过MessageDispatherComponent都发送出去 NetworkComponent这个是和服务器公用的组件 abstract class NetworkComponent : Component - AService Service - AppType AppType - Dictionary&lt;long, Session&gt; sessions - IMessagePacker MessagePacker - IMessageDispatcher MessageDispatcher + void Awake(NetworkProtocol protocol) 根据protocol类型创建Service，有TService和KService两种 + void Awake(NetworkProtocol protocol, IPEndPoint ipEndPoint) 根据protocol类型创建Service，有TService和KService两种， 将ipEndPoint传给Service的构造函数，并this.StartAccept(); + async void StartAccept() 在while循环中，不断的执行await this.Accept(); + virtual async Task&lt;Session&gt; Accept() 具体功能需要调一下 + virtual void Remove(long id) 移除Session + Session Get(long id) 获取Session + virtual Session Create(IPEndPoint ipEndPoint) 创建一个新Session + public void Update() this.Service.Update(); + override void Dispose() 网络方面还有TService，KService，Channel等，涉及到的东西比较多，具体的功能和流程需要在使用的时候弄清楚。]]></content>
      <categories>
        <category>unity</category>
        <category>game</category>
      </categories>
      <tags>
        <tag>game</tag>
        <tag>unity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blender基本操作]]></title>
    <url>%2F2018%2F01%2F03%2Fblender-start%2F</url>
    <content type="text"><![CDATA[大概在13年到14年，我曾学习和使用一段时间blender，感觉它的多边形建模功能极其方便，自带雕刻功能，还可以很方便的自定义Python脚本，一时间爱不释手。后来由于任务繁忙，并且也不是做这方面的，就很少使用了，于是就落下了。如今blender越来越多的被应用到手游制作流程中，即将到来的blender2.8也将带来许多极其强悍的功能，是时候重新温故一下blender了。 本文持续更新，陆续记录自己所使用到的功能，做一个备忘。 快捷键 num1指键盘数字区的按键，num2等按键以此类推。 视图 num1 : 前视图. num3 : 右视图. num5 : 在透视和正交视图之间切换. h : 隐藏选中的物体. alt + h : 取消物体的隐藏. z : 让视图在 wireframe/solid 2种显示模式之间切换. shift + c : 将3d游标重置到场景中央. shift + space : 让当前窗口最大化，再按一次回复到原来. space : 弹出搜索框. t : 打开/关闭3d窗口左侧的工具架，点击左侧的加号图标也可以. n : 打开/关闭3d窗口右侧的属性面板，点击右侧的加号图标也可以. 中键 : 旋转视图. ctrl + 中键 : 缩放视图. shift + 中键 : 平移视图. 数字区的句号 : 查看选中，类似于其他3d软件的f键. 注：更多快捷键可以在3d窗口的view菜单中找到 模式 tab : 在对象和编辑模式之间切换. 选择 a : 如果存在被选中的物体，就取消选中所有的物体，如果一个物体也没有被选中，就选中全部物体. b + 鼠标左键拖拽 : 方框选择，右键退出选择模式，鼠标中间拖拽是取消选中. c + 鼠标左键拖拽 : 笔刷选择，右键退出选择模式，鼠标中间拖拽是取消选中，滚动滚轮可改变笔刷大小. l : 选择鼠标下面的所有顶点，按住shift键取消选中. alt + 鼠标右键 : 选择循环边，按住shift可以加选. 变换 g : 移动工具. r : 旋转工具，按2下r进入轨迹球旋转模式. s : 缩放工具. 使用变换工具时，使用键盘输入数值(可以包含小数)，这个数值将被设置到正在被变换的那个属性上，同时这个数值也会显示在3d窗口底部的状态栏上面，按减号键可以删除上一次按键输入的数字. 在进行移动旋转缩放时，按住 x/y/z : 让目标限制在这个坐标轴上进变换. 按住ctrl : 开启捕捉功能. 按住shift : 在进行移动旋转缩放时，以非常小的幅度进行调整. 句号 : 将中心点设置到3d游标上. 逗号 : 将中心点设置到网格中心上.如果是对象模式，就是设置到物体的初始点. 点击3d窗口底部工具栏的操纵器图标可以显示出操纵杆。 通用 x : 删除选中的. shift + d : 复制选中的. shift + a : 创建网格/对象. 编辑模式 e : 挤出选中的. f : 填充面. p : 将选中的网格分离到新对象中. alt + m : 合并选中的顶点. ctrl + r : 环切(loopcut)，滚动滚轮可以改变切割的数量. w : 针对当前的模式或上下文，弹出相应的特殊菜单. ctrl + e : 边菜单. ctrl + tab : 网格选择模式切换(点/边/面). ctrl + n : 重新计算选中的顶点的法线.如果在编辑骨骼，这将重新计算骨骼的旋转. 对象模式 ctrl + j : 将多个选中的对象合并成一个网格. ctrl + a : 弹出apply菜单，具体功能需要看一下文档. ctrl + p : 将第一个选中的对象设置成后选中的对象的父亲. 姿态模式 ctrl + c : 复制选中的骨骼的姿态. shift + ctrl + v : 复制了姿态后，粘贴到rig的另一边，因为分左右边嘛. &lt; 待更新 &gt;]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows上安装tensorflow]]></title>
    <url>%2F2017%2F12%2F16%2Finstall-tensorflow%2F</url>
    <content type="text"><![CDATA[访问 tensorflow 官网方法一: 直接访问 https://tensorflow.google.cn 方法二: 修改 hosts 文件windows的hosts文件的位置是 C:\Windows\System32\drivers\etc\hosts ，在hosts文件末尾添加如下内容： 164.233.188.121 www.tensorflow.org 刷新dns缓存，在命令行执行下面命令(windows)： 1ipconfig /flushdns 在浏览器访问 www.tensorflow.org ，就可以浏览官网了。 安装cuda(win)在nvidia的官网下载cuda的安装包，按照默认选项安装就可以了。如果不出意外，安装完毕后，cuda的bin文件夹会自动添加到环境变量。 在NVIDIA官网的cudnn页面，下载与cuda适配的cudnn压缩包。解压后，会有bin、include、lib三个文件夹，将这三个文件夹拷贝到cuda的安装文件夹下面，并与cuda的bin、include、lib文件夹合并。 如果下载时的网络连接不稳定，推荐使用wget -c url命令下载。 通过pip安装如果cuda和cudnn的版本符合pip包的要求，可以直接通过pip安装pip3 install tensorflow-gpu 编译安装我一般是在虚拟机里使用linux，而虚拟机无法使用gpu加速，因此暂时不考虑在虚拟机的linux里编译tensorflow。tensorflow的windows编译有很多问题，很麻烦。想在windows机器上自己编译，最好的方案是等待wsl支持gpu后，在wsl中编译tensorflow。在OpenCL &amp; CUDA GPU support，Cannot find GPU devices on Bash可以关注一下wsl支持gpu的最新进展。 编译的过程中会下载一些文件，需要留意一下这些下载地址能否访问到。如果git clone时提示 ... port 443: Timed out，则表明这个地址被屏蔽了，为了简单省事，最好还是用shadowsocks。开启shadowsocks后，设置git的http/https代理协议12git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos; 编译完毕后，取消代理12git config --global --unset http.proxygit config --global --unset https.proxy 可以查看一下git的配置1git config --global -l 为了避免路径中有空格造成的烦恼，建议目录使用符号链接，例如1mklink /D C:\CUDA &quot;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA&quot; 检测gpu是否开启在python的交互环境下，输入如下代码，可以查看有哪些设备12from tensorflow.python.client import device_libprint(device_lib.list_local_devices()) 实际上在tensorflow开始运行后，输出的log中会有device信息，可以判断是否启用了gpu。 测试下面代码是之前的bp神经网络的tensorflow实现，可以用来测试一下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100# -*- coding:utf-8 -*-import timefrom sklearn import datasetsimport numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltdef add_layer(inputs, dim_in, dim_out, layer_n, is_output_layer=False, y=None): layer_name = f'layer&#123;layer_n&#125;' with tf.name_scope(layer_name): with tf.name_scope('weights'): Weights = tf.Variable(tf.random_normal([dim_in, dim_out])) # Weight中都是随机变量 tf.summary.histogram(layer_name + "/weights", Weights) # 可视化观看变量 with tf.name_scope('biases'): biases = tf.Variable(tf.zeros([1, dim_out])) # biases推荐初始值不为0 tf.summary.histogram(layer_name + "/biases", biases) # 可视化观看变量 with tf.name_scope('z'): z = tf.matmul(inputs, Weights) + biases # inputs*Weight+biases tf.summary.histogram(layer_name + "/z", z) # 可视化观看变量 if is_output_layer: outputs = tf.nn.softmax(z, name='outputs') loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y, name='loss') tf.summary.histogram(layer_name + '/loss', loss) # 可视化观看变量 return outputs, loss else: outputs = tf.nn.tanh(z) tf.summary.histogram(layer_name + "/outputs", outputs) # 可视化观看变量 return outputs, Nonenp.random.seed(0)X_data, y_data = datasets.make_moons(200, noise=0.20)num_examples = len(X_data)ym_data = np.zeros((num_examples, 2))ym_data[range(num_examples), y_data] = 1# 生成一个带可展开符号的域with tf.name_scope('inputs'): xs = tf.placeholder(tf.float32, name='X') ys = tf.placeholder(tf.float32, name='y')tf.set_random_seed(0)# 三层神经网络，输入层（2个神经元），隐藏层（3神经元），输出层（2个神经元）layer1, _ = add_layer(xs, 2, 3, 1) # 隐藏层predict_step, loss = add_layer(layer1, 3, 2, 2, True, ys) # 输出层with tf.name_scope('train'): train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 0.01学习率,minimize(loss)减小loss误差init = tf.global_variables_initializer()config = tf.ConfigProto()# https://tensorflow.google.cn/tutorials/using_gpu#allowing_gpu_memory_growthconfig.gpu_options.allow_growth = Truesess = tf.Session(config=config)# 合并到Summary中merged = tf.summary.merge_all()# 选定可视化存储目录writer = tf.summary.FileWriter("./", sess.graph)sess.run(init) # 先执行initstart_time = time.time()# 训练2w次num_passes = 20000for i in range(num_passes): sess.run(train_step, feed_dict=&#123;xs: X_data, ys: ym_data&#125;) if i % 50 == 0: result = sess.run(merged, feed_dict=&#123;xs: X_data, ys: ym_data&#125;) # merged也是需要run的 writer.add_summary(result, i) # result是summary类型的，需要放入writer中，i步数（x轴）time_cost = time.time() - start_timesummary_text = f'cost: &#123;time_cost&#125;'print(summary_text)# --------------------------- predict ---------------------------def predict(x): predict = sess.run(predict_step, feed_dict=&#123;xs: x&#125;) return np.argmax(predict, axis=1)def visualize(X, y): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) plt.show() plt.title("bp nn")visualize(X_data, y_data) 运行完上面的脚本后，在该脚本目录下执行tensorboard --logdir=&quot;./&quot;命令，就可以在浏览器中查看tensorboard。]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在illustrator中使用latex公式]]></title>
    <url>%2F2017%2F12%2F04%2Fillustrator-latex%2F</url>
    <content type="text"><![CDATA[Adobe Illustrator 是一款强大的矢量图形软件，当我们在里面做好图形后，可能需要在里面插入一些数学符号或者数学公式什么的，这时一般考虑使用 Latex 来输出这些特殊文本。下面介绍一下如何在 Illustrator 中使用 Latex 。 安装 LaTeXLaTeX 有很多发行版，在这里可以查看到。我在windows环境下使用的是 MikTeX，在这里下载了安装包后，按照正常的Windows软件安装流程进行安装，途中可能会提示安装一些依赖，按照提示进行操作即可。 安装完 Latex 后，需要确认一下，从 PATH 环境变量里能否搜索到 pdflatex 命令，如果不能，则需要将 pdflatex 所在的文件夹添加到 PATH 环境变量里。 安装 LaTeX 字体LaTeX 会使用一些特殊的字体，为了让导出的文本能够在 Illustrator 中正常显示，需要让 Illustrator 能够搜索到这些字体。比较简单省事的做法是将这些字体拷贝到adobe的字体文件夹里。例如在Windows上，将 D:\Program Files\MiKTeX 2.9\fonts\type1\public\amsfonts\cm 文件夹里的文件全部拷贝到 C:\Program Files\Common Files\Adobe\Fonts 文件夹。 将 LaTeX 导出到 IllustratorIllustrator 可以从PDF文件中导入单独的页面，因此，可以先将 Latex 代码编译成PDF(可以使用pdflatex)，再将PDF导入到 Illustrator 中去。 为了简化 Latex -&gt; PDF -&gt; Illustrator 这一过程，有一些脚本可供使用，例如 latex-illustrator 和 illustratorLatexEquations等。 根据我的实际使用情况，我对脚本做了一下修改，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// latex2illustrator.jsvar PDF_LATEX_EXE = "pdflatex.exe"; // Add full path if necessaryvar LAST_TEX_CODE_FILE = 'latex2illustrator_lastcode.txt';var TEX_FILE = 'latex2illustrator.tex';var PDF_FILE = 'latex2illustrator.pdf';var BAT_FILE = 'latex2illustrator.bat';var TEMP_PATH = getWorkPath();function getWorkPath() &#123; // determining the local temporary directory var temppath = Folder.temp.fsName; // path already in Windows syntax: c:\... var i = temppath.indexOf("Temporary Internet Files"); if (i &gt;= 0) temppath = temppath.substr(0, i + 4); //temppath should now contain something like C:\Documents and Settings\&lt;user&gt;\Local Settings\Temp return temppath&#125;function getLastCode() &#123; // remember the last user input in a text file var lastCode = "$$" var lastCodeFile = File(TEMP_PATH + "\\" + LAST_TEX_CODE_FILE); if (lastCodeFile.exists) &#123; lastCodeFile.open("r"); lastCode = lastCodeFile.read(); lastCodeFile.close(); &#125; return lastCode&#125;function writeLatexFile(latexCode) &#123; // add latex header etc. to create a complete latex document var latexFile = new File(TEMP_PATH + '\\' + TEX_FILE); latexFile.open("w"); // latexFile.writeln("\\documentclass&#123;standalone&#125;"); latexFile.writeln("\\documentclass&#123;article&#125;"); // add or remove additional latex packages here latexFile.writeln("\\usepackage&#123;amsmath&#125;"); latexFile.writeln("\\usepackage&#123;amsthm&#125;"); latexFile.writeln("\\usepackage&#123;amssymb&#125;"); latexFile.writeln("\\usepackage&#123;gensymb&#125;"); // for \degree latexFile.writeln("\\usepackage&#123;textcomp&#125;"); // for \textdegree latexFile.writeln("\\usepackage&#123;bm&#125;"); // bold math latexFile.writeln("\\begin&#123;document&#125;"); latexFile.writeln("\\pagestyle&#123;empty&#125;"); // no page number latexFile.writeln(latexCode); latexFile.writeln("\\end&#123;document&#125;"); latexFile.close();&#125;function generate(latexcode) &#123; var pdfFile = File(TEMP_PATH + "\\" + PDF_FILE); if (pdfFile.exists) pdfFile.remove(); // create a batch file calling latex var batchFile = new File(TEMP_PATH + '\\' + BAT_FILE); batchFile.open("w"); batchFile.writeln(PDF_LATEX_EXE + ' -aux-directory="' + TEMP_PATH + '" -include-directory="' + TEMP_PATH + '" -output-directory="' + TEMP_PATH + '" "' + TEMP_PATH + '\\' + TEX_FILE + '"'); //batchFile.writeln('pause'); batchFile.writeln('del "' + TEMP_PATH + '\\' + BAT_FILE + '"'); batchFile.close(); batchFile.execute(); for (; batchFile.exists;) // wait until the batch file has removed itself var pdfFile = File(TEMP_PATH + "\\" + PDF_FILE); if (pdfFile.exists) &#123; // import pdf file into the current document var grp = app.activeDocument.activeLayer.groupItems.createFromFile(pdfFile); // The imported objects are grouped twice. Now move the subgroup // items to the main group and skip the last item which is the page frame for (var i = grp.pageItems[0].pageItems.length; --i &gt;= 0;) grp.pageItems[0].pageItems[i].move(grp, ElementPlacement.PLACEATEND); var last = grp.pageItems.length - 1; if (last &gt;= 0 &amp;&amp; grp.pageItems[last].typename == 'PathItem') grp.pageItems[last].remove(); // Move the imported objects to the center of the current view. grp.translate(app.activeDocument.activeView.centerPoint[0] - grp.left, app.activeDocument.activeView.centerPoint[1] - grp.top); &#125; else alert("File " + TEMP_PATH + "\\" + pdfFile.name + " could not be created. LaTeX error?");&#125;function latex2illustrator() &#123; TEMP_PATH = 'E:/data'; var latexCode = getLastCode(); if (latexCode != null) &#123; writeLatexFile(latexCode); generate(latexCode); &#125;&#125;latex2illustrator() 在E:/data/latex2illustrator_lastcode.txt中写入latex公式，然后在Illustrator中使用ctrl+F12执行latex2illustrator.js脚本，就可以插入数学公式了。 参考链接: Combining LaTeX and Illustrator Scripting Adobe Illustrator to create images from LaTeX equations]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个简单的BP神经网络的实现]]></title>
    <url>%2F2017%2F12%2F02%2Fbp-neural-network%2F</url>
    <content type="text"><![CDATA[不详述神经网络模型，只记录一下实现BP神经网络时的推导过程。 输入值和输出值输入值是一个维度是n的特征向量，记作 $x=(x_1,x_2,\ldots,x_n)$。一个数据集里一般有多个样本，假定有m个样本，则将这个数据集记作 $X = (x^{(1)},x^{(2)},\ldots,x^{(m)})$ 。 输出值一般是一个数值，用于表示属于哪个类别。对于m个样本，输出值的集合记作 $y=(y^{(1)}, y^{(2)}, \ldots, y^{(m)})$ 。 输入层的节点个数取决于输入的特征向量的维度。 输出层的节点个数取决于拥有的类别个数。如果只有2类，则可以只用一个输出节点用于预测0或1。 隐藏层的节点越多，则越复杂的函数可以fit到。但是这样做的代价也比较大，首先，会增大训练参数和进行预测的计算量。其次，大量的参数也容易导致过拟合。 需要根据具体情况选择节点的个数。 有一个经验公式可以确定隐含层节点数目，如下 h=\sqrt{m+n} + a其中h为隐含层节点数目，m为输入层节点数目，n为输出层节点数目，a为1~10之间的调节常数。 正向传播 这里约定上标 (i) 为样本在样本集中的序号，上标 [i] 为神经网络的层的序号，下标 [i] 为网络中某一层的节点的序号，log的底数默认是e。 神经网络使用正向传播进行预测。 对于一个3层的神经网络，可以这样计算预测值 $\hat y$ ： \begin{align} & z^{[1]} = xW^{[1]}+ b^{[1]} \\ & a^{[1]} = \sigma (z^{[1]}) \\ & z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\ & a^{[2]} = \hat y = softmax (z^{[2]}) \\ \end{align}神经网络里的计算都需要进行向量化，具体来说 对于第1层的输入值和输出值 X = \begin{pmatrix} x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\ x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\ \vdots & \vdots & \ddots & \vdots \\ x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)} \\ \end{pmatrix} = \begin{pmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\ \end{pmatrix} , \qquad y = \begin{pmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)} \\ \end{pmatrix}第2层 假设第2层有u个节点，则这层的权重和偏置为 W^{[1]} = \begin{pmatrix} w_{1[1]} & w_{1[2]} & \cdots & w_{1[u]} \\ w_{2[1]} & w_{2[2]} & \cdots & w_{2[u]} \\ \vdots & \vdots & \ddots & \vdots \\ w_{n[1]} & w_{n[2]} & \cdots & w_{n[u]} \\ \end{pmatrix} = \begin{pmatrix} w_{[1]} & w_{[2]} & \ldots & w_{[u]} \end{pmatrix} , \qquad b^{[1]} = \begin{pmatrix} b_{[1]} & b_{[2]} & \ldots & b_{[u]} \end{pmatrix}根据矩阵的分块的性质 \begin{align} XW^{[1]} &= \begin{pmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\ \end{pmatrix} \begin{pmatrix} w_{[1]} & w_{[2]} & \ldots & w_{[u]} \end{pmatrix} = AB = \begin{pmatrix} A_{11} \\ A_{21} \\ \vdots \\ A_{m1} \\ \end{pmatrix} \begin{pmatrix} B_{11} & B_{12} & \ldots & B_{1u} \\ \end{pmatrix} \\ & = \begin{pmatrix} C_{11} & C_{12} & \cdots & C_{1u} \\ C_{21} & C_{22} & \cdots & C_{2u} \\ \vdots & \vdots & \ddots & \vdots \\ C_{m1} & C_{m2} & \cdots & C_{mu} \\ \end{pmatrix} = \begin{pmatrix} A_{11}B_{11} & A_{11}B_{12} & \cdots & A_{11}B_{1u} \\ A_{21}B_{11} & A_{21}B_{12} & \cdots & A_{21}B_{1u} \\ \vdots & \vdots & \ddots & \vdots \\ A_{m1}B_{11} & A_{m1}B_{12} & \cdots & A_{m1}B_{1u} \\ \end{pmatrix} \\ & = \begin{pmatrix} x^{(1)}w_{[1]} & x^{(1)}w_{[2]} & \cdots & x^{(1)}w_{[u]} \\ x^{(2)}w_{[1]} & x^{(2)}w_{[2]} & \cdots & x^{(2)}w_{[u]} \\ \vdots & \vdots & \ddots & \vdots \\ x^{(m)}w_{[1]} & x^{(m)}w_{[2]} & \cdots & x^{(m)}w_{[u]} \\ \end{pmatrix} \end{align}参考numpy中的boardcast，则有 \begin{align} z^{[1]} &= XW^{[1]} + boardcast\ (b^{[1]}) = \begin{pmatrix} x^{(1)}w_{[1]} + b_{[1]} & x^{(1)}w_{[2]} + b_{[2]} & \cdots & x^{(1)}w_{[u]} + b_{[u]} \\ x^{(2)}w_{[1]} + b_{[1]} & x^{(2)}w_{[2]} + b_{[2]} & \cdots & x^{(2)}w_{[u]} + b_{[u]} \\ \vdots & \vdots & \ddots & \vdots \\ x^{(m)}w_{[1]} + b_{[1]} & x^{(m)}w_{[2]} + b_{[2]} & \cdots & x^{(m)}w_{[u]} + b_{[u]} \\ \end{pmatrix} \\ & = \begin{pmatrix} z_{[1]}^{(1)} & z_{[2]}^{(1)} & \cdots & z_{[u]}^{(1)} \\ z_{[1]}^{(2)} & z_{[2]}^{(2)} & \cdots & z_{[u]}^{(2)} \\ \vdots & \vdots & \ddots & \vdots \\ z_{[1]}^{(m)} & z_{[2]}^{(m)} & \cdots & z_{[u]}^{(m)} \\ \end{pmatrix} \end{align} a^{[1]} = \begin{pmatrix} \sigma(z_{[1]}^{(1)}) & \sigma(z_{[2]}^{(1)}) & \cdots & \sigma(z_{[u]}^{(1)}) \\ \sigma(z_{[1]}^{(2)}) & \sigma(z_{[2]}^{(2)}) & \cdots & \sigma(z_{[u]}^{(2)}) \\ \vdots & \vdots & \ddots & \vdots \\ \sigma(z_{[1]}^{(m)}) & \sigma(z_{[2]}^{(m)}) & \cdots & \sigma(z_{[u]}^{(m)}) \\ \end{pmatrix}以上的计算方式可以推广到多层神经网络中的任一隐藏层，对于第$l$层，将 $X$ 类比成 $a^{[l-1]}$ ， $W^{[1]}$ 类比成 $W^{[l]}$ ， $b^{[1]}$ 类比成 $b^{[l]}$ ， $z^{[1]}$ 类比成 $z^{[l]}$ ， $a^{[1]}$ 类比成 $a^{[l]}$ ，就可以了。 对于输出层，因为我们希望网络输出各个分类的概率，所以将输出层的激活函数选为softmax 。softmax 是一种简便的将原始评分转换成概率的方法。可以将 softmax 看做 logistic 函数的在多分类下的推广。 a^{[2]} = softmax (z^{[2]}) = \begin{pmatrix} softmax(z^{[2](1)}) \\ softmax(z^{[2](2)}) \\ \vdots \\ softmax(z^{[2](m)}) \\ \end{pmatrix}权重和偏置的初始化权重 w 不能全部置为0，这样会导致每个层的所有节点的计算都是相同的。如果激活函数使用sigmoid或者tanh，随机出来的 w 最好小一点，一般乘以0.01。因为如果 w 比较大，通过激活函数计算出来的值会落到接近1的位置，导致学习速度变慢。 偏置 b 可以全部初始化为0，但是推荐初始值不为0。 激活函数 tanh sigmoid function ReLUs softmaxsoftmax 又称为归一化指数函数，是广义上的 logistic 函数。假设k 维向量 $z$ 的各项是任意实数，使用 softmax 对 $z$ 进行“压缩”处理后，压缩后的向量 $\sigma (z)$ 的每一项的值在 [0, 1] 之间，所有项之和等于1。函数公式如下 \sigma (z)_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad for\ j = 1, \ldots, K.一个简单的例子如下 1234import numpy as npz = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])exp_scores = np.exp(z)probs = exp_scores / np.sum(exp_scores) # 应用 softmax 在神经网络的输出层中，有C个分类，对于给定的输入 $z$ ，每个分类的概率可以表示为： \begin{bmatrix} P(t=1|z) \\ P(t=2|z) \\ \vdots \\ P(t=C|z) \\ \end{bmatrix} = \frac {1}{ \sum_{k=1}^C e^{z_k} } \begin{bmatrix} e^{z_1} \\ e^{z_2} \\ \vdots \\ e^{z_C} \\ \end{bmatrix}其中，$P(t=c|z)$ 表示，在给定输入$z$时，该输入数据是$c$分类的概率。 对于 softmax 函数 a_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad for\ j = 1, \ldots, K.softmax 函数的求导过程比较特别，分如下2种情况。这是因为i = j 时，$z_i$ 与 $z_j$ 是同一个变量，按偏导数的定义，将多元函数关于一个自变量求偏导数时，就将其余的自变量看成常数，因此需要分2种情况处理。 \begin{align} &if \ j=i \\ &\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr) = \frac { (e^{z_j})^\prime \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_j} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 } = \frac { e^{z_j} } { \sum_k e^{z_k} } - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_j} } { \sum_k e^{z_k} } = a_j ( 1-a_j ) \\ &if \ j \neq i \\ &\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr) = \frac { 0 \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_i} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 } = - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_i} } { \sum_k e^{z_k} } = -a_j a_i \end{align}反向传播反向传播主要思想是：（1）将训练集数据输入到输入层，经过隐藏层，最后达到输出层并输出结果，这是前向传播过程；（2）由于输出层的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。 代价函数我们把定义估计值与实际值之间误差(单个样本)的函数叫作误差损失(loss)函数，代价(cost)函数是各个样本的loss函数的平均。 如果误差损失函数采用二次代价函数 ，在实际中，如果误差越大，参数调整的幅度可能更小，训练更缓慢。使用交叉熵代价函数替换二次代价函数，可以解决学习缓慢的问题。 示性函数：$1\{\cdot\}$取值规则为：$1\{值为真的表达式\}=1$ ，$1\{值为假的表达式\}=0$ 。举例来说，表达式 $1\{2+2=4\}$的值为1，$1\{1+1=5\}$的值为 0。 在分类问题中，交叉熵代价函数与对数似然代价函数在形式上是基本一致的。 对于输出层的softmax激活函数，假设有m个样本，k个类别，将$p(x)$记为$1\{ y^{(i)} = j \}$，$q(x)$记为softmax函数的输出值 则根据交叉熵公式， H(p,q) = - \sum_x p(x)log\ q(x)可以得到交叉熵代价函数为： J(\theta) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \frac{e^{\theta_j^T x^{(i)}}}{ \sum_{l=1}^k e^{ \theta_l^T x^{(i)} } } \biggl] \Biggl]从似然函数的角度分析，记$h_{\theta j}(x)= \frac{e^{\theta_j^T x}}{ \sum_{l=1}^k e^{ \theta_l^T x } }$ (h一般是hypothesis的缩写)，在一个样本中，对于输入x的分类结果为j的概率为 P(y=j|x; \theta) = h_{\theta j} (x)^{ 1\{ y = j \} }将所有分类的概率综合起来，则有： P(y|x; \theta) = \prod_{j=1}^k h_{\theta j} (x)^{ 1\{ y = j \} }取似然函数为： \begin{align} L(\theta) &= \prod_{i=1}^m P(y^{(i)} | x^{(i)}; \theta) \\ &= \prod_{i=1}^m \biggl[ \prod_{j=1}^k h_{\theta j} (x^{(i)})^{1\{ y^{(i)} = j \}} \biggl] \end{align}对数似然函数为： \begin{align} l(\theta) &= log\ L(\theta) \\ &= \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ h_{\theta j} (x^{(i)}) \biggl] \end{align}最大似然估计就是要求得使$l(\theta)$取最大值时的$\theta$ 。一般将它乘上一个负系数-1/m，即： J(\theta) = - \frac{1}{m} l(\theta)则$J(\theta)$取最小值时的$\theta$为要求的最佳参数。这也就是上面的交叉熵代价函数。 log MN = log M + log N 很多文献里对数都没标底数，这说明可以取任意底数，一般取e或2，取不同的底数时，对数值只相差了一个常数系数，对算法不会有影响。 相关的具体分析参考如下链接： Softmax回归 Softmax Regression Softmax分类函数 neural_network_implementation_intermezzo02 Improving the way neural networks learn 交叉熵代价函数 交叉熵代价函数 cross-entropy loss二次代价函数的不足考察一下二次代价函数 C = \frac {1}{2n} \sum_x \| y(x) - a^L(x) \|^2其中，C表示代价，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。为简单起见，以一个样本为例进行说明，此时二次代价函数为： C = \frac {(y-a)^2}{2}在用梯度下降法(gradient descent) 调整权重w和偏置b的过程中，w和b的梯度推导如下： \begin{align} &\frac { \partial C }{ \partial w } = (a - y)\sigma^\prime(z) x \\ &\frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z) \\ \end{align}可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数或tanh函数，观察这些激活函数的图像，当初始的代价（误差）越大时，梯度（导数）越小，训练的速度越慢。这与我们的期望不符，即不能错误越大，改正的幅度越大，从而学习得越快。 交叉熵代价函数为了克服二次代价函数学习缓慢的缺点，引入了交叉熵代价函数： C = - \frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)]其中，x表示样本，n表示样本的总数。重新计算参数w的梯度： \begin{align} \frac{\partial C}{\partial w_j} &= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \frac{\partial \sigma(z)}{\partial w_j} \\ &= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \sigma^\prime(z)x_j \\ &= \frac{1}{n} \sum_x \frac{ \sigma^\prime(z)x_j }{\sigma(z) (1-\sigma(z))} (\sigma(z)-y) \\ &= \frac{1}{n} \sum_x x_j (\sigma(z)-y) \\ \end{align}其中，w的梯度公式中原来的 $\sigma^\prime(z)$ 被消掉了；另外，该梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数w调整得越快，训练速度也就越快。同理可得，b的梯度为： \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y)交叉熵代价函数的来源用交叉熵代替二次代价函数的想法源自哪里？ 以偏置b的梯度计算为例 \begin{align} \frac{\partial C}{\partial b} &= \frac{\partial C}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial b} \\ &= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \cdot \frac{\partial (wx+b)}{\partial b} \\ &= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \\ &= \frac{\partial C}{\partial a} \cdot a(1-a) \\ \end{align}而二次代价函数推导出来的b的梯度公式为： \frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z)为了消掉该公式中的 $\sigma^\prime(z)$ ，需要找到一个代价函数使得： \frac { \partial C }{ \partial b } = (a - y)即 \frac { \partial C }{ \partial a } \cdot a(1-a) = (a - y)对方程进行关于a的积分，可得： C = -[yln\ a + (1-y)ln(1-a)] + constant其中constant是积分常量。这是一个单独训练样本X对代价函数的贡献。为了得到整个的代价函数，还需要对所有的训练样本进行平均，可得： C = -\frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)] + constant而这就是前面的交叉熵代价函数。 在分类问题中，交叉熵其实就是对数似然函数的最大化。 关于交叉熵的更多内容，参考以下链接： 交叉熵（Cross-Entropy） Cross entropy 怎样理解 Cross Entropy 信息论的熵 Entropy 反向传播算法 求导的链式法则 (Chain rule)：表达式：$(f(g(x)))^\prime = f^\prime (g(x)) g^\prime (x)$其他形式：$\frac {dy}{dx} = \frac {dy}{dz} \cdot \frac {dz}{dx}$ 求$J(W, b)$的最小值可以使用梯度下降法，根据梯度下降法可得$W$和$b$的更新过程： W^{[l]}_{[i]j} := W^{[l]}_{[i]j} - \alpha \frac{\partial}{\partial \ W^{[l]}_{[i]j}} J(W, b) \\ b^{[l]}_{[i]} := b^{[l]}_{[i]} - \alpha \frac{\partial}{\partial \ b^{[l]}_{[i]}} J(W, b)其中，$\alpha$为学习步长，$W^{[l]}_{[i]j}$为第l层的第i个节点的权重第j个分量，$b^{[l]}_{[i]}$为第l层的第i个节点的偏置。 输出层输出层的激活函数采用的是softmax函数。根据前文，输出层的误差采用交叉熵代价函数来衡量，即： x^{(i)} = a ^{[1](i)}\\ z_{[j]}^{[2](i)} = x^{(i)}W_{[j]}^{[2]} + b_{[j]}^{[2]} \\ a_{[j]}^{[2](i)} = \frac{e^{z_{[j]}^{[2](i)}}}{ \sum_{k=1}^K e^{ z_{[k]}^{[2](i)} } } \\ J(W, b) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \ a_{[j]}^{[2](i)} \biggl] \Biggl]其中，依照前文约定，下标 [j] 为第$j$个节点的序号，上标 (i) 为样本在样本集中的序号。 输出层的第$l$个节点的权重$W_{[l]}$的第$t$个分量，求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$) \begin{align} \frac{\partial}{\partial \ W_{[l]t}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ W_{[l]t}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ W_{[l]t}} \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]} \biggl) + \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}} \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \cdot x^{(i)}_t \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl( 1\{ y^{(i)} = l \} - a_{[l]} \biggl) \cdot x^{(i)}_t \Biggl] \\ \end{align}输出层的第$l$个节点的偏置$b_{[l]}$的求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$) \begin{align} \frac{\partial}{\partial \ b_{[l]}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ b_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ b_{[l]}} \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]} \biggl) + \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}} \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) + \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr) \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \Biggl] \\ &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \biggl] \Biggl] \\ \end{align}在很多的文献中，会把上式记成如下形式，其中上标(i)表示是第i个样本： \begin{align} \delta_{[l]}^{[2](i)} &= \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} = \frac{\partial}{ \partial \ z_{[l]}^{[2](i)} } \biggl[- \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}^{[2](i)} \biggl] = a_{[l]}^{[2](i)} - 1\{ y^{(i)} = l \} \\ \frac{\partial}{\partial \ W_{[l]t}^{[2]}} J(W, b) &= \frac{\partial }{\partial \ W_{[l]t}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)}) = \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ W_{[l]t}^{[2]}} ) = \frac{1}{m} \sum_{i=1}^m ( \delta_{[l]}^{[2](i)} \cdot a_t^{[1](i)} ) \\ \frac{\partial}{\partial \ b_{[l]}^{[2]}} J(W, b) &= \frac{\partial }{\partial \ b_{[l]}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)}) = \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ b_{[l]}^{[2]}} ) = \frac{1}{m} \sum_{i=1}^m \delta_{[l]}^{[2](i)} \\ \end{align} 这里log的底数用的是e 这里的x指的是输出层的上一层的输出 关于$\delta$的原文是：for each node $i$ in layer $l$, we would like to compute an “error term” $\delta^{(l)}$ that measures how much that node was “responsible” for any errors in our output ，链接在这里 隐藏层隐藏层的激活函数采用的是tanh函数。 先考虑只有一个隐藏层的情况，参考下面这张图片，可以发现当前层的每个节点的权重和偏置会影响下一层的各个节点 \begin{align} & z^{[1]} = xW^{[1]}+ b^{[1]} \\ & a^{[1]} = \sigma (z^{[1]}) = tanh(z^{(i)}) \\ & z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\ & a^{[2]} = \hat y = softmax (z^{[2]}) \\ \end{align} \begin{align} \frac{\partial}{\partial \ W^{[1]}_{[v]t}} J(W, b) &= \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } E^{(i)} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggr] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \quad \text{(multivariate chain rule)}\\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggr] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \cdot x_t^{(i)} \biggr] \\ \end{align} \begin{align} \frac{\partial}{\partial \ b^{[1]}_{[v]}} J(W, b) &= \frac{\partial}{ \partial \ b^{[1]}_{[v]} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ b^{[1]}_{[v]} } E^{(i)} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggl] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \quad \text{(multivariate chain rule)}\\ &= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggl] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \biggr] \\ \end{align}按照输出层的$\delta$的定义，可以定义任意一层的$\delta$，并在计算的时候将本层的$\delta$传递给下一层，从而计算各层的权重和偏置的导数。 几个变量相互之间有依赖关系，这时某个变量的偏导数不能反映变化率，要表示在该变量上的变化率，应该使用该变量的全导数。变量相互独立时，偏导数可以表示变化率。 参考链接： Total derivative 如何理解神经网络里面的反向传播算法 A Step by Step Backpropagation Example 终止条件 权重的更新低于某个阈值； 预测的错误率低于某个阈值； 达到预设一定的循环次数； 向量化下面这张图描述了正向传播时，各变量的维度 其中$x$的横线表示$x$可以看做是由一组横向量组成的，$W$的竖线表示$W$可以看做是由一组竖向量组成的。 附录反向传播的一种错误求导在隐藏层的反向传播求导时，下面的求导方式是错误的： \begin{align} \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } &= \sum_o \bigl[ \frac{\partial \ E_{[o]}^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \bigr] \\ \end{align}原因是$z_{[o]}^{(i)[2]}$对所有的$E_{[j]}^{(i)}$都有影响，而这里只考虑了与$z_{[o]}^{(i)[2]}$相对应的$E_{[o]}^{(i)}$。 输出层的求导是没问题的，因为考虑了所有的分量。 数学复习 矩阵A(m, n)，m指行数，n指列数 sigmoid函数求导 \begin{align} \sigma^\prime(z) &= \bigl( \frac{1}{1 + e^{-z}} \bigr)^\prime = (-1)(1+e^{-z})^{(-1)-1} \cdot (e^{-z})^\prime = \frac{1}{(1+e^{-z})^2} \cdot (e^{-z}) \\ &= \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} = \frac{1}{1+e^{-z}} \cdot (1-\frac{1}{1+e^{-z}}) \\ &= \sigma(z)(1-\sigma(z)) \end{align}使用数学软件在一些推导中，特别是矩阵的计算，想直观的看一下展开后的结果，如果完全手算，会比较费时，可以使用mathematica进行符号计算。例如，计算矩阵的相乘，在mathematica中输入以下代码 12&#123;u, n, m&#125; = &#123;3, 2, 5&#125;Array[Subscript[w, ##] &amp;, &#123;u, n&#125;] . Array[Subscript[x, ##] &amp;, &#123;n, m&#125;] 得到结果 \left( \begin{array}{ccccc} w_{1,1} x_{1,1}+w_{1,2} x_{2,1} & w_{1,1} x_{1,2}+w_{1,2} x_{2,2} & w_{1,1} x_{1,3}+w_{1,2} x_{2,3} & w_{1,1} x_{1,4}+w_{1,2} x_{2,4} & w_{1,1} x_{1,5}+w_{1,2} x_{2,5} \\ w_{2,1} x_{1,1}+w_{2,2} x_{2,1} & w_{2,1} x_{1,2}+w_{2,2} x_{2,2} & w_{2,1} x_{1,3}+w_{2,2} x_{2,3} & w_{2,1} x_{1,4}+w_{2,2} x_{2,4} & w_{2,1} x_{1,5}+w_{2,2} x_{2,5} \\ w_{3,1} x_{1,1}+w_{3,2} x_{2,1} & w_{3,1} x_{1,2}+w_{3,2} x_{2,2} & w_{3,1} x_{1,3}+w_{3,2} x_{2,3} & w_{3,1} x_{1,4}+w_{3,2} x_{2,4} & w_{3,1} x_{1,5}+w_{3,2} x_{2,5} \\ \end{array} \right)代码下面是3层bp神经网络的python实现，取自这里，我做了一些修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# -*- coding:utf-8 -*-import timeimport numpy as npfrom sklearn import datasetsimport matplotlib.pyplot as pltdef generate_data(): np.random.seed(0) X, y = datasets.make_moons(200, noise=0.20) return X, ydef visualize(X, y, model): x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = predict(model, np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral) plt.show() plt.title("bp nn")def predict(model, x): layer1, layer2 = model feedforward(x, layer1) feedforward(layer1.a, layer2, True) return np.argmax(layer2.a, axis=1)class Layer(object): def __init__(self, last_layer_dim: int, dim: int): self.W = np.random.randn(last_layer_dim, dim) / np.sqrt(last_layer_dim) self.b = np.zeros((1, dim)) self.z = None self.a = None self.delta = None self.dW = None self.db = Nonedef softmax(X): # m行dim列 exp_scores = np.exp(X) probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) return probsdef feedforward(x: np.ndarray, layer: Layer, is_final: bool=False): layer.z = x.dot(layer.W) + layer.b # m行dim列 if is_final: layer.a = softmax(layer.z) else: layer.a = np.tanh(layer.z)def backprop(x: np.ndarray, y: np.ndarray, layer: Layer, nextlayer: Layer, num_examples: int, is_final: bool=False): if is_final: delta = layer.a delta[range(num_examples), y] -= 1 # m行dim列, a_&#123;[l]&#125;^&#123;(i)&#125; - 1&#123;y_&#123;(i)&#125;=l&#125; # layer.dW = (x.T).dot(delta) / num_examples # layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples layer.dW = (x.T).dot(delta) layer.db = np.sum(delta, axis=0, keepdims=True) layer.delta = delta else: delta = nextlayer.delta.dot(nextlayer.W.T) * (1 - np.power(layer.a, 2)) # * 对应元素相乘 # layer.dW = np.dot(x.T, delta) / num_examples # layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples layer.dW = np.dot(x.T, delta) layer.db = np.sum(delta, axis=0, keepdims=True) layer.delta = delta learn_rate = 0.01 layer.W += -learn_rate * layer.dW layer.b += -learn_rate * layer.dbdef build_model(X, y, nn_hdim, num_passes=20000, print_loss=False): num_examples = len(X) np.random.seed(0) input_dim = X.shape[1] nn_output_dim = 2 layer1 = Layer(input_dim, nn_hdim) layer2 = Layer(nn_hdim, nn_output_dim) for i in range(0, num_passes): feedforward(X, layer1) feedforward(layer1.a, layer2, True) backprop(layer1.a, y, layer2, None, num_examples, True) backprop(X, y, layer1, layer2, num_examples) model = [layer1, layer2] return modeldef main(): X, y = generate_data() start_time = time.time() model = build_model(X, y, 3) time_cost = time.time() - start_time summary = f'cost: &#123;time_cost&#125;' print(summary) visualize(X, y, model)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[android开发中的一些经验]]></title>
    <url>%2F2017%2F12%2F01%2Fandroid-issues-md%2F</url>
    <content type="text"><![CDATA[记录一下android开发中的一些经验，本文将持续更新。 jar命令的使用JAR文件即 Java Archive File，是 Java 的一种文档格式。JAR 文件实际上就是 ZIP 文件，使用unzip xxx.jar -d dest/命令即可解压。 jar命令的说明如下： 下面是jar命令的一些常用用法： 显示jar包 1jar tvf hello.jar # 查看hello.jar包的内容 解压jar包 1jar xvf hello.jar # 解压hello.jar至当前目录 更多用法参考 JAR命令&amp;JAR包详解 java的反编译很多情况下，当我们使用别人的jar包时，很有可能会遇到一些问题，这时需要了解一下jar包里到底有些什么，方便定位问题所在。jar命令只是对jar包进行了一下解包，想查看其中的代码细节，还需要进行反编译。Java Decompiler是一款比较好用的反编译软件，在实际使用中，能够反编译出大部分的java代码。eclipse、android studio安装Java Decompiler插件后，在使用第三方jar库时，会减少很多烦恼。 获取apk的签名和MD5指纹在很多情况下，需要确认apk的签名。例如，接入渠道sdk后，sdk的初始化或者登陆、支付等功能出现了问题，就有可能是apk的SHA1签名与在渠道后台配置的SHA1签名不一致导致的。 获取apk的SHA1签名的一般步骤是: 解压apk：unzip testapp.apk。 找到解压出来的RSA文件。 将终端切到RSA文件所在的目录, 在命令行输入 keytool -printcert -file ./***.RSA ，即可获取sha1签名和md5指纹。具体操作如图 apktool的使用解包和打包apktool 是一款逆向工程工具。在这里有相关的文档。 基础功能 解压apk，生成可读的AndroidManifest.xml， 1apktool d testapp.apk 解压jar 1apktool d foo.jar smali调试，参考wiki 查看apk里面的java源代码 下载dex2jar 使用unzip解压apk：unzip testapp.apk 使用dex2jar将dex文件转换成jar文件 使用jd-gui打开生成的classes-dex2jar.jar，就可以查看java源代码了 具体操作如图 修改apk里面的内容 使用apktool解压apk，apktool d testapp.apk 更新so包，比如可以将release版的so替换成debug版的so 更新资源文件 更改smali，smali的语法可以参考这篇文章 打包回apk，打包出来的文件在apk文件夹中的dist目录下 对重新打包后的apk文件进行签名1jarsigner -verbose -keystore yourKey.keystore -storepass yourPassword path/to/apk /path/to/keystore/file 查看apk的一些信息使用android studio直接打开apk包，可以快速的浏览apk中的一些信息，但是会在用户文件夹生成一个临时工程，占用c盘资源。 log查看目前常用的获取android上面的log的方法有这些： 直接使用logcat命令，一般是将logcat的输出重定向到文件，然后再在文件里查找需要的信息 使用eclipse或者android studio的logcat窗口查看log。eclipse在非调试模式下，logcat窗口并不是很好用；android studio有一定几率连不上设备，这时很会恼火。 app收集log并显示在屏幕上，由于log有可能会刷新得很快，在设备的屏幕上，想定位到具体信息并不是很方便，并且这种方式对app的性能有一定的影响。 个人感觉比较好的方式是参考一下AirDroid，开发一个独立的app用于收集android上的log，这个app开启一个web服务，并将收集到的log输出到这个web上，在电脑上的浏览器中访问这个web，可以获取到设备上的实时log。 一些c/c++库在android上的编译 编译Boost，参考Boost-for-Android 编译Python，参考python的issue30386 android studio引用外部工程很多时候，库工程并不是放在项目文件夹下面，而是放在其他位置，常见的原因是想将这个库工程作为一个公共的库，在几个项目之间使用。 android studio中引用外部库的方法是这样的，在项目的settings.gradle文件中添加如下语句：12include &apos;:BaiduLBS&apos;project(&apos;:BaiduLBS&apos;).projectDir = new File(settingsDir, &apos;../platform/android/sdk/BaiduLBS&apos;) 即可添加相对路径在../platform/android/sdk/BaiduLBS处的外部库BaiduLBS。 横竖屏的问题当手机进行横竖屏切换，弹出键盘，窗口大小发生变化等情况发生时，activity会重新走一遍OnCreate等生命周期方法。要避免这种行为，需要在AndroidManifest.xml中，为activity添加android:configChanges属性，例如1234&lt;activity android:name="com.xxx.MainActivity" android:configChanges="orientation|keyboardHidden|screenSize" &gt;&lt;/activity&gt; 这时，当有orientation、keyboardHidden、screenSize情况发生时，就不会重建activity并调用OnCreate等方法，而是调用原实例的onConfigurationChanged方法。 游戏画面只有屏幕一半在android手机上玩游戏时，经常会遇到这样一种情况，就是不知道做了一些什么操作，屏幕上只有部分(不一定是1/2)区域有游戏画面，其余区域是黑色的。 个人感觉是经过了某些操作，导致GLSurfaceView的Layout有问题。我使用下面的方法试了一下，能够很大程度的降低这种情况的发生123456789101112131415161718192021// 固定为横屏private void resetGLSurfaceViewSize() &#123; DisplayMetrics dm = new DisplayMetrics(); getWindowManager().getDefaultDisplay().getMetrics(dm); int w = dm.widthPixels; int h = dm.heightPixels; Log.d(TAG, "屏幕的分辨率为：" + w + "*" + h); if (w &lt;= 0 || h &lt;= 0) &#123; return; &#125; w = w &gt; h ? w : h; h = w &gt; h ? h : w; mGLSurfaceView.getLayoutParams().width = w; mGLSurfaceView.getLayoutParams().height = h;&#125;@Overridepublic void onConfigurationChanged(Configuration newConfig) &#123; super.onConfigurationChanged(newConfig); resetGLSurfaceViewSize();&#125; 多次点击app icon的问题如果一个游戏接入了第3方的登录模块，当打开游戏弹出第3方的登录框时，按下home键回到桌面，再次点击这个游戏的icon，期望能返回游戏，并且登录框不被清除，这时需要将游戏activity的launchMode设置为singleTop，如下1234&lt;activity android:name="com.xxx.MainActivity" android:launchMode="singleTop" &gt;&lt;/activity&gt;]]></content>
      <categories>
        <category>android</category>
      </categories>
      <tags>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HLSL 常用函数]]></title>
    <url>%2F2017%2F12%2F01%2Fhlsl-library%2F</url>
    <content type="text"><![CDATA[HLSL是unity推荐的shader语言，HLSL和Cg很相似。这里整理了一下网络上收集到的相关资料，方便自己学习和查询。 HLSL固有函数Intrinsic Functions (DirectX HLSL) 函数名 用法 描述 Description Minimum shader model abs abs(x) 计算输入值的绝对值。 Absolute value (per component). 11 acos acos(x) 返回输入值反余弦值。 Returns the arccosine of each component of x. 11 all all(x) 测试非0值。 Test if all components of x are nonzero. 11 AllMemoryBarrier Blocks execution of all threads in a group until all memory accesses have been completed. 5 AllMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all memory accesses have been completed and all threads in the group have reached this call. 5 any any(x) 测试输入值中的任何非零值。 Test if any component of x is nonzero. 11 asdouble Reinterprets a cast value into a double. 5 asfloat asfloat(x) Convert the input type to a float. 4 asin asin(x) 返回输入值的反正弦值。 Returns the arcsine of each component of x. 11 asint asint(x) Convert the input type to an integer. 4 asuint Reinterprets the bit pattern of a 64-bit type to a uint. 5 asuint asuint(x) Convert the input type to an unsigned integer. 4 atan atan(x) 返回输入值的反正切值。 Returns the arctangent of x. 11 atan2 atan2(y, x) 返回y/x的反正切值。 Returns the arctangent of of two values (x,y). 11 ceil ceil(x) 返回大于或等于输入值的最小整数。 Returns the smallest integer which is greater than or equal to x. 11 clamp clamp(x, min, max) 把输入值限制在[min, max]范围内。 Clamps x to the range [min, max]. 11 clip clip(x) 如果输入向量中的任何元素小于0，则丢弃当前像素。 Discards the current pixel, if any component of x is less than zero. 11 cos cos(x) 返回输入值的余弦。 Returns the cosine of x. 11 cosh cosh(x) 返回输入值的双曲余弦。 Returns the hyperbolic cosine of x. 11 countbits Counts the number of bits (per component) in the input integer. 5 cross cross(x, y) 返回两个3D向量的叉积。 Returns the cross product of two 3D vectors. 11 D3DCOLORtoUBYTE4 D3DCOLORtoUBYTE4(x) Swizzles and scales components of the 4D vector xto compensate for the lack of UBYTE4 support in some hardware. 11 ddx ddx(x) 返回关于屏幕坐标x轴的偏导数。 Returns the partial derivative of x with respect to the screen-space x-coordinate. 21 ddx_coarse Computes a low precision partial derivative with respect to the screen-space x-coordinate. 5 ddx_fine Computes a high precision partial derivative with respect to the screen-space x-coordinate. 5 ddy ddy(x) 返回关于屏幕坐标y轴的偏导数。 Returns the partial derivative of x with respect to the screen-space y-coordinate. 21 ddy_coarse Computes a low precision partial derivative with respect to the screen-space y-coordinate. 5 ddy_fine Computes a high precision partial derivative with respect to the screen-space y-coordinate. 5 degrees degrees(x) 弧度到角度的转换 Converts x from radians to degrees. 11 determinant determinant(m) 返回输入矩阵的值。 Returns the determinant of the square matrix m. 11 DeviceMemoryBarrier Blocks execution of all threads in a group until all device memory accesses have been completed. 5 DeviceMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all device memory accesses have been completed and all threads in the group have reached this call. 5 distance distance(x, y) 返回两个输入点间的距离。 Returns the distance between two points. 11 dot dot(x, y) 返回两个向量的点积。 Returns the dot product of two vectors. 1 dst Calculates a distance vector. 5 EvaluateAttributeAtCentroid Evaluates at the pixel centroid. 5 EvaluateAttributeAtSample Evaluates at the indexed sample location. 5 EvaluateAttributeSnapped Evaluates at the pixel centroid with an offset. 5 exp exp(x) 返回以e为底数，输入值为指数的指数函数值。 Returns the base-e exponent. 11 exp2 exp2(x) 返回以2为底数，输入值为指数的指数函数值。 Base 2 exponent (per component). 11 f16tof32 Converts the float16 stored in the low-half of the uint to a float. 5 f32tof16 Converts an input into a float16 type. 5 faceforward faceforward(n, i, ng) 检测多边形是否位于正面。 Returns -n * sign(dot(i, ng)). 11 firstbithigh Gets the location of the first set bit starting from the highest order bit and working downward, per component. 5 firstbitlow Returns the location of the first set bit starting from the lowest order bit and working upward, per component. 5 floor floor(x) 返回小于等于x的最大整数。 Returns the greatest integer which is less than or equal to x. 11 fmod fmod(x, y) 返回a / b的浮点余数。 Returns the floating point remainder of x/y. 11 frac frac(x) 返回输入值的小数部分。 Returns the fractional part of x. 11 frexp frexp(x, exp) 返回输入值的尾数和指数 Returns the mantissa and exponent of x. 21 fwidth fwidth(x) 返回 abs(ddx(x)) + abs(ddy(x)) 。 Returns abs(ddx(x)) + abs(ddy(x)) 21 GetRenderTargetSampleCount GetRenderTargetSampleCount() Returns the number of render-target samples. 4 GetRenderTargetSamplePosition GetRenderTargetSamplePosition(x) Returns a sample position (x,y) for a given sample index. 4 GroupMemoryBarrier Blocks execution of all threads in a group until all group shared accesses have been completed. 5 GroupMemoryBarrierWithGroupSync Blocks execution of all threads in a group until all group shared accesses have been completed and all threads in the group have reached this call. 5 InterlockedAdd Performs a guaranteed atomic add of value to the dest resource variable. 5 InterlockedAnd Performs a guaranteed atomic and. 5 InterlockedCompareExchange Atomically compares the input to the comparison value and exchanges the result. 5 InterlockedCompareStore Atomically compares the input to the comparison value. 5 InterlockedExchange Assigns value to dest and returns the original value. 5 InterlockedMax Performs a guaranteed atomic max. 5 InterlockedMin Performs a guaranteed atomic min. 5 InterlockedOr Performs a guaranteed atomic or. 5 InterlockedXor Performs a guaranteed atomic xor. 5 isfinite isfinite(x) 如果输入值为有限值则返回true，否则返回false。 Returns true if x is finite, false otherwise. 11 isinf isinf(x) 如何输入值为无限的则返回true。 Returns true if x is +INF or -INF, false otherwise. 11 isnan isnan(x) 如果输入值为NAN或QNAN则返回true。 Returns true if x is NAN or QNAN, false otherwise. 11 ldexp ldexp(x, exp) frexp的逆运算，返回 x * 2 ^ exp。 Returns x * 2exp 11 length length(v) Returns the length of the vector v. 11 lerp lerp(x, y, s) 对输入值进行插值计算。 Returns x + s(y - x). 11 lit lit(n • l, n • h, m) 返回光照向量（环境光，漫反射光，镜面高光，1）。 Returns a lighting vector (ambient, diffuse, specular, 1) 11 log log(x) 返回以e为底的对数。 Returns the base-e logarithm of x. 11 log10 log10(x) 返回以10为底的对数。 Returns the base-10 logarithm of x. 11 log2 log2(x) 返回以2为底的对数。 Returns the base-2 logarithm of x. 11 mad Performs an arithmetic multiply/add operation on three values. 5 max max(x, y) 返回两个输入值中较大的一个。 Selects the greater of x and y. 11 min min(x, y) 返回两个输入值中较小的一个。 Selects the lesser of x and y. 11 modf modf(x, out ip) 把输入值分解为整数和小数部分。 Splits the value x into fractional and integer parts. 11 mul mul(x, y) 返回输入矩阵相乘的积。 Performs matrix multiplication using x and y. 1 noise noise(x) Generates a random value using the Perlin-noise algorithm. 11 normalize normalize(x) 返回规范化的向量，定义为 x / length(x)。 Returns a normalized vector. 11 pow pow(x, y) 返回输入值的指定次幂。 Returns xy. 11 Process2DQuadTessFactorsAvg Generates the corrected tessellation factors for a quad patch. 5 Process2DQuadTessFactorsMax Generates the corrected tessellation factors for a quad patch. 5 Process2DQuadTessFactorsMin Generates the corrected tessellation factors for a quad patch. 5 ProcessIsolineTessFactors Generates the rounded tessellation factors for an isoline. 5 ProcessQuadTessFactorsAvg Generates the corrected tessellation factors for a quad patch. 5 ProcessQuadTessFactorsMax Generates the corrected tessellation factors for a quad patch. 5 ProcessQuadTessFactorsMin Generates the corrected tessellation factors for a quad patch. 5 ProcessTriTessFactorsAvg Generates the corrected tessellation factors for a tri patch. 5 ProcessTriTessFactorsMax Generates the corrected tessellation factors for a tri patch. 5 ProcessTriTessFactorsMin Generates the corrected tessellation factors for a tri patch. 5 radians radians(x) 角度到弧度的转换。 Converts x from degrees to radians. 1 rcp Calculates a fast, approximate, per-component reciprocal. 5 reflect reflect(i, n) 返回入射光线i对表面法线n的反射光线。 Returns a reflection vector. 1 refract refract(i, n, R) 返回在入射光线i，表面法线n，折射率为eta下的折射光线v。 Returns the refraction vector. 11 reversebits Reverses the order of the bits, per component. 5 round round(x) 返回最接近于输入值的整数。 Rounds x to the nearest integer 11 rsqrt rsqrt(x) 返回输入值平方根的倒数。 Returns 1 / sqrt(x) 11 saturate saturate(x) 把输入值限制到[0, 1]之间。 Clamps x to the range [0, 1] 1 sign sign(x) 计算输入值的符号。 Computes the sign of x. 11 sin sin(x) 计算输入值的正弦值。 Returns the sine of x 11 sincos sincos(x, out s, out c) 返回输入值的正弦和余弦值。 Returns the sine and cosine of x. 11 sinh sinh(x) 返回x的双曲正弦。 Returns the hyperbolic sine of x 11 smoothstep smoothstep(min, max, x) 返回一个在输入值之间平稳变化的插值。 Returns a smooth Hermite interpolation between 0 and 1. 11 sqrt sqrt(x) 返回输入值的平方根。 Square root (per component) 11 step step(a, x) 返回（x &gt;= a）? 1 : 0。 Returns (x &gt;= a) ? 1 : 0 11 tan tan(x) 返回输入值的正切值。 Returns the tangent of x 11 tanh tanh(x) Returns the hyperbolic tangent of x 11 tex1D(s, t) 1D纹理查询。 1D texture lookup. 1 tex1D(s, t, ddx, ddy) 1D texture lookup. 21 tex1Dbias tex1Dbias(s, t) 1D texture lookup with bias. 21 tex1Dgrad tex1Dgrad(s, t, ddx, ddy) 1D texture lookup with a gradient. 21 tex1Dlod tex1Dlod(s, t) 1D texture lookup with LOD. 31 tex1Dproj tex1Dproj(s, t) 1D texture lookup with projective divide. 21 tex2D(s, t) 2D纹理查询。 2D texture lookup. 11 tex2D(s, t, ddx, ddy) 2D texture lookup. 21 tex2Dbias tex2Dbias(s, t) 2D texture lookup with bias. 21 tex2Dgrad tex2Dgrad(s, t, ddx, ddy) 2D texture lookup with a gradient. 21 tex2Dlod tex2Dlod(s, t) 2D texture lookup with LOD. 3 tex2Dproj tex2Dproj(s, t) 2D texture lookup with projective divide. 21 tex3D(s, t) 3D纹理查询。 3D texture lookup. 11 tex3D(s, t, ddx, ddy) 3D texture lookup. 21 tex3Dbias tex3Dbias(s, t) 3D texture lookup with bias. 21 tex3Dgrad tex3Dgrad(s, t, ddx, ddy) 3D texture lookup with a gradient. 21 tex3Dlod tex3Dlod(s, t) 3D texture lookup with LOD. 31 tex3Dproj tex3Dproj(s, t) 3D texture lookup with projective divide. 21 texCUBE(s, t) 立方纹理查询。 Cube texture lookup. 11 texCUBE(s, t, ddx, ddy) Cube texture lookup. 21 texCUBEbias texCUBEbias(s, t) Cube texture lookup with bias. 21 texCUBEgrad texCUBEgrad(s, t, ddx, ddy) Cube texture lookup with a gradient. 21 texCUBElod Cube texture lookup with LOD. 31 texCUBEproj texCUBEproj(s, t) Cube texture lookup with projective divide. 21 transpose transpose(m) 返回输入矩阵的转置。 Returns the transpose of the matrix m. 1 trunc trunc(x) Truncates floating-point value(s) to integer value(s) 1 参考链接 每天30分钟看Shader—(1)HLSL固有函数 【Intrinsic Functions (DirectX HLSL)】]]></content>
      <categories>
        <category>shader</category>
      </categories>
      <tags>
        <tag>shader</tag>
      </tags>
  </entry>
</search>
