<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[windows上安装tensorflow]]></title>
      <url>/2017/12/16/install-tensorflow/</url>
      <content type="html"><![CDATA[<h3 id="访问-tensorflow-官网"><a href="#访问-tensorflow-官网" class="headerlink" title="访问 tensorflow 官网"></a>访问 tensorflow 官网</h3><p>方法一: 直接访问 <a href="https://tensorflow.google.cn" target="_blank" rel="noopener">https://tensorflow.google.cn</a></p>
<a id="more"></a>
<p>方法二:</p>
<ol>
<li><p>修改 hosts 文件<br>windows的hosts文件的位置是 <code>C:\Windows\System32\drivers\etc\hosts</code> ，在hosts文件末尾添加如下内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">64.233.188.121 www.tensorflow.org</span><br></pre></td></tr></table></figure>
</li>
<li><p>刷新dns缓存，在命令行执行下面命令(windows)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ipconfig /flushdns</span><br></pre></td></tr></table></figure>
</li>
<li><p>在浏览器访问 www.tensorflow.org ，就可以浏览官网了。</p>
</li>
</ol>
<h3 id="安装cuda-win"><a href="#安装cuda-win" class="headerlink" title="安装cuda(win)"></a>安装cuda(win)</h3><p>在nvidia的官网下载cuda的安装包，按照默认选项安装就可以了。如果不出意外，安装完毕后，cuda的bin文件夹会自动添加到环境变量。</p>
<p>在NVIDIA官网的<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">cudnn</a>页面，下载与cuda适配的cudnn压缩包。解压后，会有bin、include、lib三个文件夹，将这三个文件夹拷贝到cuda的安装文件夹下面，并与cuda的bin、include、lib文件夹合并。</p>
<p>如果下载时的网络连接不稳定，推荐使用<code>wget -c url</code>命令下载。</p>
<h3 id="通过pip安装"><a href="#通过pip安装" class="headerlink" title="通过pip安装"></a>通过pip安装</h3><p>如果cuda和cudnn的版本符合pip包的要求，可以直接通过pip安装<br><code>pip3 install tensorflow-gpu</code></p>
<h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><p>我一般是在虚拟机里使用linux，而虚拟机无法使用gpu加速，因此暂时不考虑在虚拟机的linux里编译tensorflow。tensorflow的windows编译有很多问题，很麻烦。想在windows机器上自己编译，最好的方案是等待wsl支持gpu后，在wsl中编译tensorflow。在<a href="https://wpdev.uservoice.com/forums/266908-command-prompt-console-bash-on-ubuntu-on-windo/suggestions/16108045-opencl-cuda-gpu-support" target="_blank" rel="noopener">OpenCL &amp; CUDA GPU support</a>，<a href="https://github.com/Microsoft/WSL/issues/829" target="_blank" rel="noopener">Cannot find GPU devices on Bash</a>可以关注一下wsl支持gpu的最新进展。</p>
<p>编译的过程中会下载一些文件，需要留意一下这些下载地址能否访问到。如果<code>git clone</code>时提示 <code>... port 443: Timed out</code>，则表明这个地址被屏蔽了，为了简单省事，最好还是用shadowsocks。开启shadowsocks后，设置git的http/https代理协议<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.proxy &apos;socks5://127.0.0.1:1080&apos;</span><br><span class="line">git config --global https.proxy &apos;socks5://127.0.0.1:1080&apos;</span><br></pre></td></tr></table></figure></p>
<p>编译完毕后，取消代理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --unset http.proxy</span><br><span class="line">git config --global --unset https.proxy</span><br></pre></td></tr></table></figure></p>
<p>可以查看一下git的配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global -l</span><br></pre></td></tr></table></figure></p>
<p>为了避免路径中有空格造成的烦恼，建议目录使用符号链接，例如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mklink /D C:\CUDA &quot;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA&quot;</span><br></pre></td></tr></table></figure></p>
<h3 id="检测gpu是否开启"><a href="#检测gpu是否开启" class="headerlink" title="检测gpu是否开启"></a>检测gpu是否开启</h3><p>在python的交互环境下，输入如下代码，可以查看有哪些设备<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">print(device_lib.list_local_devices())</span><br></pre></td></tr></table></figure></p>
<p>实际上在tensorflow开始运行后，输出的log中会有device信息，可以判断是否启用了gpu。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>下面代码是之前的bp神经网络的tensorflow实现，可以用来测试一下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, dim_in, dim_out, layer_n, is_output_layer=False, y=None)</span>:</span></span><br><span class="line">    layer_name = <span class="string">f'layer<span class="subst">&#123;layer_n&#125;</span>'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([dim_in, dim_out]))  <span class="comment"># Weight中都是随机变量</span></span><br><span class="line">            tf.summary.histogram(layer_name + <span class="string">"/weights"</span>, Weights)  <span class="comment"># 可视化观看变量</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</span><br><span class="line">            biases = tf.Variable(tf.zeros([<span class="number">1</span>, dim_out]))  <span class="comment"># biases推荐初始值不为0</span></span><br><span class="line">            tf.summary.histogram(layer_name + <span class="string">"/biases"</span>, biases)  <span class="comment"># 可视化观看变量</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'z'</span>):</span><br><span class="line">            z = tf.matmul(inputs, Weights) + biases  <span class="comment"># inputs*Weight+biases</span></span><br><span class="line">            tf.summary.histogram(layer_name + <span class="string">"/z"</span>, z)  <span class="comment"># 可视化观看变量</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> is_output_layer:</span><br><span class="line">            outputs = tf.nn.softmax(z, name=<span class="string">'outputs'</span>)</span><br><span class="line">            loss = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=y, name=<span class="string">'loss'</span>)</span><br><span class="line">            tf.summary.histogram(layer_name + <span class="string">'/loss'</span>, loss)  <span class="comment"># 可视化观看变量</span></span><br><span class="line">            <span class="keyword">return</span> outputs, loss</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            outputs = tf.nn.tanh(z)</span><br><span class="line">            tf.summary.histogram(layer_name + <span class="string">"/outputs"</span>, outputs)  <span class="comment"># 可视化观看变量</span></span><br><span class="line">            <span class="keyword">return</span> outputs, <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X_data, y_data = datasets.make_moons(<span class="number">200</span>, noise=<span class="number">0.20</span>)</span><br><span class="line">num_examples = len(X_data)</span><br><span class="line">ym_data = np.zeros((num_examples, <span class="number">2</span>))</span><br><span class="line">ym_data[range(num_examples), y_data] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个带可展开符号的域</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</span><br><span class="line">    xs = tf.placeholder(tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">    ys = tf.placeholder(tf.float32, name=<span class="string">'y'</span>)</span><br><span class="line"></span><br><span class="line">tf.set_random_seed(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 三层神经网络，输入层（2个神经元），隐藏层（3神经元），输出层（2个神经元）</span></span><br><span class="line">layer1, _ = add_layer(xs, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">predict_step, loss = add_layer(layer1, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="keyword">True</span>, ys)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)  <span class="comment"># 0.01学习率,minimize(loss)减小loss误差</span></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line"><span class="comment"># https://tensorflow.google.cn/tutorials/using_gpu#allowing_gpu_memory_growth</span></span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">sess = tf.Session(config=config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并到Summary中</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选定可视化存储目录</span></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"./"</span>, sess.graph)</span><br><span class="line"></span><br><span class="line">sess.run(init)  <span class="comment"># 先执行init</span></span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># 训练2w次</span></span><br><span class="line">num_passes = <span class="number">20000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_passes):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: X_data, ys: ym_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        result = sess.run(merged, feed_dict=&#123;xs: X_data, ys: ym_data&#125;)  <span class="comment"># merged也是需要run的</span></span><br><span class="line">        writer.add_summary(result, i)  <span class="comment"># result是summary类型的，需要放入writer中，i步数（x轴）</span></span><br><span class="line">time_cost = time.time() - start_time</span><br><span class="line">summary_text = <span class="string">f'cost: <span class="subst">&#123;time_cost&#125;</span>'</span></span><br><span class="line">print(summary_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------- predict ---------------------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x)</span>:</span></span><br><span class="line">    predict = sess.run(predict_step, feed_dict=&#123;xs: x&#125;)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(predict, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.title(<span class="string">"bp nn"</span>)</span><br><span class="line"></span><br><span class="line">visualize(X_data, y_data)</span><br></pre></td></tr></table></figure></p>
<p>运行完上面的脚本后，在该脚本目录下执行<code>tensorboard --logdir=&quot;./&quot;</code>命令，就可以在浏览器中查看tensorboard。</p>
]]></content>
      
        <categories>
            
            <category> ml </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ml </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在illustrator中使用latex公式]]></title>
      <url>/2017/12/04/illustrator-latex/</url>
      <content type="html"><![CDATA[<p>Adobe Illustrator 是一款强大的矢量图形软件，当我们在里面做好图形后，可能需要在里面插入一些数学符号或者数学公式什么的，这时一般考虑使用 Latex 来输出这些特殊文本。下面介绍一下如何在 Illustrator 中使用 Latex 。</p>
<a id="more"></a>
<h2 id="安装-LaTeX"><a href="#安装-LaTeX" class="headerlink" title="安装 LaTeX"></a>安装 LaTeX</h2><p>LaTeX 有很多发行版，在<a href="http://latex.org/know-how/latex-distributions" target="_blank" rel="noopener">这里</a>可以查看到。我在windows环境下使用的是 <a href="https://miktex.org/" target="_blank" rel="noopener">MikTeX</a>，在<a href="https://miktex.org/download" target="_blank" rel="noopener">这里</a>下载了安装包后，按照正常的Windows软件安装流程进行安装，途中可能会提示安装一些依赖，按照提示进行操作即可。</p>
<p>安装完 Latex 后，需要确认一下，从 PATH 环境变量里能否搜索到 pdflatex 命令，如果不能，则需要将 pdflatex 所在的文件夹添加到 PATH 环境变量里。</p>
<h2 id="安装-LaTeX-字体"><a href="#安装-LaTeX-字体" class="headerlink" title="安装 LaTeX 字体"></a>安装 LaTeX 字体</h2><p>LaTeX 会使用一些特殊的字体，为了让导出的文本能够在 Illustrator 中正常显示，需要让 Illustrator 能够搜索到这些字体。比较简单省事的做法是将这些字体拷贝到adobe的字体文件夹里。例如在Windows上，将 <code>D:\Program Files\MiKTeX 2.9\fonts\type1\public\amsfonts\cm</code> 文件夹里的文件全部拷贝到 <code>C:\Program Files\Common Files\Adobe\Fonts</code> 文件夹。</p>
<h2 id="将-LaTeX-导出到-Illustrator"><a href="#将-LaTeX-导出到-Illustrator" class="headerlink" title="将 LaTeX 导出到 Illustrator"></a>将 LaTeX 导出到 Illustrator</h2><p>Illustrator 可以从PDF文件中导入单独的页面，因此，可以先将 Latex 代码编译成PDF(可以使用pdflatex)，再将PDF导入到 Illustrator 中去。</p>
<p>为了简化 <code>Latex -&gt; PDF -&gt; Illustrator</code> 这一过程，有一些脚本可供使用，例如 <a href="https://github.com/mkuznets/latex-illustrator" target="_blank" rel="noopener">latex-illustrator</a> 和 <a href="https://dl.dropboxusercontent.com/s/otp2zdmqx2peaf5/illustratorLatexEquations.zip?dl=0" target="_blank" rel="noopener">illustratorLatexEquations</a>等。</p>
<p>根据我的实际使用情况，我对脚本做了一下修改，代码如下：<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// latex2illustrator.js</span></span><br><span class="line"><span class="keyword">var</span> PDF_LATEX_EXE = <span class="string">"pdflatex.exe"</span>; <span class="comment">// Add full path if necessary</span></span><br><span class="line"><span class="keyword">var</span> LAST_TEX_CODE_FILE = <span class="string">'latex2illustrator_lastcode.txt'</span>;</span><br><span class="line"><span class="keyword">var</span> TEX_FILE = <span class="string">'latex2illustrator.tex'</span>;</span><br><span class="line"><span class="keyword">var</span> PDF_FILE = <span class="string">'latex2illustrator.pdf'</span>;</span><br><span class="line"><span class="keyword">var</span> BAT_FILE = <span class="string">'latex2illustrator.bat'</span>;</span><br><span class="line"><span class="keyword">var</span> TEMP_PATH = getWorkPath();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getWorkPath</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="comment">// determining the local temporary directory</span></span><br><span class="line">    <span class="keyword">var</span> temppath = Folder.temp.fsName; <span class="comment">// path already in Windows syntax: c:\...</span></span><br><span class="line">    <span class="keyword">var</span> i = temppath.indexOf(<span class="string">"Temporary Internet Files"</span>);</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= <span class="number">0</span>) temppath = temppath.substr(<span class="number">0</span>, i + <span class="number">4</span>);</span><br><span class="line">    <span class="comment">//temppath should now contain something like C:\Documents and Settings\&lt;user&gt;\Local Settings\Temp</span></span><br><span class="line">    <span class="keyword">return</span> temppath</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">getLastCode</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="comment">// remember the last user input in a text file</span></span><br><span class="line">    <span class="keyword">var</span> lastCode = <span class="string">"$$"</span></span><br><span class="line">    <span class="keyword">var</span> lastCodeFile = File(TEMP_PATH + <span class="string">"\\"</span> + LAST_TEX_CODE_FILE);</span><br><span class="line">    <span class="keyword">if</span> (lastCodeFile.exists) &#123;</span><br><span class="line">        lastCodeFile.open(<span class="string">"r"</span>);</span><br><span class="line">        lastCode = lastCodeFile.read();</span><br><span class="line">        lastCodeFile.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> lastCode</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">writeLatexFile</span>(<span class="params">latexCode</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// add latex header etc. to create a complete latex document</span></span><br><span class="line">    <span class="keyword">var</span> latexFile = <span class="keyword">new</span> File(TEMP_PATH + <span class="string">'\\'</span> + TEX_FILE);</span><br><span class="line">    latexFile.open(<span class="string">"w"</span>);</span><br><span class="line">    <span class="comment">// latexFile.writeln("\\documentclass&#123;standalone&#125;");</span></span><br><span class="line">    latexFile.writeln(<span class="string">"\\documentclass&#123;article&#125;"</span>);</span><br><span class="line">    <span class="comment">// add or remove additional latex packages here</span></span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;amsmath&#125;"</span>);</span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;amsthm&#125;"</span>);</span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;amssymb&#125;"</span>);</span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;gensymb&#125;"</span>); <span class="comment">// for \degree</span></span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;textcomp&#125;"</span>); <span class="comment">// for \textdegree</span></span><br><span class="line">    latexFile.writeln(<span class="string">"\\usepackage&#123;bm&#125;"</span>); <span class="comment">// bold math</span></span><br><span class="line">    latexFile.writeln(<span class="string">"\\begin&#123;document&#125;"</span>);</span><br><span class="line">    latexFile.writeln(<span class="string">"\\pagestyle&#123;empty&#125;"</span>); <span class="comment">// no page number</span></span><br><span class="line">    latexFile.writeln(latexCode);</span><br><span class="line">    latexFile.writeln(<span class="string">"\\end&#123;document&#125;"</span>);</span><br><span class="line">    latexFile.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">generate</span>(<span class="params">latexcode</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> pdfFile = File(TEMP_PATH + <span class="string">"\\"</span> + PDF_FILE);</span><br><span class="line">    <span class="keyword">if</span> (pdfFile.exists)</span><br><span class="line">        pdfFile.remove();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create a batch file calling latex</span></span><br><span class="line">    <span class="keyword">var</span> batchFile = <span class="keyword">new</span> File(TEMP_PATH + <span class="string">'\\'</span> + BAT_FILE);</span><br><span class="line">    batchFile.open(<span class="string">"w"</span>);</span><br><span class="line">    batchFile.writeln(PDF_LATEX_EXE + <span class="string">' -aux-directory="'</span> + TEMP_PATH + <span class="string">'" -include-directory="'</span> + TEMP_PATH + <span class="string">'" -output-directory="'</span> + TEMP_PATH + <span class="string">'" "'</span> + TEMP_PATH + <span class="string">'\\'</span> + TEX_FILE + <span class="string">'"'</span>);</span><br><span class="line">    <span class="comment">//batchFile.writeln('pause');</span></span><br><span class="line">    batchFile.writeln(<span class="string">'del "'</span> + TEMP_PATH + <span class="string">'\\'</span> + BAT_FILE + <span class="string">'"'</span>);</span><br><span class="line">    batchFile.close();</span><br><span class="line">    batchFile.execute();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (; batchFile.exists;)</span><br><span class="line">        <span class="comment">// wait until the batch file has removed itself</span></span><br><span class="line">        <span class="keyword">var</span> pdfFile = File(TEMP_PATH + <span class="string">"\\"</span> + PDF_FILE);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pdfFile.exists) &#123;</span><br><span class="line">        <span class="comment">// import pdf file into the current document</span></span><br><span class="line">        <span class="keyword">var</span> grp = app.activeDocument.activeLayer.groupItems.createFromFile(pdfFile);</span><br><span class="line">        <span class="comment">// The imported objects are grouped twice. Now move the subgroup</span></span><br><span class="line">        <span class="comment">// items to the main group and skip the last item which is the page frame</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">var</span> i = grp.pageItems[<span class="number">0</span>].pageItems.length; --i &gt;= <span class="number">0</span>;)</span><br><span class="line">            grp.pageItems[<span class="number">0</span>].pageItems[i].move(grp, ElementPlacement.PLACEATEND);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> last = grp.pageItems.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (last &gt;= <span class="number">0</span> &amp;&amp; grp.pageItems[last].typename == <span class="string">'PathItem'</span>)</span><br><span class="line">            grp.pageItems[last].remove();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Move the imported objects to the center of the current view.</span></span><br><span class="line">        grp.translate(app.activeDocument.activeView.centerPoint[<span class="number">0</span>] - grp.left, app.activeDocument.activeView.centerPoint[<span class="number">1</span>] - grp.top);</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">        alert(<span class="string">"File "</span> + TEMP_PATH + <span class="string">"\\"</span> + pdfFile.name + <span class="string">" could not be created. LaTeX error?"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">latex2illustrator</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    TEMP_PATH = <span class="string">'E:/data'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> latexCode = getLastCode();</span><br><span class="line">    <span class="keyword">if</span> (latexCode != <span class="literal">null</span>) &#123;</span><br><span class="line">        writeLatexFile(latexCode);</span><br><span class="line">        generate(latexCode);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">latex2illustrator()</span><br></pre></td></tr></table></figure></p>
<p>在<code>E:/data/latex2illustrator_lastcode.txt</code>中写入latex公式，然后在Illustrator中使用ctrl+F12执行latex2illustrator.js脚本，就可以插入数学公式了。</p>
<p>参考链接:</p>
<ol>
<li><a href="http://latex.org/know-how/latexs-friends/61-latexs-friends-others/381-combining-latex-and-illustrator" target="_blank" rel="noopener">Combining LaTeX and Illustrator</a></li>
<li><a href="http://larsonvonh.github.io/data/tools/adobe_tools/latex_equations_illustrator/latex_equations_illustrator.html" target="_blank" rel="noopener">Scripting Adobe Illustrator to create images from LaTeX equations</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> tool </category>
            
        </categories>
        
        
        <tags>
            
            <tag> tool </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[一个简单的BP神经网络的实现]]></title>
      <url>/2017/12/02/bp-neural-network/</url>
      <content type="html"><![CDATA[<p>不详述神经网络模型，只记录一下实现BP神经网络时的推导过程。</p>
<a id="more"></a>
<h2 id="输入值和输出值"><a href="#输入值和输出值" class="headerlink" title="输入值和输出值"></a>输入值和输出值</h2><p>输入值是一个维度是n的特征向量，记作 $x=(x_1,x_2,\ldots,x_n)$。一个数据集里一般有多个样本，假定有m个样本，则将这个数据集记作 $X = (x^{(1)},x^{(2)},\ldots,x^{(m)})$ 。</p>
<p>输出值一般是一个数值，用于表示属于哪个类别。对于m个样本，输出值的集合记作 $y=(y^{(1)}, y^{(2)}, \ldots, y^{(m)})$ 。</p>
<p>输入层的节点个数取决于输入的特征向量的维度。</p>
<p>输出层的节点个数取决于拥有的类别个数。如果只有2类，则可以只用一个输出节点用于预测0或1。</p>
<p>隐藏层的节点越多，则越复杂的函数可以fit到。但是这样做的代价也比较大，首先，会增大训练参数和进行预测的计算量。其次，大量的参数也容易导致过拟合。  需要根据具体情况选择节点的个数。</p>
<p>有一个经验公式可以确定隐含层节点数目，如下</p>
<script type="math/tex; mode=display">
h=\sqrt{m+n} + a</script><p>其中h为隐含层节点数目，m为输入层节点数目，n为输出层节点数目，a为1~10之间的调节常数。</p>
<h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><blockquote>
<p>这里约定上标 (i) 为样本在样本集中的序号，上标 [i] 为神经网络的层的序号，下标 [i] 为网络中某一层的节点的序号，log的底数默认是e。</p>
</blockquote>
<p>神经网络使用正向传播进行预测。</p>
<p>对于一个3层的神经网络，可以这样计算预测值 $\hat y$ ：</p>
<script type="math/tex; mode=display">
\begin{align}
& z^{[1]} = xW^{[1]}+ b^{[1]} \\
& a^{[1]} = \sigma (z^{[1]}) \\
& z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\
& a^{[2]} = \hat y = softmax (z^{[2]}) \\
\end{align}</script><p>神经网络里的计算都需要进行向量化，具体来说</p>
<p>对于第1层的输入值和输出值</p>
<script type="math/tex; mode=display">
X =   \begin{pmatrix}
        x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
        x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)} \\
        \end{pmatrix} = \begin{pmatrix}
        x^{(1)} \\
        x^{(2)} \\
        \vdots \\
        x^{(m)} \\
        \end{pmatrix} , \qquad y = \begin{pmatrix}
        y^{(1)} \\
        y^{(2)} \\
        \vdots \\
        y^{(m)} \\
        \end{pmatrix}</script><p>第2层</p>
<p>假设第2层有u个节点，则这层的权重和偏置为</p>
<script type="math/tex; mode=display">
W^{[1]} = \begin{pmatrix}
        w_{1[1]} & w_{1[2]} & \cdots & w_{1[u]} \\
        w_{2[1]} & w_{2[2]} & \cdots & w_{2[u]} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        w_{n[1]} & w_{n[2]} & \cdots & w_{n[u]} \\
        \end{pmatrix} 
        = \begin{pmatrix}
        w_{[1]} & w_{[2]} &  \ldots & w_{[u]} 
        \end{pmatrix} , \qquad 
b^{[1]} = \begin{pmatrix}
        b_{[1]} & b_{[2]} & \ldots & b_{[u]} \end{pmatrix}</script><p>根据矩阵的分块的性质</p>
<script type="math/tex; mode=display">
\begin{align}
XW^{[1]} &= \begin{pmatrix}
        x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(m)} \\
        \end{pmatrix}
    \begin{pmatrix}
        w_{[1]} & w_{[2]} &  \ldots & w_{[u]} 
        \end{pmatrix} = 
AB = \begin{pmatrix}
        A_{11} \\ A_{21} \\ \vdots \\ A_{m1} \\
        \end{pmatrix}  \begin{pmatrix}
        B_{11} & B_{12} & \ldots & B_{1u} \\
        \end{pmatrix} \\
       & = \begin{pmatrix}
        C_{11} & C_{12} & \cdots & C_{1u} \\
        C_{21} & C_{22} & \cdots & C_{2u} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        C_{m1} & C_{m2} & \cdots & C_{mu} \\
        \end{pmatrix} 
        = \begin{pmatrix}
        A_{11}B_{11} & A_{11}B_{12} & \cdots & A_{11}B_{1u} \\
        A_{21}B_{11} & A_{21}B_{12} & \cdots & A_{21}B_{1u} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        A_{m1}B_{11} & A_{m1}B_{12} & \cdots & A_{m1}B_{1u} \\
        \end{pmatrix}  \\
        & =  \begin{pmatrix}
        x^{(1)}w_{[1]} & x^{(1)}w_{[2]} & \cdots & x^{(1)}w_{[u]} \\
        x^{(2)}w_{[1]} & x^{(2)}w_{[2]} & \cdots & x^{(2)}w_{[u]} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        x^{(m)}w_{[1]} & x^{(m)}w_{[2]} & \cdots & x^{(m)}w_{[u]} \\
        \end{pmatrix}
\end{align}</script><p>参考numpy中的boardcast，则有</p>
<script type="math/tex; mode=display">
\begin{align}
z^{[1]} &= XW^{[1]} + boardcast\ (b^{[1]}) =  \begin{pmatrix}
        x^{(1)}w_{[1]} + b_{[1]} & x^{(1)}w_{[2]} + b_{[2]} & \cdots & x^{(1)}w_{[u]} + b_{[u]} \\
        x^{(2)}w_{[1]} + b_{[1]} & x^{(2)}w_{[2]} + b_{[2]} & \cdots & x^{(2)}w_{[u]} + b_{[u]} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        x^{(m)}w_{[1]} + b_{[1]} & x^{(m)}w_{[2]} + b_{[2]} & \cdots & x^{(m)}w_{[u]} + b_{[u]} \\
        \end{pmatrix} \\
        & =  \begin{pmatrix}
        z_{[1]}^{(1)} & z_{[2]}^{(1)} & \cdots & z_{[u]}^{(1)} \\
        z_{[1]}^{(2)} & z_{[2]}^{(2)} & \cdots & z_{[u]}^{(2)} \\
        \vdots     & \vdots    & \ddots & \vdots \\
        z_{[1]}^{(m)} & z_{[2]}^{(m)} & \cdots & z_{[u]}^{(m)} \\
        \end{pmatrix}
\end{align}</script><script type="math/tex; mode=display">
a^{[1]} =  \begin{pmatrix}
        \sigma(z_{[1]}^{(1)}) & \sigma(z_{[2]}^{(1)}) & \cdots & \sigma(z_{[u]}^{(1)}) \\
        \sigma(z_{[1]}^{(2)}) & \sigma(z_{[2]}^{(2)}) & \cdots & \sigma(z_{[u]}^{(2)}) \\
        \vdots     & \vdots    & \ddots & \vdots \\
        \sigma(z_{[1]}^{(m)}) & \sigma(z_{[2]}^{(m)}) & \cdots & \sigma(z_{[u]}^{(m)}) \\
        \end{pmatrix}</script><p>以上的计算方式可以推广到多层神经网络中的任一隐藏层，对于第$l$层，将 $X$ 类比成 $a^{[l-1]}$ ， $W^{[1]}$ 类比成 $W^{[l]}$ ， $b^{[1]}$ 类比成 $b^{[l]}$ ， $z^{[1]}$ 类比成 $z^{[l]}$ ， $a^{[1]}$ 类比成 $a^{[l]}$ ，就可以了。</p>
<p>对于输出层，因为我们希望网络输出各个分类的概率，所以将输出层的激活函数选为<a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax</a> 。softmax 是一种简便的将原始评分转换成概率的方法。可以将 softmax 看做 logistic 函数的在多分类下的推广。</p>
<script type="math/tex; mode=display">
a^{[2]} =  softmax (z^{[2]}) = \begin{pmatrix}
        softmax(z^{[2](1)}) \\ softmax(z^{[2](2)}) \\ \vdots \\ softmax(z^{[2](m)}) \\
        \end{pmatrix}</script><h3 id="权重和偏置的初始化"><a href="#权重和偏置的初始化" class="headerlink" title="权重和偏置的初始化"></a>权重和偏置的初始化</h3><p>权重 w 不能全部置为0，这样会导致每个层的所有节点的计算都是相同的。如果激活函数使用sigmoid或者tanh，随机出来的 w 最好小一点，一般乘以0.01。因为如果 w 比较大，通过激活函数计算出来的值会落到接近1的位置，导致学习速度变慢。</p>
<p>偏置 b 可以全部初始化为0，但是推荐初始值不为0。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><ul>
<li><a href="https://en.wikipedia.org/wiki/Hyperbolic_function#Tanh" target="_blank" rel="noopener">tanh</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener">sigmoid function</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener">ReLUs</a></li>
</ul>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p><a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax</a> 又称为归一化指数函数，是广义上的 logistic 函数。假设k 维向量 $z$ 的各项是任意实数，使用 softmax 对 $z$ 进行“压缩”处理后，压缩后的向量 $\sigma (z)$ 的每一项的值在 [0, 1] 之间，所有项之和等于1。函数公式如下</p>
<script type="math/tex; mode=display">
\sigma (z)_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad  for\ j = 1, \ldots, K.</script><p>一个简单的例子如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">exp_scores = np.exp(z)</span><br><span class="line">probs = exp_scores / np.sum(exp_scores)  <span class="comment"># 应用 softmax</span></span><br></pre></td></tr></table></figure>
<p>在神经网络的输出层中，有C个分类，对于给定的输入 $z$ ，每个分类的概率可以表示为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    P(t=1|z) \\ P(t=2|z) \\ \vdots \\ P(t=C|z) \\
\end{bmatrix} = \frac {1}{ \sum_{k=1}^C e^{z_k} } \begin{bmatrix}
    e^{z_1} \\ e^{z_2} \\ \vdots \\ e^{z_C} \\
\end{bmatrix}</script><p>其中，$P(t=c|z)$ 表示，在给定输入$z$时，该输入数据是$c$分类的概率。</p>
<p>对于 softmax 函数</p>
<script type="math/tex; mode=display">
a_j = \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \quad  for\ j = 1, \ldots, K.</script><p>softmax 函数的求导过程比较特别，分如下2种情况。这是因为i = j 时，$z_i$ 与 $z_j$ 是同一个变量，按偏导数的定义，将多元函数关于一个自变量求偏导数时，就将其余的自变量看成常数，因此需要分2种情况处理。</p>
<script type="math/tex; mode=display">
\begin{align}
&if \ j=i \\
&\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr)
= \frac { (e^{z_j})^\prime \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_j} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 }
= \frac { e^{z_j} } { \sum_k e^{z_k} } - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_j} } { \sum_k e^{z_k} }
= a_j ( 1-a_j )
\\
&if \ j \neq i \\
&\qquad \frac{ \partial a_j }{ \partial z_i } = \frac{ \partial }{ \partial z_i } \Bigl( \frac {e^{z_j}}{ \sum_{k=1}^K e^{z_k} } \Bigr)
= \frac { 0 \cdot \sum_k e^{z_k} - e^{z_j} \cdot e^{z_i} } { \Bigl( \sum_k e^{z_k} \Bigr)^2 }
= - \frac { e^{z_j} } { \sum_k e^{z_k} } \cdot \frac { e^{z_i} } { \sum_k e^{z_k} }
= -a_j a_i
\end{align}</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>反向传播主要思想是：<br>（1）将训练集数据输入到输入层，经过隐藏层，最后达到输出层并输出结果，这是前向传播过程；<br>（2）由于输出层的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；<br>（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。</p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>我们把定义估计值与实际值之间误差(单个样本)的函数叫作误差损失(loss)函数，代价(cost)函数是各个样本的loss函数的平均。</p>
<p>如果误差损失函数采用二次代价函数 ，在实际中，如果误差越大，参数调整的幅度可能更小，训练更缓慢。使用交叉熵代价函数替换二次代价函数，可以解决学习缓慢的问题。</p>
<blockquote>
<p>示性函数：$1\{\cdot\}$<br>取值规则为：$1\{值为真的表达式\}=1$ ，$1\{值为假的表达式\}=0$ 。举例来说，表达式 $1\{2+2=4\}$的值为1，$1\{1+1=5\}$的值为 0。</p>
</blockquote>
<p>在分类问题中，交叉熵代价函数与对数似然代价函数在形式上是基本一致的。</p>
<p>对于输出层的softmax激活函数，假设有m个样本，k个类别，将$p(x)$记为$1\{ y^{(i)} = j \}$，$q(x)$记为softmax函数的输出值</p>
<p>则根据交叉熵公式，</p>
<script type="math/tex; mode=display">
H(p,q) = - \sum_x p(x)log\ q(x)</script><p>可以得到交叉熵代价函数为：</p>
<script type="math/tex; mode=display">
J(\theta) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m  \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \frac{e^{\theta_j^T x^{(i)}}}{ \sum_{l=1}^k e^{ \theta_l^T x^{(i)} } }  \biggl] \Biggl]</script><p>从似然函数的角度分析，记$h_{\theta j}(x)= \frac{e^{\theta_j^T x}}{ \sum_{l=1}^k e^{ \theta_l^T x } }$  (h一般是hypothesis的缩写)，在一个样本中，对于输入x的分类结果为j的概率为</p>
<script type="math/tex; mode=display">
P(y=j|x; \theta) = h_{\theta j} (x)^{ 1\{ y = j \}  }</script><p>将所有分类的概率综合起来，则有：</p>
<script type="math/tex; mode=display">
P(y|x; \theta) = \prod_{j=1}^k h_{\theta j} (x)^{ 1\{ y = j \}  }</script><p>取似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
L(\theta) &= \prod_{i=1}^m P(y^{(i)} | x^{(i)}; \theta) \\
&= \prod_{i=1}^m \biggl[ \prod_{j=1}^k h_{\theta j} (x^{(i)})^{1\{ y^{(i)} = j \}} \biggl]
\end{align}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">
\begin{align}
l(\theta) &= log\ L(\theta) \\
&= \sum_{i=1}^m \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ h_{\theta j} (x^{(i)})  \biggl]
\end{align}</script><p>最大似然估计就是要求得使$l(\theta)$取最大值时的$\theta$ 。一般将它乘上一个负系数<strong>-1/m</strong>，即：</p>
<script type="math/tex; mode=display">
J(\theta) = - \frac{1}{m} l(\theta)</script><p>则$J(\theta)$取最小值时的$\theta$为要求的最佳参数。这也就是上面的交叉熵代价函数。</p>
<blockquote>
<ol>
<li>log MN = log M + log N</li>
<li>很多文献里对数都没标底数，这说明可以取任意底数，一般取e或2，取不同的底数时，对数值只相差了一个常数系数，对算法不会有影响。</li>
</ol>
</blockquote>
<p>相关的具体分析参考如下链接：</p>
<ul>
<li><a href="http://deeplearning.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="noopener">Softmax回归</a></li>
<li><a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="noopener">Softmax Regression</a></li>
<li><a href="http://www.jianshu.com/p/8eb17fa41164" target="_blank" rel="noopener">Softmax分类函数</a></li>
<li><a href="https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/neural_net_implementation/neural_network_implementation_intermezzo02.ipynb" target="_blank" rel="noopener">neural_network_implementation_intermezzo02</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap3.html" target="_blank" rel="noopener">Improving the way neural networks learn</a></li>
<li><a href="http://blog.csdn.net/u014313009/article/details/51043064" target="_blank" rel="noopener">交叉熵代价函数</a></li>
</ul>
<h3 id="交叉熵代价函数-cross-entropy-loss"><a href="#交叉熵代价函数-cross-entropy-loss" class="headerlink" title="交叉熵代价函数 cross-entropy loss"></a>交叉熵代价函数 <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression" target="_blank" rel="noopener">cross-entropy loss</a></h3><h4 id="二次代价函数的不足"><a href="#二次代价函数的不足" class="headerlink" title="二次代价函数的不足"></a>二次代价函数的不足</h4><p>考察一下二次代价函数</p>
<script type="math/tex; mode=display">
C = \frac {1}{2n} \sum_x \| y(x) - a^L(x) \|^2</script><p>其中，C表示代价，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。为简单起见，以一个样本为例进行说明，此时二次代价函数为：</p>
<script type="math/tex; mode=display">
C = \frac {(y-a)^2}{2}</script><p>在用梯度下降法(gradient descent) 调整权重w和偏置b的过程中，w和b的梯度推导如下：</p>
<script type="math/tex; mode=display">
\begin{align}
&\frac { \partial C }{ \partial w } = (a - y)\sigma^\prime(z) x \\
&\frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z) \\
\end{align}</script><p>可以看出，w和b的梯度跟激活函数的梯度成正比，激活函数的梯度越大，w和b的大小调整得越快，训练收敛得就越快。而神经网络常用的激活函数为sigmoid函数或tanh函数，观察这些激活函数的图像，当初始的代价（误差）越大时，梯度（导数）越小，训练的速度越慢。这与我们的期望不符，即不能错误越大，改正的幅度越大，从而学习得越快。</p>
<h4 id="交叉熵代价函数"><a href="#交叉熵代价函数" class="headerlink" title="交叉熵代价函数"></a>交叉熵代价函数</h4><p>为了克服二次代价函数学习缓慢的缺点，引入了交叉熵代价函数：</p>
<script type="math/tex; mode=display">
C = - \frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)]</script><p>其中，x表示样本，n表示样本的总数。重新计算参数w的梯度：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial C}{\partial w_j} &= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \frac{\partial \sigma(z)}{\partial w_j} \\
&= -\frac{1}{n} \sum_x \Bigl( \frac{y}{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \Bigr) \sigma^\prime(z)x_j \\
&= \frac{1}{n} \sum_x  \frac{ \sigma^\prime(z)x_j }{\sigma(z) (1-\sigma(z))} (\sigma(z)-y) \\
&= \frac{1}{n} \sum_x  x_j (\sigma(z)-y) \\
\end{align}</script><p>其中，w的梯度公式中原来的 $\sigma^\prime(z)$ 被消掉了；另外，该梯度公式中的 $\sigma(z)-y$ 表示输出值与实际值之间的误差。所以，当误差越大，梯度就越大，参数w调整得越快，训练速度也就越快。同理可得，b的梯度为：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y)</script><h4 id="交叉熵代价函数的来源"><a href="#交叉熵代价函数的来源" class="headerlink" title="交叉熵代价函数的来源"></a>交叉熵代价函数的来源</h4><p>用交叉熵代替二次代价函数的想法源自哪里？</p>
<p>以偏置b的梯度计算为例</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial C}{\partial b} &= \frac{\partial C}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial b} \\
&= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \cdot \frac{\partial (wx+b)}{\partial b} \\
&= \frac{\partial C}{\partial a} \cdot \sigma^\prime(z) \\
&= \frac{\partial C}{\partial a} \cdot a(1-a) \\
\end{align}</script><p>而二次代价函数推导出来的b的梯度公式为：</p>
<script type="math/tex; mode=display">
\frac { \partial C }{ \partial b } = (a - y)\sigma^\prime(z)</script><p>为了消掉该公式中的 $\sigma^\prime(z)$ ，需要找到一个代价函数使得：</p>
<script type="math/tex; mode=display">
\frac { \partial C }{ \partial b } = (a - y)</script><p>即</p>
<script type="math/tex; mode=display">
\frac { \partial C }{ \partial a } \cdot a(1-a) = (a - y)</script><p>对方程进行关于a的积分，可得：</p>
<script type="math/tex; mode=display">
C = -[yln\ a + (1-y)ln(1-a)] + constant</script><p>其中constant是积分常量。这是一个单独训练样本X对代价函数的贡献。为了得到整个的代价函数，还需要对所有的训练样本进行平均，可得：</p>
<script type="math/tex; mode=display">
C = -\frac{1}{n} \sum_x [yln\ a + (1-y)ln(1-a)] + constant</script><p>而这就是前面的交叉熵代价函数。</p>
<p>在分类问题中，交叉熵其实就是对数似然函数的最大化。</p>
<p>关于交叉熵的更多内容，参考以下链接：</p>
<ul>
<li><a href="http://blog.csdn.net/rtygbwwwerr/article/details/50778098" target="_blank" rel="noopener">交叉熵（Cross-Entropy）</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Cross entropy</a></li>
<li><a href="http://shuokay.com/2017/06/23/cross-entropy/" target="_blank" rel="noopener">怎样理解 Cross Entropy</a></li>
<li><a href="http://blog.csdn.net/hguisu/article/details/27305435" target="_blank" rel="noopener">信息论的熵</a></li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" rel="noopener">Entropy</a></li>
</ul>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><blockquote>
<p>求导的链式法则 (<a href="https://en.wikipedia.org/wiki/Chain_rule" target="_blank" rel="noopener">Chain rule</a>)：<br>表达式：$(f(g(x)))^\prime = f^\prime (g(x)) g^\prime (x)$<br>其他形式：$\frac {dy}{dx} = \frac {dy}{dz} \cdot \frac {dz}{dx}$</p>
</blockquote>
<p>求$J(W, b)$的最小值可以使用梯度下降法，根据梯度下降法可得$W$和$b$的更新过程：</p>
<script type="math/tex; mode=display">
W^{[l]}_{[i]j} := W^{[l]}_{[i]j} - \alpha \frac{\partial}{\partial \ W^{[l]}_{[i]j}} J(W, b) \\
b^{[l]}_{[i]} := b^{[l]}_{[i]} - \alpha \frac{\partial}{\partial \ b^{[l]}_{[i]}} J(W, b)</script><p>其中，$\alpha$为学习步长，$W^{[l]}_{[i]j}$为第l层的第i个节点的权重第j个分量，$b^{[l]}_{[i]}$为第l层的第i个节点的偏置。</p>
<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>输出层的激活函数采用的是softmax函数。根据前文，输出层的误差采用交叉熵代价函数来衡量，即：</p>
<script type="math/tex; mode=display">
x^{(i)} = a ^{[1](i)}\\
z_{[j]}^{[2](i)} = x^{(i)}W_{[j]}^{[2]} + b_{[j]}^{[2]}  \\
a_{[j]}^{[2](i)} = \frac{e^{z_{[j]}^{[2](i)}}}{ \sum_{k=1}^K e^{ z_{[k]}^{[2](i)} } }   \\
J(W, b) = -\frac{1}{m} \Biggl[ \sum_{i=1}^m  \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log \ a_{[j]}^{[2](i)}  \biggl] \Biggl]</script><p>其中，依照前文约定，下标 [j] 为第$j$个节点的序号，上标 (i) 为样本在样本集中的序号。</p>
<p>输出层的第$l$个节点的权重$W_{[l]}$的第$t$个分量，求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$)</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \ W_{[l]t}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ W_{[l]t}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}  \biggl] \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}  \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ W_{[l]t}} \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}  \biggl] \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]}  \biggl) +  \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) +  \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}} \biggr)  \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) +  \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr)  \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \cdot x^{(i)}_t \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl( 1\{ y^{(i)} = l \} - a_{[l]} \biggl) \cdot x^{(i)}_t \Biggl] \\
\end{align}</script><p>输出层的第$l$个节点的偏置$b_{[l]}$的求导(梯度)为：(为了简化公式，没有加上标$[2]$，a和z没有加上标$(i)$)</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \ b_{[l]}} J(W, b) &= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ b_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}  \biggl] \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl[ \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}  \biggl] \biggl] \cdot \frac{\partial \ z_{[l]}}{\partial \ b_{[l]}} \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ \frac{\partial}{\partial \ z_{[l]}} \biggl( 1\{ y^{(i)} = l \} \ log\ a_{[l]}  \biggl) +  \frac{\partial}{\partial \ z_{[l]}} \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]} \biggr) \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{\partial \ (log\ a_{[l]}) }{\partial \ a_{[l]}} \cdot \frac{\partial \ a_{[l]}}{\partial \ z_{[l]}} \biggl) +  \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{\partial \ (log\ a_{[j]}) }{\partial \ a_{[j]}} \cdot \frac{\partial \ a_{[j]}}{\partial \ z_{[l]}}  \biggr)  \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \biggl( \frac{1}{a_{[l]}} \cdot a_{[l]} ( 1-a_{[l]} ) \biggl) +  \biggl( \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot \frac{1}{a_{[j]}} \cdot (-a_{[j]} a_{[l]}) \biggr)  \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} \cdot ( 1-a_{[l]} ) - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - 1\{ y^{(i)} = l \} \cdot a_{[l]} - \sum_{j=1,j \neq l}^k 1\{ y^{(i)} = j \} \cdot a_{[l]} \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \cdot \sum_{j=1}^k 1\{ y^{(i)} = j \} \biggl] \Biggl] \\
&= -\frac{1}{m} \Biggl[ \sum_{i=1}^m \biggl[ 1\{ y^{(i)} = l \} - a_{[l]} \biggl] \Biggl] \\
\end{align}</script><p>在很多的文献中，会把上式记成如下形式，其中上标(i)表示是第i个样本：</p>
<script type="math/tex; mode=display">
\begin{align}
\delta_{[l]}^{[2](i)} &= \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} = \frac{\partial}{ \partial \ z_{[l]}^{[2](i)} } \biggl[- \sum_{j=1}^k 1\{ y^{(i)} = j \} \ log\ a_{[j]}^{[2](i)}  \biggl]  = a_{[l]}^{[2](i)} - 1\{ y^{(i)} = l \} \\
\frac{\partial}{\partial \ W_{[l]t}^{[2]}} J(W, b) &= \frac{\partial  }{\partial \ W_{[l]t}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)})  
= \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ W_{[l]t}^{[2]}} )       
=  \frac{1}{m} \sum_{i=1}^m ( \delta_{[l]}^{[2](i)} \cdot a_t^{[1](i)} )       \\
\frac{\partial}{\partial \ b_{[l]}^{[2]}} J(W, b) &= \frac{\partial  }{\partial \ b_{[l]}^{[2]} } (\frac{1}{m} \sum_{i=1}^m E^{(i)})  
= \frac{1}{m} \sum_{i=1}^m ( \frac{\partial \ E^{(i)} }{\partial \ z_{[l]}^{[2](i)}} \cdot \frac{\partial \ z_{[l]}^{[2](i)}}{\partial \ b_{[l]}^{[2]}} )       
=  \frac{1}{m} \sum_{i=1}^m  \delta_{[l]}^{[2](i)}        \\
\end{align}</script><blockquote>
<ol>
<li>这里log的底数用的是e</li>
<li>这里的x指的是输出层的上一层的输出</li>
<li>关于$\delta$的原文是：for each node $i$ in layer $l$, we would like to compute an “error term” $\delta^{(l)}$ that measures how much that node was “responsible” for any errors in our output ，链接在<a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/" target="_blank" rel="noopener">这里</a></li>
</ol>
</blockquote>
<h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><p>隐藏层的激活函数采用的是tanh函数。</p>
<p>先考虑只有一个隐藏层的情况，参考下面这张图片，可以发现当前层的每个节点的权重和偏置会影响下一层的各个节点</p>
<p><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/learn-ml-2.png" alt="1"></p>
<script type="math/tex; mode=display">
\begin{align}
& z^{[1]} = xW^{[1]}+ b^{[1]} \\
& a^{[1]} = \sigma (z^{[1]}) = tanh(z^{(i)}) \\
& z^{[2]} = a^{[1]}W^{[2]} + b^{[2]} \\
& a^{[2]} = \hat y = softmax (z^{[2]}) \\
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \ W^{[1]}_{[v]t}} J(W, b) &= \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ W^{[1]}_{[v]t} } E^{(i)} \biggr] \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot  \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggr] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ W^{[1]}_{[v]t}} \biggr] \quad \text{(multivariate chain rule)}\\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggr] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \cdot x_t^{(i)} \biggr] \\
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial \ b^{[1]}_{[v]}} J(W, b) &= \frac{\partial}{ \partial \ b^{[1]}_{[v]} } \bigl( \frac{1}{m} \sum_{i=1}^m E^{(i)} \bigr) \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial}{ \partial \ b^{[1]}_{[v]} } E^{(i)} \biggr] \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ \frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} } \cdot  \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \frac{\partial \ E^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \biggl] ) \cdot \frac{\partial \ a^{[1](i)}_{[v]}}{\partial \ z^{[1](i)}_{[v]}} \cdot \frac{\partial \ z^{[1](i)}_{[v]}}{\partial \ b^{[1]}_{[v]}} \biggr] \quad \text{(multivariate chain rule)}\\
&= \frac{1}{m} \sum_{i=1}^m \biggl[ ( \sum_o \biggl[ \delta_{[o]}^{[2](i)} \cdot W_{[o]v}^{[2]} \biggl] ) \cdot (1 - (z_{[v]}^{[1](i)})^2 ) \biggr] \\
\end{align}</script><p>按照输出层的$\delta$的定义，可以定义任意一层的$\delta$，并在计算的时候将本层的$\delta$传递给下一层，从而计算各层的权重和偏置的导数。</p>
<blockquote>
<p>几个变量相互之间有依赖关系，这时某个变量的偏导数不能反映变化率，要表示在该变量上的变化率，应该使用该变量的全导数。变量相互独立时，偏导数可以表示变化率。</p>
</blockquote>
<p>参考链接：</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Total_derivative#Differentiation_with_indirect_dependencies" target="_blank" rel="noopener">Total derivative</a></li>
<li><a href="https://www.zhihu.com/question/24827633/answer/91489990" target="_blank" rel="noopener">如何理解神经网络里面的反向传播算法</a></li>
<li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" target="_blank" rel="noopener">A Step by Step Backpropagation Example</a></li>
</ul>
<h4 id="终止条件"><a href="#终止条件" class="headerlink" title="终止条件"></a>终止条件</h4><ul>
<li>权重的更新低于某个阈值；</li>
<li>预测的错误率低于某个阈值；</li>
<li>达到预设一定的循环次数；</li>
</ul>
<h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>下面这张图描述了正向传播时，各变量的维度</p>
<p><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/learn-ml-3.png" alt="1"></p>
<p>其中$x$的横线表示$x$可以看做是由一组横向量组成的，$W$的竖线表示$W$可以看做是由一组竖向量组成的。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="反向传播的一种错误求导"><a href="#反向传播的一种错误求导" class="headerlink" title="反向传播的一种错误求导"></a>反向传播的一种错误求导</h3><p>在隐藏层的反向传播求导时，下面的求导方式是错误的：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial \ E^{(i)}}{ \partial \ a^{[1](i)}_{[v]} }
&=  \sum_o \bigl[ \frac{\partial \ E_{[o]}^{(i)} }{ \partial \ z^{[2](i)}_{[o]} } \cdot \frac{\partial \ z_{[o]}^{[2](i)} }{ \partial \ a^{[1](i)}_{[v]} } \bigr]  \\
\end{align}</script><p>原因是$z_{[o]}^{(i)[2]}$对所有的$E_{[j]}^{(i)}$都有影响，而这里只考虑了与$z_{[o]}^{(i)[2]}$相对应的$E_{[o]}^{(i)}$。</p>
<p>输出层的求导是没问题的，因为考虑了所有的分量。</p>
<h3 id="数学复习"><a href="#数学复习" class="headerlink" title="数学复习"></a>数学复习</h3><ol>
<li>矩阵A(m, n)，m指行数，n指列数</li>
<li>sigmoid函数求导</li>
</ol>
<script type="math/tex; mode=display">
\begin{align}
\sigma^\prime(z) &= \bigl( \frac{1}{1 + e^{-z}} \bigr)^\prime = (-1)(1+e^{-z})^{(-1)-1} \cdot (e^{-z})^\prime = \frac{1}{(1+e^{-z})^2} \cdot  (e^{-z}) \\
&= \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} = \frac{1}{1+e^{-z}} \cdot (1-\frac{1}{1+e^{-z}}) \\
&= \sigma(z)(1-\sigma(z))
\end{align}</script><h3 id="使用数学软件"><a href="#使用数学软件" class="headerlink" title="使用数学软件"></a>使用数学软件</h3><p>在一些推导中，特别是矩阵的计算，想直观的看一下展开后的结果，如果完全手算，会比较费时，可以使用mathematica进行符号计算。例如，计算矩阵的相乘，在mathematica中输入以下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;u, n, m&#125; = &#123;3, 2, 5&#125;</span><br><span class="line">Array[Subscript[w, ##] &amp;, &#123;u, n&#125;] . Array[Subscript[x, ##] &amp;, &#123;n, m&#125;]</span><br></pre></td></tr></table></figure>
<p>得到结果</p>
<script type="math/tex; mode=display">
\left(
\begin{array}{ccccc}
 w_{1,1} x_{1,1}+w_{1,2} x_{2,1} & w_{1,1} x_{1,2}+w_{1,2} x_{2,2} & w_{1,1} x_{1,3}+w_{1,2} x_{2,3} & w_{1,1} x_{1,4}+w_{1,2} x_{2,4} & w_{1,1} x_{1,5}+w_{1,2} x_{2,5} \\
 w_{2,1} x_{1,1}+w_{2,2} x_{2,1} & w_{2,1} x_{1,2}+w_{2,2} x_{2,2} & w_{2,1} x_{1,3}+w_{2,2} x_{2,3} & w_{2,1} x_{1,4}+w_{2,2} x_{2,4} & w_{2,1} x_{1,5}+w_{2,2} x_{2,5} \\
 w_{3,1} x_{1,1}+w_{3,2} x_{2,1} & w_{3,1} x_{1,2}+w_{3,2} x_{2,2} & w_{3,1} x_{1,3}+w_{3,2} x_{2,3} & w_{3,1} x_{1,4}+w_{3,2} x_{2,4} & w_{3,1} x_{1,5}+w_{3,2} x_{2,5} \\
\end{array}
\right)</script><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>下面是3层bp神经网络的python实现，取自<a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">这里</a>，我做了一些修改</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    X, y = datasets.make_moons(<span class="number">200</span>, noise=<span class="number">0.20</span>)</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(X, y, model)</span>:</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = predict(model, np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.title(<span class="string">"bp nn"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, x)</span>:</span></span><br><span class="line">    layer1, layer2 = model</span><br><span class="line">    feedforward(x, layer1)</span><br><span class="line">    feedforward(layer1.a, layer2, <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(layer2.a, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, last_layer_dim: int, dim: int)</span>:</span></span><br><span class="line">        self.W = np.random.randn(last_layer_dim, dim) / np.sqrt(last_layer_dim)</span><br><span class="line">        self.b = np.zeros((<span class="number">1</span>, dim))</span><br><span class="line">        self.z = <span class="keyword">None</span></span><br><span class="line">        self.a = <span class="keyword">None</span></span><br><span class="line">        self.delta = <span class="keyword">None</span></span><br><span class="line">        self.dW = <span class="keyword">None</span></span><br><span class="line">        self.db = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(X)</span>:</span>  <span class="comment"># m行dim列</span></span><br><span class="line">    exp_scores = np.exp(X)</span><br><span class="line">    probs = exp_scores / np.sum(exp_scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> probs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(x: np.ndarray, layer: Layer, is_final: bool=False)</span>:</span></span><br><span class="line">    layer.z = x.dot(layer.W) + layer.b  <span class="comment"># m行dim列</span></span><br><span class="line">    <span class="keyword">if</span> is_final:</span><br><span class="line">        layer.a = softmax(layer.z)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer.a = np.tanh(layer.z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(x: np.ndarray, y: np.ndarray, layer: Layer, nextlayer: Layer, </span></span></span><br><span class="line"><span class="function"><span class="params">             num_examples: int, is_final: bool=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_final:</span><br><span class="line">        delta = layer.a</span><br><span class="line">        delta[range(num_examples), y] -= <span class="number">1</span> <span class="comment"># m行dim列, a_&#123;[l]&#125;^&#123;(i)&#125; - 1&#123;y_&#123;(i)&#125;=l&#125;</span></span><br><span class="line">        <span class="comment"># layer.dW = (x.T).dot(delta) / num_examples</span></span><br><span class="line">        <span class="comment"># layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples</span></span><br><span class="line">        layer.dW = (x.T).dot(delta)</span><br><span class="line">        layer.db = np.sum(delta, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        layer.delta = delta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        delta = nextlayer.delta.dot(nextlayer.W.T) * (<span class="number">1</span> - np.power(layer.a, <span class="number">2</span>))  <span class="comment"># * 对应元素相乘</span></span><br><span class="line">        <span class="comment"># layer.dW = np.dot(x.T, delta) / num_examples</span></span><br><span class="line">        <span class="comment"># layer.db = np.sum(delta, axis=0, keepdims=True) / num_examples</span></span><br><span class="line">        layer.dW = np.dot(x.T, delta)</span><br><span class="line">        layer.db = np.sum(delta, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">        layer.delta = delta</span><br><span class="line">    learn_rate = <span class="number">0.01</span></span><br><span class="line">    layer.W += -learn_rate * layer.dW</span><br><span class="line">    layer.b += -learn_rate * layer.db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(X, y, nn_hdim, num_passes=<span class="number">20000</span>, print_loss=False)</span>:</span></span><br><span class="line">    num_examples = len(X)</span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    input_dim = X.shape[<span class="number">1</span>]</span><br><span class="line">    nn_output_dim = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    layer1 = Layer(input_dim, nn_hdim)</span><br><span class="line">    layer2 = Layer(nn_hdim, nn_output_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_passes):</span><br><span class="line">        feedforward(X, layer1)</span><br><span class="line">        feedforward(layer1.a, layer2, <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        backprop(layer1.a, y, layer2, <span class="keyword">None</span>, num_examples, <span class="keyword">True</span>)</span><br><span class="line">        backprop(X, y, layer1, layer2, num_examples)</span><br><span class="line"></span><br><span class="line">    model = [layer1, layer2]</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    X, y = generate_data()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    model = build_model(X, y, <span class="number">3</span>)</span><br><span class="line">    time_cost = time.time() - start_time</span><br><span class="line">    summary = <span class="string">f'cost: <span class="subst">&#123;time_cost&#125;</span>'</span></span><br><span class="line">    print(summary)</span><br><span class="line">    visualize(X, y, model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
]]></content>
      
        <categories>
            
            <category> ml </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ml </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[android开发中的一些经验]]></title>
      <url>/2017/12/01/android-issues-md/</url>
      <content type="html"><![CDATA[<p>记录一下android开发中的一些经验，本文将持续更新。</p>
<a id="more"></a>
<h2 id="jar命令的使用"><a href="#jar命令的使用" class="headerlink" title="jar命令的使用"></a>jar命令的使用</h2><p><a href="https://zh.wikipedia.org/wiki/JAR_(文件格式)" target="_blank" rel="noopener">JAR</a>文件即 Java Archive File，是 Java 的一种文档格式。JAR 文件实际上就是 ZIP 文件，使用<code>unzip xxx.jar -d dest/</code>命令即可解压。</p>
<p><code>jar</code>命令的说明如下：<br><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/android-issues-1.png" alt="1"></p>
<p>下面是jar命令的一些常用用法：</p>
<ol>
<li><p>显示jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jar tvf hello.jar    # 查看hello.jar包的内容</span><br></pre></td></tr></table></figure>
</li>
<li><p>解压jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jar xvf hello.jar   # 解压hello.jar至当前目录</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>更多用法参考 <a href="http://blog.chinaunix.net/uid-692788-id-2681136.html" target="_blank" rel="noopener">JAR命令&amp;JAR包详解</a></p>
<h2 id="java的反编译"><a href="#java的反编译" class="headerlink" title="java的反编译"></a>java的反编译</h2><p>很多情况下，当我们使用别人的jar包时，很有可能会遇到一些问题，这时需要了解一下jar包里到底有些什么，方便定位问题所在。jar命令只是对jar包进行了一下解包，想查看其中的代码细节，还需要进行反编译。<a href="http://jd.benow.ca" target="_blank" rel="noopener">Java Decompiler</a>是一款比较好用的反编译软件，在实际使用中，能够反编译出大部分的java代码。eclipse、android studio安装Java Decompiler插件后，在使用第三方jar库时，会减少很多烦恼。</p>
<h2 id="获取apk的签名和MD5指纹"><a href="#获取apk的签名和MD5指纹" class="headerlink" title="获取apk的签名和MD5指纹"></a>获取apk的签名和MD5指纹</h2><p>在很多情况下，需要确认apk的签名。例如，接入渠道sdk后，sdk的初始化或者登陆、支付等功能出现了问题，就有可能是apk的SHA1签名与在渠道后台配置的SHA1签名不一致导致的。</p>
<p>获取apk的SHA1签名的一般步骤是:</p>
<ol>
<li>解压apk：<code>unzip testapp.apk</code>。</li>
<li>找到解压出来的RSA文件。</li>
<li>将终端切到RSA文件所在的目录, 在命令行输入 <code>keytool -printcert -file ./***.RSA</code> ，即可获取sha1签名和md5指纹。<br>具体操作如图<br><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/android-issues-2.png" alt="2"></li>
</ol>
<h2 id="apktool的使用"><a href="#apktool的使用" class="headerlink" title="apktool的使用"></a>apktool的使用</h2><p>解包和打包<br><a href="https://ibotpeaches.github.io/Apktool/" target="_blank" rel="noopener">apktool</a> 是一款逆向工程工具。在<a href="https://ibotpeaches.github.io/Apktool/documentation/" target="_blank" rel="noopener">这里</a>有相关的文档。</p>
<h3 id="基础功能"><a href="#基础功能" class="headerlink" title="基础功能"></a>基础功能</h3><ol>
<li><p>解压apk，生成可读的AndroidManifest.xml，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apktool d testapp.apk</span><br></pre></td></tr></table></figure>
</li>
<li><p>解压jar</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apktool d foo.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>smali调试，参考<a href="https://github.com/JesusFreke/smali/wiki/smalidea" target="_blank" rel="noopener">wiki</a></p>
</li>
</ol>
<h3 id="查看apk里面的java源代码"><a href="#查看apk里面的java源代码" class="headerlink" title="查看apk里面的java源代码"></a>查看apk里面的java源代码</h3><ol>
<li>下载<a href="https://github.com/pxb1988/dex2jar" target="_blank" rel="noopener">dex2jar</a></li>
<li>使用unzip解压apk：<code>unzip testapp.apk</code></li>
<li>使用dex2jar将dex文件转换成jar文件</li>
<li>使用jd-gui打开生成的classes-dex2jar.jar，就可以查看java源代码了</li>
</ol>
<p>具体操作如图</p>
<p><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/android-issues-3.png" alt="3"><br><img src="http://ozy76jm8o.bkt.clouddn.com/blog/images/android-issues-4.png" alt="4"></p>
<h3 id="修改apk里面的内容"><a href="#修改apk里面的内容" class="headerlink" title="修改apk里面的内容"></a>修改apk里面的内容</h3><ol>
<li>使用apktool解压apk，<code>apktool d testapp.apk</code></li>
<li>更新so包，比如可以将release版的so替换成debug版的so</li>
<li>更新资源文件</li>
<li>更改smali，smali的语法可以参考<a href="http://blog.csdn.net/wdaming1986/article/details/8299996" target="_blank" rel="noopener">这篇文章</a></li>
<li>打包回apk，打包出来的文件在apk文件夹中的dist目录下</li>
<li>对重新打包后的apk文件进行签名<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jarsigner -verbose -keystore yourKey.keystore -storepass yourPassword path/to/apk /path/to/keystore/file</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="查看apk的一些信息"><a href="#查看apk的一些信息" class="headerlink" title="查看apk的一些信息"></a>查看apk的一些信息</h2><p>使用android studio直接打开apk包，可以快速的浏览apk中的一些信息，但是会在用户文件夹生成一个临时工程，占用c盘资源。</p>
<h2 id="log查看"><a href="#log查看" class="headerlink" title="log查看"></a>log查看</h2><p>目前常用的获取android上面的log的方法有这些：</p>
<ol>
<li>直接使用logcat命令，一般是将logcat的输出重定向到文件，然后再在文件里查找需要的信息</li>
<li>使用eclipse或者android studio的logcat窗口查看log。eclipse在非调试模式下，logcat窗口并不是很好用；android studio有一定几率连不上设备，这时很会恼火。</li>
<li>app收集log并显示在屏幕上，由于log有可能会刷新得很快，在设备的屏幕上，想定位到具体信息并不是很方便，并且这种方式对app的性能有一定的影响。</li>
</ol>
<p>个人感觉比较好的方式是参考一下AirDroid，开发一个独立的app用于收集android上的log，这个app开启一个web服务，并将收集到的log输出到这个web上，在电脑上的浏览器中访问这个web，可以获取到设备上的实时log。</p>
<h2 id="一些c-c-库在android上的编译"><a href="#一些c-c-库在android上的编译" class="headerlink" title="一些c/c++库在android上的编译"></a>一些c/c++库在android上的编译</h2><ul>
<li>编译Boost，参考<a href="https://github.com/dec1/Boost-for-Android" target="_blank" rel="noopener">Boost-for-Android</a></li>
<li>编译Python，参考python的<a href="https://bugs.python.org/issue30386" target="_blank" rel="noopener">issue30386</a></li>
</ul>
<h2 id="android-studio引用外部工程"><a href="#android-studio引用外部工程" class="headerlink" title="android studio引用外部工程"></a>android studio引用外部工程</h2><p>很多时候，库工程并不是放在项目文件夹下面，而是放在其他位置，常见的原因是想将这个库工程作为一个公共的库，在几个项目之间使用。</p>
<p>android studio中引用外部库的方法是这样的，在项目的settings.gradle文件中添加如下语句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">include &apos;:BaiduLBS&apos;</span><br><span class="line">project(&apos;:BaiduLBS&apos;).projectDir = new File(settingsDir, &apos;../platform/android/sdk/BaiduLBS&apos;)</span><br></pre></td></tr></table></figure></p>
<p>即可添加相对路径在<code>../platform/android/sdk/BaiduLBS</code>处的外部库<code>BaiduLBS</code>。</p>
<h2 id="横竖屏的问题"><a href="#横竖屏的问题" class="headerlink" title="横竖屏的问题"></a>横竖屏的问题</h2><p>当手机进行横竖屏切换，弹出键盘，窗口大小发生变化等情况发生时，activity会重新走一遍OnCreate等生命周期方法。要避免这种行为，需要在AndroidManifest.xml中，为activity添加<code>android:configChanges</code>属性，例如<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">activity</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:name</span>=<span class="string">"com.xxx.MainActivity"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:configChanges</span>=<span class="string">"orientation|keyboardHidden|screenSize"</span> &gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">activity</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>这时，当有orientation、keyboardHidden、screenSize情况发生时，就不会重建activity并调用OnCreate等方法，而是调用原实例的<code>onConfigurationChanged</code>方法。</p>
<h2 id="游戏画面只有屏幕一半"><a href="#游戏画面只有屏幕一半" class="headerlink" title="游戏画面只有屏幕一半"></a>游戏画面只有屏幕一半</h2><p>在android手机上玩游戏时，经常会遇到这样一种情况，就是不知道做了一些什么操作，屏幕上只有部分(不一定是1/2)区域有游戏画面，其余区域是黑色的。</p>
<p>个人感觉是经过了某些操作，导致GLSurfaceView的Layout有问题。我使用下面的方法试了一下，能够很大程度的降低这种情况的发生<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 固定为横屏</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">resetGLSurfaceViewSize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    DisplayMetrics dm = <span class="keyword">new</span> DisplayMetrics();</span><br><span class="line">    getWindowManager().getDefaultDisplay().getMetrics(dm);</span><br><span class="line">    <span class="keyword">int</span> w = dm.widthPixels;</span><br><span class="line">    <span class="keyword">int</span> h = dm.heightPixels;</span><br><span class="line">    Log.d(TAG, <span class="string">"屏幕的分辨率为："</span> + w + <span class="string">"*"</span> + h);</span><br><span class="line">    <span class="keyword">if</span> (w &lt;= <span class="number">0</span> || h &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    w = w &gt; h ? w : h;</span><br><span class="line">    h = w &gt; h ? h : w;</span><br><span class="line">    mGLSurfaceView.getLayoutParams().width = w;</span><br><span class="line">    mGLSurfaceView.getLayoutParams().height = h;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onConfigurationChanged</span><span class="params">(Configuration newConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.onConfigurationChanged(newConfig);</span><br><span class="line">    resetGLSurfaceViewSize();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="多次点击app-icon的问题"><a href="#多次点击app-icon的问题" class="headerlink" title="多次点击app icon的问题"></a>多次点击app icon的问题</h2><p>如果一个游戏接入了第3方的登录模块，当打开游戏弹出第3方的登录框时，按下home键回到桌面，再次点击这个游戏的icon，期望能返回游戏，并且登录框不被清除，这时需要将游戏activity的launchMode设置为singleTop，如下<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">activity</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:name</span>=<span class="string">"com.xxx.MainActivity"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">android:launchMode</span>=<span class="string">"singleTop"</span> &gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">activity</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> android </category>
            
        </categories>
        
        
        <tags>
            
            <tag> android </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[HLSL 常用函数]]></title>
      <url>/2017/12/01/hlsl-library/</url>
      <content type="html"><![CDATA[<p>HLSL是<a href="https://docs.unity3d.com/Manual/SL-ShadingLanguage.html" target="_blank" rel="noopener">unity推荐</a>的shader语言，HLSL和Cg很相似。这里整理了一下网络上收集到的相关资料，方便自己学习和查询。</p>
<h2 id="HLSL固有函数"><a href="#HLSL固有函数" class="headerlink" title="HLSL固有函数"></a>HLSL固有函数</h2><p>Intrinsic Functions (DirectX HLSL)</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th>函数名</th>
<th>用法</th>
<th>描述</th>
<th>Description</th>
<th>Minimum shader model</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509562" target="_blank" rel="noopener">abs</a></td>
<td>abs(x)</td>
<td>计算输入值的绝对值。</td>
<td>Absolute value (per component).</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509563" target="_blank" rel="noopener">acos</a></td>
<td>acos(x)</td>
<td>返回输入值反余弦值。</td>
<td>Returns the arccosine of each component of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509564" target="_blank" rel="noopener">all</a></td>
<td>all(x)</td>
<td>测试非0值。</td>
<td>Test if all components of x are nonzero.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471350" target="_blank" rel="noopener">AllMemoryBarrier</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all memory accesses have been completed.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471351" target="_blank" rel="noopener">AllMemoryBarrierWithGroupSync</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all memory accesses have been completed and all threads in the group have reached this call.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509565" target="_blank" rel="noopener">any</a></td>
<td>any(x)</td>
<td>测试输入值中的任何非零值。</td>
<td>Test if any component of x is nonzero.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/dd607357" target="_blank" rel="noopener">asdouble</a></td>
<td></td>
<td></td>
<td>Reinterprets a cast value into a double.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509570" target="_blank" rel="noopener">asfloat</a></td>
<td>asfloat(x)</td>
<td></td>
<td>Convert the input type to a float.</td>
<td>4</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509571" target="_blank" rel="noopener">asin</a></td>
<td>asin(x)</td>
<td>返回输入值的反正弦值。</td>
<td>Returns the arcsine of each component of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509572" target="_blank" rel="noopener">asint</a></td>
<td>asint(x)</td>
<td></td>
<td>Convert the input type to an integer.</td>
<td>4</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471354" target="_blank" rel="noopener">asuint</a></td>
<td></td>
<td></td>
<td>Reinterprets the bit pattern of a 64-bit type to a uint.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509573" target="_blank" rel="noopener">asuint</a></td>
<td>asuint(x)</td>
<td></td>
<td>Convert the input type to an unsigned integer.</td>
<td>4</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509574" target="_blank" rel="noopener">atan</a></td>
<td>atan(x)</td>
<td>返回输入值的反正切值。</td>
<td>Returns the arctangent of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509575" target="_blank" rel="noopener">atan2</a></td>
<td>atan2(y, x)</td>
<td>返回y/x的反正切值。</td>
<td>Returns the arctangent of of two values (x,y).</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509577" target="_blank" rel="noopener">ceil</a></td>
<td>ceil(x)</td>
<td>返回大于或等于输入值的最小整数。</td>
<td>Returns the smallest integer which is greater than or equal to x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb204824" target="_blank" rel="noopener">clamp</a></td>
<td>clamp(x, min, max)</td>
<td>把输入值限制在[min, max]范围内。</td>
<td>Clamps x to the range [min, max].</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb204826" target="_blank" rel="noopener">clip</a></td>
<td>clip(x)</td>
<td>如果输入向量中的任何元素小于0，则丢弃当前像素。</td>
<td>Discards the current pixel, if any component of x is less than zero.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509583" target="_blank" rel="noopener">cos</a></td>
<td>cos(x)</td>
<td>返回输入值的余弦。</td>
<td>Returns the cosine of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509584" target="_blank" rel="noopener">cosh</a></td>
<td>cosh(x)</td>
<td>返回输入值的双曲余弦。</td>
<td>Returns the hyperbolic cosine of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471355" target="_blank" rel="noopener">countbits</a></td>
<td></td>
<td></td>
<td>Counts the number of bits (per component) in the input integer.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509585" target="_blank" rel="noopener">cross</a></td>
<td>cross(x, y)</td>
<td>返回两个3D向量的叉积。</td>
<td>Returns the cross product of two 3D vectors.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509586" target="_blank" rel="noopener">D3DCOLORtoUBYTE4</a></td>
<td>D3DCOLORtoUBYTE4(x)</td>
<td></td>
<td>Swizzles and scales components of the 4D vector xto compensate for the lack of UBYTE4 support in some hardware.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509588" target="_blank" rel="noopener">ddx</a></td>
<td>ddx(x)</td>
<td>返回关于屏幕坐标x轴的偏导数。</td>
<td>Returns the partial derivative of x with respect to the screen-space x-coordinate.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471361" target="_blank" rel="noopener">ddx_coarse</a></td>
<td></td>
<td></td>
<td>Computes a low precision partial derivative with respect to the screen-space x-coordinate.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471362" target="_blank" rel="noopener">ddx_fine</a></td>
<td></td>
<td></td>
<td>Computes a high precision partial derivative with respect to the screen-space x-coordinate.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509589" target="_blank" rel="noopener">ddy</a></td>
<td>ddy(x)</td>
<td>返回关于屏幕坐标y轴的偏导数。</td>
<td>Returns the partial derivative of x with respect to the screen-space y-coordinate.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471364" target="_blank" rel="noopener">ddy_coarse</a></td>
<td></td>
<td></td>
<td>Computes a low precision partial derivative with respect to the screen-space y-coordinate.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471365" target="_blank" rel="noopener">ddy_fine</a></td>
<td></td>
<td></td>
<td>Computes a high precision partial derivative with respect to the screen-space y-coordinate.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509590" target="_blank" rel="noopener">degrees</a></td>
<td>degrees(x)</td>
<td>弧度到角度的转换</td>
<td>Converts x from radians to degrees.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509591" target="_blank" rel="noopener">determinant</a></td>
<td>determinant(m)</td>
<td>返回输入矩阵的值。</td>
<td>Returns the determinant of the square matrix m.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471366" target="_blank" rel="noopener">DeviceMemoryBarrier</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all device memory accesses have been completed.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471367" target="_blank" rel="noopener">DeviceMemoryBarrierWithGroupSync</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all device memory accesses have been completed and all threads in the group have reached this call.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509592" target="_blank" rel="noopener">distance</a></td>
<td>distance(x, y)</td>
<td>返回两个输入点间的距离。</td>
<td>Returns the distance between two points.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509594" target="_blank" rel="noopener">dot</a></td>
<td>dot(x, y)</td>
<td>返回两个向量的点积。</td>
<td>Returns the dot product of two vectors.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471368" target="_blank" rel="noopener">dst</a></td>
<td></td>
<td></td>
<td>Calculates a distance vector.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471394" target="_blank" rel="noopener">EvaluateAttributeAtCentroid</a></td>
<td></td>
<td></td>
<td>Evaluates at the pixel centroid.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471395" target="_blank" rel="noopener">EvaluateAttributeAtSample</a></td>
<td></td>
<td></td>
<td>Evaluates at the indexed sample location.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471396" target="_blank" rel="noopener">EvaluateAttributeSnapped</a></td>
<td></td>
<td></td>
<td>Evaluates at the pixel centroid with an offset.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509595" target="_blank" rel="noopener">exp</a></td>
<td>exp(x)</td>
<td>返回以e为底数，输入值为指数的指数函数值。</td>
<td>Returns the base-e exponent.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509596" target="_blank" rel="noopener">exp2</a></td>
<td>exp2(x)</td>
<td>返回以2为底数，输入值为指数的指数函数值。</td>
<td>Base 2 exponent (per component).</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471397" target="_blank" rel="noopener">f16tof32</a></td>
<td></td>
<td></td>
<td>Converts the float16 stored in the low-half of the uint to a float.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471399" target="_blank" rel="noopener">f32tof16</a></td>
<td></td>
<td></td>
<td>Converts an input into a float16 type.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509598" target="_blank" rel="noopener">faceforward</a></td>
<td>faceforward(n, i, ng)</td>
<td>检测多边形是否位于正面。</td>
<td>Returns -n * sign(dot(i, ng)).</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471400" target="_blank" rel="noopener">firstbithigh</a></td>
<td></td>
<td></td>
<td>Gets the location of the first set bit starting from the highest order bit and working downward, per component.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471401" target="_blank" rel="noopener">firstbitlow</a></td>
<td></td>
<td></td>
<td>Returns the location of the first set bit starting from the lowest order bit and working upward, per component.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509599" target="_blank" rel="noopener">floor</a></td>
<td>floor(x)</td>
<td>返回小于等于x的最大整数。</td>
<td>Returns the greatest integer which is less than or equal to x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509601" target="_blank" rel="noopener">fmod</a></td>
<td>fmod(x, y)</td>
<td>返回a / b的浮点余数。</td>
<td>Returns the floating point remainder of x/y.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509603" target="_blank" rel="noopener">frac</a></td>
<td>frac(x)</td>
<td>返回输入值的小数部分。</td>
<td>Returns the fractional part of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509604" target="_blank" rel="noopener">frexp</a></td>
<td>frexp(x, exp)</td>
<td>返回输入值的尾数和指数</td>
<td>Returns the mantissa and exponent of x.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509608" target="_blank" rel="noopener">fwidth</a></td>
<td>fwidth(x)</td>
<td>返回 abs(ddx(x)) + abs(ddy(x)) 。</td>
<td>Returns abs(ddx(x)) + abs(ddy(x))</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb943996" target="_blank" rel="noopener">GetRenderTargetSampleCount</a></td>
<td>GetRenderTargetSampleCount()</td>
<td></td>
<td>Returns the number of render-target samples.</td>
<td>4</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb943997" target="_blank" rel="noopener">GetRenderTargetSamplePosition</a></td>
<td>GetRenderTargetSamplePosition(x)</td>
<td></td>
<td>Returns a sample position (x,y) for a given sample index.</td>
<td>4</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471403" target="_blank" rel="noopener">GroupMemoryBarrier</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all group shared accesses have been completed.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471404" target="_blank" rel="noopener">GroupMemoryBarrierWithGroupSync</a></td>
<td></td>
<td></td>
<td>Blocks execution of all threads in a group until all group shared accesses have been completed and all threads in the group have reached this call.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471406" target="_blank" rel="noopener">InterlockedAdd</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic add of value to the dest resource variable.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471407" target="_blank" rel="noopener">InterlockedAnd</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic and.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471409" target="_blank" rel="noopener">InterlockedCompareExchange</a></td>
<td></td>
<td></td>
<td>Atomically compares the input to the comparison value and exchanges the result.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471410" target="_blank" rel="noopener">InterlockedCompareStore</a></td>
<td></td>
<td></td>
<td>Atomically compares the input to the comparison value.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471411" target="_blank" rel="noopener">InterlockedExchange</a></td>
<td></td>
<td></td>
<td>Assigns value to dest and returns the original value.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471412" target="_blank" rel="noopener">InterlockedMax</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic max.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471413" target="_blank" rel="noopener">InterlockedMin</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic min.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471414" target="_blank" rel="noopener">InterlockedOr</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic or.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471415" target="_blank" rel="noopener">InterlockedXor</a></td>
<td></td>
<td></td>
<td>Performs a guaranteed atomic xor.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509612" target="_blank" rel="noopener">isfinite</a></td>
<td>isfinite(x)</td>
<td>如果输入值为有限值则返回true，否则返回false。</td>
<td>Returns true if x is finite, false otherwise.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509613" target="_blank" rel="noopener">isinf</a></td>
<td>isinf(x)</td>
<td>如何输入值为无限的则返回true。</td>
<td>Returns true if x is +INF or -INF, false otherwise.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509614" target="_blank" rel="noopener">isnan</a></td>
<td>isnan(x)</td>
<td>如果输入值为NAN或QNAN则返回true。</td>
<td>Returns true if x is NAN or QNAN, false otherwise.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509616" target="_blank" rel="noopener">ldexp</a></td>
<td>ldexp(x, exp)</td>
<td>frexp的逆运算，返回 x * 2 ^ exp。</td>
<td>Returns x * 2exp</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509617" target="_blank" rel="noopener">length</a></td>
<td>length(v)</td>
<td></td>
<td>Returns the length of the vector v.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509618" target="_blank" rel="noopener">lerp</a></td>
<td>lerp(x, y, s)</td>
<td>对输入值进行插值计算。</td>
<td>Returns x + s(y - x).</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509619" target="_blank" rel="noopener">lit</a></td>
<td>lit(n • l, n • h, m)</td>
<td>返回光照向量（环境光，漫反射光，镜面高光，1）。</td>
<td>Returns a lighting vector (ambient, diffuse, specular, 1)</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509620" target="_blank" rel="noopener">log</a></td>
<td>log(x)</td>
<td>返回以e为底的对数。</td>
<td>Returns the base-e logarithm of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509621" target="_blank" rel="noopener">log10</a></td>
<td>log10(x)</td>
<td>返回以10为底的对数。</td>
<td>Returns the base-10 logarithm of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509622" target="_blank" rel="noopener">log2</a></td>
<td>log2(x)</td>
<td>返回以2为底的对数。</td>
<td>Returns the base-2 logarithm of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471418" target="_blank" rel="noopener">mad</a></td>
<td></td>
<td></td>
<td>Performs an arithmetic multiply/add operation on three values.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509624" target="_blank" rel="noopener">max</a></td>
<td>max(x, y)</td>
<td>返回两个输入值中较大的一个。</td>
<td>Selects the greater of x and y.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509625" target="_blank" rel="noopener">min</a></td>
<td>min(x, y)</td>
<td>返回两个输入值中较小的一个。</td>
<td>Selects the lesser of x and y.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509627" target="_blank" rel="noopener">modf</a></td>
<td>modf(x, out ip)</td>
<td>把输入值分解为整数和小数部分。</td>
<td>Splits the value x into fractional and integer parts.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509628" target="_blank" rel="noopener">mul</a></td>
<td>mul(x, y)</td>
<td>返回输入矩阵相乘的积。</td>
<td>Performs matrix multiplication using x and y.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509629" target="_blank" rel="noopener">noise</a></td>
<td>noise(x)</td>
<td></td>
<td>Generates a random value using the Perlin-noise algorithm.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509630" target="_blank" rel="noopener">normalize</a></td>
<td>normalize(x)</td>
<td>返回规范化的向量，定义为 x / length(x)。</td>
<td>Returns a normalized vector.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509636" target="_blank" rel="noopener">pow</a></td>
<td>pow(x, y)</td>
<td>返回输入值的指定次幂。</td>
<td>Returns xy.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471426" target="_blank" rel="noopener">Process2DQuadTessFactorsAvg</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471427" target="_blank" rel="noopener">Process2DQuadTessFactorsMax</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471428" target="_blank" rel="noopener">Process2DQuadTessFactorsMin</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471429" target="_blank" rel="noopener">ProcessIsolineTessFactors</a></td>
<td></td>
<td></td>
<td>Generates the rounded tessellation factors for an isoline.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471430" target="_blank" rel="noopener">ProcessQuadTessFactorsAvg</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471431" target="_blank" rel="noopener">ProcessQuadTessFactorsMax</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471432" target="_blank" rel="noopener">ProcessQuadTessFactorsMin</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a quad patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471433" target="_blank" rel="noopener">ProcessTriTessFactorsAvg</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a tri patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471434" target="_blank" rel="noopener">ProcessTriTessFactorsMax</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a tri patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471435" target="_blank" rel="noopener">ProcessTriTessFactorsMin</a></td>
<td></td>
<td></td>
<td>Generates the corrected tessellation factors for a tri patch.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509637" target="_blank" rel="noopener">radians</a></td>
<td>radians(x)</td>
<td>角度到弧度的转换。</td>
<td>Converts x from degrees to radians.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471436" target="_blank" rel="noopener">rcp</a></td>
<td></td>
<td></td>
<td>Calculates a fast, approximate, per-component reciprocal.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509639" target="_blank" rel="noopener">reflect</a></td>
<td>reflect(i, n)</td>
<td>返回入射光线i对表面法线n的反射光线。</td>
<td>Returns a reflection vector.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509640" target="_blank" rel="noopener">refract</a></td>
<td>refract(i, n, R)</td>
<td>返回在入射光线i，表面法线n，折射率为eta下的折射光线v。</td>
<td>Returns the refraction vector.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471437" target="_blank" rel="noopener">reversebits</a></td>
<td></td>
<td></td>
<td>Reverses the order of the bits, per component.</td>
<td>5</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509642" target="_blank" rel="noopener">round</a></td>
<td>round(x)</td>
<td>返回最接近于输入值的整数。</td>
<td>Rounds x to the nearest integer</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509643" target="_blank" rel="noopener">rsqrt</a></td>
<td>rsqrt(x)</td>
<td>返回输入值平方根的倒数。</td>
<td>Returns 1 / sqrt(x)</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509645" target="_blank" rel="noopener">saturate</a></td>
<td>saturate(x)</td>
<td>把输入值限制到[0, 1]之间。</td>
<td>Clamps x to the range [0, 1]</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509649" target="_blank" rel="noopener">sign</a></td>
<td>sign(x)</td>
<td>计算输入值的符号。</td>
<td>Computes the sign of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509651" target="_blank" rel="noopener">sin</a></td>
<td>sin(x)</td>
<td>计算输入值的正弦值。</td>
<td>Returns the sine of x</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509652" target="_blank" rel="noopener">sincos</a></td>
<td>sincos(x, out s, out c)</td>
<td>返回输入值的正弦和余弦值。</td>
<td>Returns the sine and cosine of x.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509653" target="_blank" rel="noopener">sinh</a></td>
<td>sinh(x)</td>
<td>返回x的双曲正弦。</td>
<td>Returns the hyperbolic sine of x</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509658" target="_blank" rel="noopener">smoothstep</a></td>
<td>smoothstep(min, max, x)</td>
<td>返回一个在输入值之间平稳变化的插值。</td>
<td>Returns a smooth Hermite interpolation between 0 and 1.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509662" target="_blank" rel="noopener">sqrt</a></td>
<td>sqrt(x)</td>
<td>返回输入值的平方根。</td>
<td>Square root (per component)</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509665" target="_blank" rel="noopener">step</a></td>
<td>step(a, x)</td>
<td>返回（x &gt;= a）? 1 : 0。</td>
<td>Returns (x &gt;= a) ? 1 : 0</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509670" target="_blank" rel="noopener">tan</a></td>
<td>tan(x)</td>
<td>返回输入值的正切值。</td>
<td>Returns the tangent of x</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509671" target="_blank" rel="noopener">tanh</a></td>
<td>tanh(x)</td>
<td></td>
<td>Returns the hyperbolic tangent of x</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509672" target="_blank" rel="noopener">tex1D(s, t)</a></td>
<td></td>
<td>1D纹理查询。</td>
<td>1D texture lookup.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471388" target="_blank" rel="noopener">tex1D(s, t, ddx, ddy)</a></td>
<td></td>
<td></td>
<td>1D texture lookup.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509673" target="_blank" rel="noopener">tex1Dbias</a></td>
<td>tex1Dbias(s, t)</td>
<td></td>
<td>1D texture lookup with bias.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509674" target="_blank" rel="noopener">tex1Dgrad</a></td>
<td>tex1Dgrad(s, t, ddx, ddy)</td>
<td></td>
<td>1D texture lookup with a gradient.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509675" target="_blank" rel="noopener">tex1Dlod</a></td>
<td>tex1Dlod(s, t)</td>
<td></td>
<td>1D texture lookup with LOD.</td>
<td>3<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509676" target="_blank" rel="noopener">tex1Dproj</a></td>
<td>tex1Dproj(s, t)</td>
<td></td>
<td>1D texture lookup with projective divide.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509677" target="_blank" rel="noopener">tex2D(s, t)</a></td>
<td></td>
<td>2D纹理查询。</td>
<td>2D texture lookup.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471389" target="_blank" rel="noopener">tex2D(s, t, ddx, ddy)</a></td>
<td></td>
<td></td>
<td>2D texture lookup.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509678" target="_blank" rel="noopener">tex2Dbias</a></td>
<td>tex2Dbias(s, t)</td>
<td></td>
<td>2D texture lookup with bias.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509679" target="_blank" rel="noopener">tex2Dgrad</a></td>
<td>tex2Dgrad(s, t, ddx, ddy)</td>
<td></td>
<td>2D texture lookup with a gradient.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509680" target="_blank" rel="noopener">tex2Dlod</a></td>
<td>tex2Dlod(s, t)</td>
<td></td>
<td>2D texture lookup with LOD.</td>
<td>3</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509681" target="_blank" rel="noopener">tex2Dproj</a></td>
<td>tex2Dproj(s, t)</td>
<td></td>
<td>2D texture lookup with projective divide.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509682" target="_blank" rel="noopener">tex3D(s, t)</a></td>
<td></td>
<td>3D纹理查询。</td>
<td>3D texture lookup.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471391" target="_blank" rel="noopener">tex3D(s, t, ddx, ddy)</a></td>
<td></td>
<td></td>
<td>3D texture lookup.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509683" target="_blank" rel="noopener">tex3Dbias</a></td>
<td>tex3Dbias(s, t)</td>
<td></td>
<td>3D texture lookup with bias.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509684" target="_blank" rel="noopener">tex3Dgrad</a></td>
<td>tex3Dgrad(s, t, ddx, ddy)</td>
<td></td>
<td>3D texture lookup with a gradient.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509685" target="_blank" rel="noopener">tex3Dlod</a></td>
<td>tex3Dlod(s, t)</td>
<td></td>
<td>3D texture lookup with LOD.</td>
<td>3<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509686" target="_blank" rel="noopener">tex3Dproj</a></td>
<td>tex3Dproj(s, t)</td>
<td></td>
<td>3D texture lookup with projective divide.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509687" target="_blank" rel="noopener">texCUBE(s, t)</a></td>
<td></td>
<td>立方纹理查询。</td>
<td>Cube texture lookup.</td>
<td>1<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/ff471392" target="_blank" rel="noopener">texCUBE(s, t, ddx, ddy)</a></td>
<td></td>
<td></td>
<td>Cube texture lookup.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509688" target="_blank" rel="noopener">texCUBEbias</a></td>
<td>texCUBEbias(s, t)</td>
<td></td>
<td>Cube texture lookup with bias.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509689" target="_blank" rel="noopener">texCUBEgrad</a></td>
<td>texCUBEgrad(s, t, ddx, ddy)</td>
<td></td>
<td>Cube texture lookup with a gradient.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509690" target="_blank" rel="noopener">texCUBElod</a></td>
<td></td>
<td></td>
<td>Cube texture lookup with LOD.</td>
<td>3<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509691" target="_blank" rel="noopener">texCUBEproj</a></td>
<td>texCUBEproj(s, t)</td>
<td></td>
<td>Cube texture lookup with projective divide.</td>
<td>2<sup>1</sup></td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/bb509701" target="_blank" rel="noopener">transpose</a></td>
<td>transpose(m)</td>
<td>返回输入矩阵的转置。</td>
<td>Returns the transpose of the matrix m.</td>
<td>1</td>
</tr>
<tr>
<td><a href="http://preview.library.microsoft.com/en-us/library/cc308065" target="_blank" rel="noopener">trunc</a></td>
<td>trunc(x)</td>
<td></td>
<td>Truncates floating-point value(s) to integer value(s)</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<p>参考链接</p>
<ul>
<li><a href="http://www.cppblog.com/lai3d/archive/2008/10/23/64889.html" target="_blank" rel="noopener">每天30分钟看Shader—(1)HLSL固有函数 【Intrinsic Functions (DirectX HLSL)】</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> shader </category>
            
        </categories>
        
        
        <tags>
            
            <tag> shader </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
